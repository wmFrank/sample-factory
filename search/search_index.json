{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Sample Factory \u00b6 High-throughput reinforcement learning codebase. Resources: Paper: https://arxiv.org/abs/2006.11751 Discord: https://discord.gg/BCfHWaSMkr What is Sample Factory? \u00b6 Sample Factory is one of the fastest RL libraries focused on very efficient synchronous and asynchronous implementations of policy gradients (PPO). Sample Factory is thoroughly tested, used by many researchers and practitioners, and is actively maintained. Our implementation is known to reach SOTA performance in a variety of domains while minimizing RL experiment training time and hardware requirements. Clips below demonstrate ViZDoom, IsaacGym, DMLab-30, Megaverse, Mujoco, and Atari agents trained with Sample Factory: Key features \u00b6 Highly optimized algorithm architecture for maximum learning throughput Synchronous and asynchronous training regimes Serial (single-process) mode for easy debugging Optimal performance in both CPU-based and GPU-accelerated environments Single- & multi-agent training, self-play, supports training multiple policies at once on one or many GPUs Population-Based Training ( PBT ) Discrete, continuous, hybrid action spaces Vector-based, image-based, dictionary observation spaces Automatically creates a model architecture by parsing action/observation space specification. Supports custom model architectures Library is designed to be imported into other projects, custom environments are first-class citizens Detailed WandB and Tensorboard summaries , custom metrics HuggingFace \ud83e\udd17 integration (upload trained models and metrics to the Hub) Multiple example environment integrations with tuned parameters and trained models Next steps \u00b6 Check out the following guides to get started: Installation Basic Usage","title":"Overview"},{"location":"#sample-factory","text":"High-throughput reinforcement learning codebase. Resources: Paper: https://arxiv.org/abs/2006.11751 Discord: https://discord.gg/BCfHWaSMkr","title":"Sample Factory"},{"location":"#what-is-sample-factory","text":"Sample Factory is one of the fastest RL libraries focused on very efficient synchronous and asynchronous implementations of policy gradients (PPO). Sample Factory is thoroughly tested, used by many researchers and practitioners, and is actively maintained. Our implementation is known to reach SOTA performance in a variety of domains while minimizing RL experiment training time and hardware requirements. Clips below demonstrate ViZDoom, IsaacGym, DMLab-30, Megaverse, Mujoco, and Atari agents trained with Sample Factory:","title":"What is Sample Factory?"},{"location":"#key-features","text":"Highly optimized algorithm architecture for maximum learning throughput Synchronous and asynchronous training regimes Serial (single-process) mode for easy debugging Optimal performance in both CPU-based and GPU-accelerated environments Single- & multi-agent training, self-play, supports training multiple policies at once on one or many GPUs Population-Based Training ( PBT ) Discrete, continuous, hybrid action spaces Vector-based, image-based, dictionary observation spaces Automatically creates a model architecture by parsing action/observation space specification. Supports custom model architectures Library is designed to be imported into other projects, custom environments are first-class citizens Detailed WandB and Tensorboard summaries , custom metrics HuggingFace \ud83e\udd17 integration (upload trained models and metrics to the Hub) Multiple example environment integrations with tuned parameters and trained models","title":"Key features"},{"location":"#next-steps","text":"Check out the following guides to get started: Installation Basic Usage","title":"Next steps"},{"location":"01-get-started/basic-usage/","text":"Basic Usage \u00b6 Usage examples \u00b6 Use command line to train an agent using one of the existing integrations, e.g. Mujoco (might need to run pip install sample-factory[mujoco] ): python -m sf_examples.mujoco.train_mujoco --env = mujoco_ant --experiment = Ant --train_dir = ./train_dir Stop the experiment when the desired performance is reached and then evaluate the agent: python -m sf_examples.mujoco.enjoy_mujoco --env = mujoco_ant --experiment = Ant --train_dir = ./train_dir Do the same in a pixel-based environment such as VizDoom (might need to run pip install sample-factory[vizdoom] , please also see docs for VizDoom-specific instructions): python -m sf_examples.vizdoom.train_vizdoom --env = doom_basic --experiment = DoomBasic --train_dir = ./train_dir --num_workers = 16 --num_envs_per_worker = 10 --train_for_env_steps = 1000000 python -m sf_examples.vizdoom.enjoy_vizdoom --env = doom_basic --experiment = DoomBasic --train_dir = ./train_dir Monitoring experiments \u00b6 Monitor any running or completed experiment with Tensorboard: tensorboard --logdir = ./train_dir (or see the docs for WandB integration). Next steps \u00b6 Read more about configuring experiments in the Configuration guide. Follow the instructions in the Customizing guide to train an agent in your own environment.","title":"Basic Usage"},{"location":"01-get-started/basic-usage/#basic-usage","text":"","title":"Basic Usage"},{"location":"01-get-started/basic-usage/#usage-examples","text":"Use command line to train an agent using one of the existing integrations, e.g. Mujoco (might need to run pip install sample-factory[mujoco] ): python -m sf_examples.mujoco.train_mujoco --env = mujoco_ant --experiment = Ant --train_dir = ./train_dir Stop the experiment when the desired performance is reached and then evaluate the agent: python -m sf_examples.mujoco.enjoy_mujoco --env = mujoco_ant --experiment = Ant --train_dir = ./train_dir Do the same in a pixel-based environment such as VizDoom (might need to run pip install sample-factory[vizdoom] , please also see docs for VizDoom-specific instructions): python -m sf_examples.vizdoom.train_vizdoom --env = doom_basic --experiment = DoomBasic --train_dir = ./train_dir --num_workers = 16 --num_envs_per_worker = 10 --train_for_env_steps = 1000000 python -m sf_examples.vizdoom.enjoy_vizdoom --env = doom_basic --experiment = DoomBasic --train_dir = ./train_dir","title":"Usage examples"},{"location":"01-get-started/basic-usage/#monitoring-experiments","text":"Monitor any running or completed experiment with Tensorboard: tensorboard --logdir = ./train_dir (or see the docs for WandB integration).","title":"Monitoring experiments"},{"location":"01-get-started/basic-usage/#next-steps","text":"Read more about configuring experiments in the Configuration guide. Follow the instructions in the Customizing guide to train an agent in your own environment.","title":"Next steps"},{"location":"01-get-started/installation/","text":"Installation \u00b6 Just install from PyPI: pip install sample-factory SF is known to work on Linux and macOS. There is no Windows support at this time. Install from sources \u00b6 git clone git@github.com:alex-petrenko/sample-factory.git cd sample-factory pip install -e . # or install with optional dependencies pip install -e . [ dev,mujoco,atari,vizdoom ] Environment support \u00b6 To run Sample Factory with one of the available environment integrations, please refer to the corresponding documentation sections: Mujoco Atari ViZDoom DeepMind Lab Megaverse Envpool Isaac Gym Quad-Swarm-RL Sample Factory allows users to easily add custom environments and models, refer to Customizing Sample Factory for more information.","title":"Installation"},{"location":"01-get-started/installation/#installation","text":"Just install from PyPI: pip install sample-factory SF is known to work on Linux and macOS. There is no Windows support at this time.","title":"Installation"},{"location":"01-get-started/installation/#install-from-sources","text":"git clone git@github.com:alex-petrenko/sample-factory.git cd sample-factory pip install -e . # or install with optional dependencies pip install -e . [ dev,mujoco,atari,vizdoom ]","title":"Install from sources"},{"location":"01-get-started/installation/#environment-support","text":"To run Sample Factory with one of the available environment integrations, please refer to the corresponding documentation sections: Mujoco Atari ViZDoom DeepMind Lab Megaverse Envpool Isaac Gym Quad-Swarm-RL Sample Factory allows users to easily add custom environments and models, refer to Customizing Sample Factory for more information.","title":"Environment support"},{"location":"02-configuration/cfg-params/","text":"Full Parameter Reference \u00b6 The command line arguments / config parameters for training using Sample Factory can be found by running your training script with the --help flag. The list of config parameters below was obtained from running python -m sf_examples.train_gym_env --env=CartPole-v1 --help . These params can be used in any environment. Other environments may have other custom params than can also be viewed with --help flag when running the environment-specific training script. usage: train_gym_env.py [-h] [--algo ALGO] --env ENV [--experiment EXPERIMENT] [--train_dir TRAIN_DIR] [--restart_behavior {resume,restart,overwrite}] [--device {gpu,cpu}] [--seed SEED] [--num_policies NUM_POLICIES] [--async_rl ASYNC_RL] [--serial_mode SERIAL_MODE] [--batched_sampling BATCHED_SAMPLING] [--num_batches_to_accumulate NUM_BATCHES_TO_ACCUMULATE] [--worker_num_splits WORKER_NUM_SPLITS] [--policy_workers_per_policy POLICY_WORKERS_PER_POLICY] [--max_policy_lag MAX_POLICY_LAG] [--num_workers NUM_WORKERS] [--num_envs_per_worker NUM_ENVS_PER_WORKER] [--batch_size BATCH_SIZE] [--num_batches_per_epoch NUM_BATCHES_PER_EPOCH] [--num_epochs NUM_EPOCHS] [--rollout ROLLOUT] [--recurrence RECURRENCE] [--shuffle_minibatches SHUFFLE_MINIBATCHES] [--gamma GAMMA] [--reward_scale REWARD_SCALE] [--reward_clip REWARD_CLIP] [--value_bootstrap VALUE_BOOTSTRAP] [--normalize_returns NORMALIZE_RETURNS] [--exploration_loss_coeff EXPLORATION_LOSS_COEFF] [--value_loss_coeff VALUE_LOSS_COEFF] [--kl_loss_coeff KL_LOSS_COEFF] [--exploration_loss {entropy,symmetric_kl}] [--gae_lambda GAE_LAMBDA] [--ppo_clip_ratio PPO_CLIP_RATIO] [--ppo_clip_value PPO_CLIP_VALUE] [--with_vtrace WITH_VTRACE] [--vtrace_rho VTRACE_RHO] [--vtrace_c VTRACE_C] [--optimizer {adam,lamb}] [--adam_eps ADAM_EPS] [--adam_beta1 ADAM_BETA1] [--adam_beta2 ADAM_BETA2] [--max_grad_norm MAX_GRAD_NORM] [--learning_rate LEARNING_RATE] [--lr_schedule {constant,kl_adaptive_minibatch,kl_adaptive_epoch}] [--lr_schedule_kl_threshold LR_SCHEDULE_KL_THRESHOLD] [--obs_subtract_mean OBS_SUBTRACT_MEAN] [--obs_scale OBS_SCALE] [--normalize_input NORMALIZE_INPUT] [--normalize_input_keys [NORMALIZE_INPUT_KEYS [NORMALIZE_INPUT_KEYS ...]]] [--decorrelate_experience_max_seconds DECORRELATE_EXPERIENCE_MAX_SECONDS] [--decorrelate_envs_on_one_worker DECORRELATE_ENVS_ON_ONE_WORKER] [--actor_worker_gpus [ACTOR_WORKER_GPUS [ACTOR_WORKER_GPUS ...]]] [--set_workers_cpu_affinity SET_WORKERS_CPU_AFFINITY] [--force_envs_single_thread FORCE_ENVS_SINGLE_THREAD] [--default_niceness DEFAULT_NICENESS] [--log_to_file LOG_TO_FILE] [--experiment_summaries_interval EXPERIMENT_SUMMARIES_INTERVAL] [--flush_summaries_interval FLUSH_SUMMARIES_INTERVAL] [--stats_avg STATS_AVG] [--summaries_use_frameskip SUMMARIES_USE_FRAMESKIP] [--heartbeat_interval HEARTBEAT_INTERVAL] [--heartbeat_reporting_interval HEARTBEAT_REPORTING_INTERVAL] [--train_for_env_steps TRAIN_FOR_ENV_STEPS] [--train_for_seconds TRAIN_FOR_SECONDS] [--save_every_sec SAVE_EVERY_SEC] [--keep_checkpoints KEEP_CHECKPOINTS] [--load_checkpoint_kind {latest,best}] [--save_milestones_sec SAVE_MILESTONES_SEC] [--save_best_every_sec SAVE_BEST_EVERY_SEC] [--save_best_metric SAVE_BEST_METRIC] [--save_best_after SAVE_BEST_AFTER] [--benchmark BENCHMARK] [--encoder_mlp_layers [ENCODER_MLP_LAYERS [ENCODER_MLP_LAYERS ...]]] [--encoder_conv_architecture {convnet_simple,convnet_impala,convnet_atari,resnet_impala}] [--encoder_conv_mlp_layers [ENCODER_CONV_MLP_LAYERS [ENCODER_CONV_MLP_LAYERS ...]]] [--use_rnn USE_RNN] [--rnn_size RNN_SIZE] [--rnn_type {gru,lstm}] [--rnn_num_layers RNN_NUM_LAYERS] [--decoder_mlp_layers [DECODER_MLP_LAYERS [DECODER_MLP_LAYERS ...]]] [--nonlinearity {elu,relu,tanh}] [--policy_initialization {orthogonal,xavier_uniform,torch_default}] [--policy_init_gain POLICY_INIT_GAIN] [--actor_critic_share_weights ACTOR_CRITIC_SHARE_WEIGHTS] [--adaptive_stddev ADAPTIVE_STDDEV] [--continuous_tanh_scale CONTINUOUS_TANH_SCALE] [--initial_stddev INITIAL_STDDEV] [--use_env_info_cache USE_ENV_INFO_CACHE] [--env_gpu_actions ENV_GPU_ACTIONS] [--env_gpu_observations ENV_GPU_OBSERVATIONS] [--env_frameskip ENV_FRAMESKIP] [--env_framestack ENV_FRAMESTACK] [--pixel_format PIXEL_FORMAT] [--use_record_episode_statistics USE_RECORD_EPISODE_STATISTICS] [--with_wandb WITH_WANDB] [--wandb_user WANDB_USER] [--wandb_project WANDB_PROJECT] [--wandb_group WANDB_GROUP] [--wandb_job_type WANDB_JOB_TYPE] [--wandb_tags [WANDB_TAGS [WANDB_TAGS ...]]] [--with_pbt WITH_PBT] [--pbt_mix_policies_in_one_env PBT_MIX_POLICIES_IN_ONE_ENV] [--pbt_period_env_steps PBT_PERIOD_ENV_STEPS] [--pbt_start_mutation PBT_START_MUTATION] [--pbt_replace_fraction PBT_REPLACE_FRACTION] [--pbt_mutation_rate PBT_MUTATION_RATE] [--pbt_replace_reward_gap PBT_REPLACE_REWARD_GAP] [--pbt_replace_reward_gap_absolute PBT_REPLACE_REWARD_GAP_ABSOLUTE] [--pbt_optimize_gamma PBT_OPTIMIZE_GAMMA] [--pbt_target_objective PBT_TARGET_OBJECTIVE] [--pbt_perturb_min PBT_PERTURB_MIN] [--pbt_perturb_max PBT_PERTURB_MAX] optional arguments: -h, --help Print the help message (default: False) --algo ALGO Algorithm to use (default: APPO) --env ENV Name of the environment to use (default: None) --experiment EXPERIMENT Unique experiment name. This will also be the name for the experiment folder in the train dir.If the experiment folder with this name aleady exists the experiment will be RESUMED!Any parameters passed from command line that do not match the parameters stored in the experiment config.json file will be overridden. (default: default_experiment) --train_dir TRAIN_DIR Root for all experiments (default: /home/runner/work/sample-factory/sample- factory/train_dir) --restart_behavior {resume,restart,overwrite} How to handle the experiment if the directory with the same name already exists. \"resume\" (default) will resume the experiment, \"restart\" will preserve the existing experiment folder under a different name (with \"old\" suffix) and will start training from scratch, \"overwrite\" will delete the existing experiment folder and start from scratch. This parameter does not have any effect if the experiment directory does not exist. (default: resume) --device {gpu,cpu} CPU training is only recommended for smaller e.g. MLP policies (default: gpu) --seed SEED Set a fixed seed value (default: None) --num_policies NUM_POLICIES Number of policies to train jointly, i.e. for multi- agent environments (default: 1) --async_rl ASYNC_RL Collect experience asynchronously while learning on the previous batch. This is significantly different from standard synchronous actor-critic (or PPO) because not all of the experience will be collected by the latest policy thus increasing policy lag. Negative effects of using async_rl can range from negligible (just grants you throughput boost) to quite serious where you can consider switching it off. It all depends how sensitive your experiment is to policy lag. Envs with complex action spaces and RNN policies tend to be particularly sensitive. (default: True) --serial_mode SERIAL_MODE Enable serial mode: run everything completely synchronously in the same process (default: False) --batched_sampling BATCHED_SAMPLING Batched sampling allows the data to be processed in big batches on the rollout worker.This is especially important for GPU-accelerated vectorized environments such as Megaverse or IsaacGym. As a downside, in batched mode we do not support (for now) some of the features, such as population-based self-play or inactive agents, plus each batched sampler (rollout worker) process only collects data for a single policy. Another issue between batched/non-batched sampling is handling of infos. In batched mode we assume that infos is a single dictionary of lists/tensors containing info for each environment in a vector. If you need some complex info dictionary handling and your environment might return dicts with different keys, on different rollout steps, you probably need non-batched mode. (default: False) --num_batches_to_accumulate NUM_BATCHES_TO_ACCUMULATE This parameter governs the maximum number of training batches the learner can accumulate before further experience collection is stopped. The default value will set this to 2, so if the experience collection is faster than the training, the learner will accumulate enough minibatches for 2 iterations of training but no more. This is a good balance between policy-lag and throughput. When the limit is reached, the learner will notify the actor workers that they ought to stop the experience collection until accumulated minibatches are processed. Set this parameter to 1 to further reduce policy-lag. If the experience collection is very non-uniform, increasing this parameter can increase overall throughput, at the cost of increased policy-lag. (default: 2) --worker_num_splits WORKER_NUM_SPLITS Typically we split a vector of envs into two parts for \"double buffered\" experience collection Set this to 1 to disable double buffering. Set this to 3 for triple buffering! (default: 2) --policy_workers_per_policy POLICY_WORKERS_PER_POLICY Number of policy workers that compute forward pass (per policy) (default: 1) --max_policy_lag MAX_POLICY_LAG Max policy lag in policy versions. Discard all experience that is older than this. (default: 1000) --num_workers NUM_WORKERS Number of parallel environment workers. Should be less than num_envs and should divide num_envs.Use this in async mode. (default: 2) --num_envs_per_worker NUM_ENVS_PER_WORKER Number of envs on a single CPU actor, in high- throughput configurations this should be in 10-30 range for Atari/VizDoomMust be even for double- buffered sampling! (default: 2) --batch_size BATCH_SIZE Minibatch size for SGD (default: 1024) --num_batches_per_epoch NUM_BATCHES_PER_EPOCH This determines the training dataset size for each iteration of training. We collect this many minibatches before performing any SGD. Example: if batch_size=128 and num_batches_per_epoch=2, then learner will process 2*128=256 environment transitions in one training iteration. (default: 1) --num_epochs NUM_EPOCHS Number of training epochs on a dataset of collected experiences of size batch_size x num_batches_per_epoch (default: 1) --rollout ROLLOUT Length of the rollout from each environment in timesteps.Once we collect this many timesteps on actor worker, we send this trajectory to the learner.The length of the rollout will determine how many timesteps are used to calculate bootstrappedMonte- Carlo estimates of discounted rewards, advantages, GAE, or V-trace targets. Shorter rolloutsreduce variance, but the estimates are less precise (bias vs variance tradeoff).For RNN policies, this should be a multiple of --recurrence, so every rollout will be splitinto (n = rollout / recurrence) segments for backpropagation. V-trace algorithm currently requires thatrollout == recurrence, which what you want most of the time anyway.Rollout length is independent from the episode length. Episode length can be both shorter or longer thanrollout, although for PBT training it is currently recommended that rollout << episode_len(see function finalize_trajectory in actor_worker.py) (default: 32) --recurrence RECURRENCE Trajectory length for backpropagation through time. If recurrence=1 there is no backpropagation through time, and experience is shuffled completely randomlyFor V-trace recurrence should be equal to rollout length. (default: 32) --shuffle_minibatches SHUFFLE_MINIBATCHES Whether to randomize and shuffle minibatches between iterations (this is a slow operation when batches are large, disabling this increases learner throughput when training with multiple epochs/minibatches per epoch) (default: False) --gamma GAMMA Discount factor (default: 0.99) --reward_scale REWARD_SCALE Multiply all rewards by this factor before feeding into RL algorithm.Sometimes the overall scale of rewards is too high which makes value estimation a harder regression task.Loss values become too high which requires a smaller learning rate, etc. (default: 1.0) --reward_clip REWARD_CLIP Clip rewards between [-c, c]. Default [-1000, 1000] should mean no clipping for most envs (unless rewards are very large/small) (default: 1000.0) --value_bootstrap VALUE_BOOTSTRAP Bootstrap returns from value estimates if episode is terminated by timeout. More info here: https://github.com/Denys88/rl_games/issues/128 (default: False) --normalize_returns NORMALIZE_RETURNS Whether to use running mean and standard deviation to normalize discounted returns (default: True) --exploration_loss_coeff EXPLORATION_LOSS_COEFF Coefficient for the exploration component of the loss function. (default: 0.003) --value_loss_coeff VALUE_LOSS_COEFF Coefficient for the critic loss (default: 0.5) --kl_loss_coeff KL_LOSS_COEFF Coefficient for fixed KL loss (as used by Schulman et al. in https://arxiv.org/pdf/1707.06347.pdf). Highly recommended for environments with continuous action spaces. (default: 0.0) --exploration_loss {entropy,symmetric_kl} Usually the exploration loss is based on maximizing the entropy of the probability distribution. Note that mathematically maximizing entropy of the categorical probability distribution is exactly the same as minimizing the (regular) KL-divergence between this distribution and a uniform prior. The downside of using the entropy term (or regular asymmetric KL- divergence) is the fact that penalty does not increase as probabilities of some actions approach zero. I.e. numerically, there is almost no difference between an action distribution with a probability epsilon > 0 for some action and an action distribution with a probability = zero for this action. For many tasks the first (epsilon) distribution is preferrable because we keep some (albeit small) amount of exploration, while the second distribution will never explore this action ever again.Unlike the entropy term, symmetric KL divergence between the action distribution and a uniform prior approaches infinity when entropy of the distribution approaches zero, so it can prevent the pathological situations where the agent stops exploring. Empirically, symmetric KL-divergence yielded slightly better results on some problems. (default: entropy) --gae_lambda GAE_LAMBDA Generalized Advantage Estimation discounting (only used when V-trace is False) (default: 0.95) --ppo_clip_ratio PPO_CLIP_RATIO We use unbiased clip(x, 1+e, 1/(1+e)) instead of clip(x, 1+e, 1-e) in the paper (default: 0.1) --ppo_clip_value PPO_CLIP_VALUE Maximum absolute change in value estimate until it is clipped. Sensitive to value magnitude (default: 1.0) --with_vtrace WITH_VTRACE Enables V-trace off-policy correction. If this is True, then GAE is not used (default: False) --vtrace_rho VTRACE_RHO rho_hat clipping parameter of the V-trace algorithm (importance sampling truncation) (default: 1.0) --vtrace_c VTRACE_C c_hat clipping parameter of the V-trace algorithm. Low values for c_hat can reduce variance of the advantage estimates (similar to GAE lambda < 1) (default: 1.0) --optimizer {adam,lamb} Type of optimizer to use (default: adam) --adam_eps ADAM_EPS Adam epsilon parameter (1e-8 to 1e-5 seem to reliably work okay, 1e-3 and up does not work) (default: 1e-06) --adam_beta1 ADAM_BETA1 Adam momentum decay coefficient (default: 0.9) --adam_beta2 ADAM_BETA2 Adam second momentum decay coefficient (default: 0.999) --max_grad_norm MAX_GRAD_NORM Max L2 norm of the gradient vector, set to 0 to disable gradient clipping (default: 4.0) --learning_rate LEARNING_RATE LR (default: 0.0001) --lr_schedule {constant,kl_adaptive_minibatch,kl_adaptive_epoch} Learning rate schedule to use. Constant keeps constant learning rate throughout training.kl_adaptive* schedulers look at --lr_schedule_kl_threshold and if KL-divergence with behavior policyafter the last minibatch/epoch significantly deviates from this threshold, lr is apropriatelyincreased or decreased (default: constant) --lr_schedule_kl_threshold LR_SCHEDULE_KL_THRESHOLD Used with kl_adaptive_* schedulers (default: 0.008) --obs_subtract_mean OBS_SUBTRACT_MEAN Observation preprocessing, mean value to subtract from observation (e.g. 128.0 for 8-bit RGB) (default: 0.0) --obs_scale OBS_SCALE Observation preprocessing, divide observation tensors by this scalar (e.g. 128.0 for 8-bit RGB) (default: 1.0) --normalize_input NORMALIZE_INPUT Whether to use running mean and standard deviation to normalize observations (default: True) --normalize_input_keys [NORMALIZE_INPUT_KEYS [NORMALIZE_INPUT_KEYS ...]] Which observation keys to use for normalization. If None, all observation keys are used (be careful with this!) (default: None) --decorrelate_experience_max_seconds DECORRELATE_EXPERIENCE_MAX_SECONDS Decorrelating experience serves two benefits. First: this is better for learning because samples from workers come from random moments in the episode, becoming more \"i.i.d\".Second, and more important one: this is good for environments with highly non-uniform one-step times, including long and expensive episode resets. If experience is not decorrelatedthen training batches will come in bursts e.g. after a bunch of environments finished resets and many iterations on the learner might be required,which will increase the policy-lag of the new experience collected. The performance of the Sample Factory is best when experience is generated as more-or-lessuniform stream. Try increasing this to 100-200 seconds to smoothen the experience distribution in time right from the beginning (it will eventually spread out and settle anyways) (default: 0) --decorrelate_envs_on_one_worker DECORRELATE_ENVS_ON_ONE_WORKER In addition to temporal decorrelation of worker processes, also decorrelate envs within one worker process. For environments with a fixed episode length it can prevent the reset from happening in the same rollout for all envs simultaneously, which makes experience collection more uniform. (default: True) --actor_worker_gpus [ACTOR_WORKER_GPUS [ACTOR_WORKER_GPUS ...]] By default, actor workers only use CPUs. Changes this if e.g. you need GPU-based rendering on the actors (default: []) --set_workers_cpu_affinity SET_WORKERS_CPU_AFFINITY Whether to assign workers to specific CPU cores or not. The logic is beneficial for most workloads because prevents a lot of context switching.However for some environments it can be better to disable it, to allow one worker to use all cores some of the time. This can be the case for some DMLab environments with very expensive episode resetthat can use parallel CPU cores for level generation. (default: True) --force_envs_single_thread FORCE_ENVS_SINGLE_THREAD Some environments may themselves use parallel libraries such as OpenMP or MKL. Since we parallelize environments on the level of workers, there is no need to keep this parallel semantic.This flag uses threadpoolctl to force libraries such as OpenMP and MKL to use only a single thread within the environment.Enabling this is recommended unless you are running fewer workers than CPU cores. threadpoolctl has caused a bunch of crashes in the past, so this feature is disabled by default at this moment. (default: False) --default_niceness DEFAULT_NICENESS Niceness of the highest priority process (the learner). Values below zero require elevated privileges. (default: 0) --log_to_file LOG_TO_FILE Whether to log to a file (sf_log.txt in the experiment folder) or not. If False, logs to stdout only. It can make sense to disable this in a slow server filesystem environment like NFS. (default: True) --experiment_summaries_interval EXPERIMENT_SUMMARIES_INTERVAL How often in seconds we write avg. statistics about the experiment (reward, episode length, extra stats...) (default: 10) --flush_summaries_interval FLUSH_SUMMARIES_INTERVAL How often do we flush tensorboard summaries (set to higher value for slow NFS-based server filesystems) (default: 30) --stats_avg STATS_AVG How many episodes to average to measure performance (avg. reward etc) (default: 100) --summaries_use_frameskip SUMMARIES_USE_FRAMESKIP Whether to multiply training steps by frameskip when recording summaries, FPS, etc. When this flag is set to True, x-axis for all summaries corresponds to the total number of simulated steps, i.e. with frameskip=4 the x-axis value of 4 million will correspond to 1 million frames observed by the policy. (default: True) --heartbeat_interval HEARTBEAT_INTERVAL How often in seconds components send a heartbeat signal to the runner to verify they are not stuck (default: 20) --heartbeat_reporting_interval HEARTBEAT_REPORTING_INTERVAL How often in seconds the runner checks for heartbeats (default: 180) --train_for_env_steps TRAIN_FOR_ENV_STEPS Stop after all policies are trained for this many env steps (default: 10000000000) --train_for_seconds TRAIN_FOR_SECONDS Stop training after this many seconds (default: 10000000000) --save_every_sec SAVE_EVERY_SEC Checkpointing rate (default: 120) --keep_checkpoints KEEP_CHECKPOINTS Number of model checkpoints to keep (default: 2) --load_checkpoint_kind {latest,best} Whether to load from latest or best checkpoint (default: latest) --save_milestones_sec SAVE_MILESTONES_SEC Save intermediate checkpoints in a separate folder for later evaluation (default=never) (default: -1) --save_best_every_sec SAVE_BEST_EVERY_SEC How often we check if we should save the policy with the best score ever (default: 5) --save_best_metric SAVE_BEST_METRIC Save \"best\" policies based on this metric (just env reward by default) (default: reward) --save_best_after SAVE_BEST_AFTER Start saving \"best\" policies after this many env steps to filter lucky episodes that succeed and dominate the statistics early on (default: 100000) --benchmark BENCHMARK Benchmark mode (default: False) --encoder_mlp_layers [ENCODER_MLP_LAYERS [ENCODER_MLP_LAYERS ...]] In case of MLP encoder, sizes of layers to use. This is ignored if observations are images. (default: [512, 512]) --encoder_conv_architecture {convnet_simple,convnet_impala,convnet_atari,resnet_impala} Architecture of the convolutional encoder. See models.py for details. VizDoom and DMLab examples demonstrate how to define custom architectures. (default: convnet_simple) --encoder_conv_mlp_layers [ENCODER_CONV_MLP_LAYERS [ENCODER_CONV_MLP_LAYERS ...]] Optional fully connected layers after the convolutional encoder head. (default: [512]) --use_rnn USE_RNN Whether to use RNN core in a policy or not (default: True) --rnn_size RNN_SIZE Size of the RNN hidden state in recurrent model (e.g. GRU or LSTM) (default: 512) --rnn_type {gru,lstm} Type of RNN cell to use if use_rnn is True (default: gru) --rnn_num_layers RNN_NUM_LAYERS Number of RNN layers to use if use_rnn is True (default: 1) --decoder_mlp_layers [DECODER_MLP_LAYERS [DECODER_MLP_LAYERS ...]] Optional decoder MLP layers after the policy core. If empty (default) decoder is identity function. (default: []) --nonlinearity {elu,relu,tanh} Type of nonlinearity to use. (default: elu) --policy_initialization {orthogonal,xavier_uniform,torch_default} NN weight initialization (default: orthogonal) --policy_init_gain POLICY_INIT_GAIN Gain parameter of PyTorch initialization schemas (i.e. Xavier) (default: 1.0) --actor_critic_share_weights ACTOR_CRITIC_SHARE_WEIGHTS Whether to share the weights between policy and value function (default: True) --adaptive_stddev ADAPTIVE_STDDEV Only for continuous action distributions, whether stddev is state-dependent or just a single learned parameter (default: True) --continuous_tanh_scale CONTINUOUS_TANH_SCALE Only for continuous action distributions, whether to use tanh squashing and what scale to use. Applies tanh(mu / scale) * scale to distribution means. Experimental. Currently only works with adaptive_stddev=False (TODO). (default: 0.0) --initial_stddev INITIAL_STDDEV Initial value for non-adaptive stddev. Only makes sense for continuous action spaces (default: 1.0) --use_env_info_cache USE_ENV_INFO_CACHE Whether to use cached env info (default: False) --env_gpu_actions ENV_GPU_ACTIONS Set to true if environment expects actions on GPU (i.e. as a GPU-side PyTorch tensor) (default: False) --env_gpu_observations ENV_GPU_OBSERVATIONS Setting this to True together with non-empty --actor_worker_gpus will make observations GPU-side PyTorch tensors. Otherwise data will be on CPU. For CPU-based envs just set --actor_worker_gpus to empty list then this parameter does not matter. (default: True) --env_frameskip ENV_FRAMESKIP Number of frames for action repeat (frame skipping). Setting this to >1 will not add any wrappers that will do frame-skipping, although this can be used in the environment factory function to add these wrappers or to tell the environment itself to skip a desired number of frames i.e. as it is done in VizDoom. FPS metrics will be multiplied by the frameskip value, i.e. 100000FPS with frameskip=4 actually corresponds to 100000/4=25000 samples per second observed by the policy. Frameskip=1 (default) means no frameskip, we process every frame. (default: 1) --env_framestack ENV_FRAMESTACK Frame stacking (only used in Atari, and it is usually set to 4) (default: 1) --pixel_format PIXEL_FORMAT PyTorch expects CHW by default, Ray & TensorFlow expect HWC (default: CHW) --use_record_episode_statistics USE_RECORD_EPISODE_STATISTICS Whether to use gym RecordEpisodeStatistics wrapper to keep track of reward (default: False) --with_wandb WITH_WANDB Enables Weights and Biases integration (default: False) --wandb_user WANDB_USER WandB username (entity). Must be specified from command line! Also see https://docs.wandb.ai/quickstart#1.-set-up-wandb (default: None) --wandb_project WANDB_PROJECT WandB \"Project\" (default: sample_factory) --wandb_group WANDB_GROUP WandB \"Group\" (to group your experiments). By default this is the name of the env. (default: None) --wandb_job_type WANDB_JOB_TYPE WandB job type (default: SF) --wandb_tags [WANDB_TAGS [WANDB_TAGS ...]] Tags can help with finding experiments in WandB web console (default: []) --with_pbt WITH_PBT Enables population-based training (PBT) (default: False) --pbt_mix_policies_in_one_env PBT_MIX_POLICIES_IN_ONE_ENV For multi-agent envs, whether we mix different policies in one env. (default: True) --pbt_period_env_steps PBT_PERIOD_ENV_STEPS Periodically replace the worst policies with the best ones and perturb the hyperparameters (default: 5000000) --pbt_start_mutation PBT_START_MUTATION Allow initial diversification, start PBT after this many env steps (default: 20000000) --pbt_replace_fraction PBT_REPLACE_FRACTION A portion of policies performing worst to be replace by better policies (rounded up) (default: 0.3) --pbt_mutation_rate PBT_MUTATION_RATE Probability that a parameter mutates (default: 0.15) --pbt_replace_reward_gap PBT_REPLACE_REWARD_GAP Relative gap in true reward when replacing weights of the policy with a better performing one (default: 0.1) --pbt_replace_reward_gap_absolute PBT_REPLACE_REWARD_GAP_ABSOLUTE Absolute gap in true reward when replacing weights of the policy with a better performing one (default: 1e-06) --pbt_optimize_gamma PBT_OPTIMIZE_GAMMA Whether to optimize gamma, discount factor, or not (experimental) (default: False) --pbt_target_objective PBT_TARGET_OBJECTIVE Policy stat to optimize with PBT. true_objective (default) is equal to raw env reward if not specified, but can also be any other per-policy stat.For DMlab-30 use value \"dmlab_target_objective\" (which is capped human normalized score) (default: true_objective) --pbt_perturb_min PBT_PERTURB_MIN When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default: 1.1) --pbt_perturb_max PBT_PERTURB_MAX When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default: 1.5)","title":"Full Parameter Reference"},{"location":"02-configuration/cfg-params/#full-parameter-reference","text":"The command line arguments / config parameters for training using Sample Factory can be found by running your training script with the --help flag. The list of config parameters below was obtained from running python -m sf_examples.train_gym_env --env=CartPole-v1 --help . These params can be used in any environment. Other environments may have other custom params than can also be viewed with --help flag when running the environment-specific training script. usage: train_gym_env.py [-h] [--algo ALGO] --env ENV [--experiment EXPERIMENT] [--train_dir TRAIN_DIR] [--restart_behavior {resume,restart,overwrite}] [--device {gpu,cpu}] [--seed SEED] [--num_policies NUM_POLICIES] [--async_rl ASYNC_RL] [--serial_mode SERIAL_MODE] [--batched_sampling BATCHED_SAMPLING] [--num_batches_to_accumulate NUM_BATCHES_TO_ACCUMULATE] [--worker_num_splits WORKER_NUM_SPLITS] [--policy_workers_per_policy POLICY_WORKERS_PER_POLICY] [--max_policy_lag MAX_POLICY_LAG] [--num_workers NUM_WORKERS] [--num_envs_per_worker NUM_ENVS_PER_WORKER] [--batch_size BATCH_SIZE] [--num_batches_per_epoch NUM_BATCHES_PER_EPOCH] [--num_epochs NUM_EPOCHS] [--rollout ROLLOUT] [--recurrence RECURRENCE] [--shuffle_minibatches SHUFFLE_MINIBATCHES] [--gamma GAMMA] [--reward_scale REWARD_SCALE] [--reward_clip REWARD_CLIP] [--value_bootstrap VALUE_BOOTSTRAP] [--normalize_returns NORMALIZE_RETURNS] [--exploration_loss_coeff EXPLORATION_LOSS_COEFF] [--value_loss_coeff VALUE_LOSS_COEFF] [--kl_loss_coeff KL_LOSS_COEFF] [--exploration_loss {entropy,symmetric_kl}] [--gae_lambda GAE_LAMBDA] [--ppo_clip_ratio PPO_CLIP_RATIO] [--ppo_clip_value PPO_CLIP_VALUE] [--with_vtrace WITH_VTRACE] [--vtrace_rho VTRACE_RHO] [--vtrace_c VTRACE_C] [--optimizer {adam,lamb}] [--adam_eps ADAM_EPS] [--adam_beta1 ADAM_BETA1] [--adam_beta2 ADAM_BETA2] [--max_grad_norm MAX_GRAD_NORM] [--learning_rate LEARNING_RATE] [--lr_schedule {constant,kl_adaptive_minibatch,kl_adaptive_epoch}] [--lr_schedule_kl_threshold LR_SCHEDULE_KL_THRESHOLD] [--obs_subtract_mean OBS_SUBTRACT_MEAN] [--obs_scale OBS_SCALE] [--normalize_input NORMALIZE_INPUT] [--normalize_input_keys [NORMALIZE_INPUT_KEYS [NORMALIZE_INPUT_KEYS ...]]] [--decorrelate_experience_max_seconds DECORRELATE_EXPERIENCE_MAX_SECONDS] [--decorrelate_envs_on_one_worker DECORRELATE_ENVS_ON_ONE_WORKER] [--actor_worker_gpus [ACTOR_WORKER_GPUS [ACTOR_WORKER_GPUS ...]]] [--set_workers_cpu_affinity SET_WORKERS_CPU_AFFINITY] [--force_envs_single_thread FORCE_ENVS_SINGLE_THREAD] [--default_niceness DEFAULT_NICENESS] [--log_to_file LOG_TO_FILE] [--experiment_summaries_interval EXPERIMENT_SUMMARIES_INTERVAL] [--flush_summaries_interval FLUSH_SUMMARIES_INTERVAL] [--stats_avg STATS_AVG] [--summaries_use_frameskip SUMMARIES_USE_FRAMESKIP] [--heartbeat_interval HEARTBEAT_INTERVAL] [--heartbeat_reporting_interval HEARTBEAT_REPORTING_INTERVAL] [--train_for_env_steps TRAIN_FOR_ENV_STEPS] [--train_for_seconds TRAIN_FOR_SECONDS] [--save_every_sec SAVE_EVERY_SEC] [--keep_checkpoints KEEP_CHECKPOINTS] [--load_checkpoint_kind {latest,best}] [--save_milestones_sec SAVE_MILESTONES_SEC] [--save_best_every_sec SAVE_BEST_EVERY_SEC] [--save_best_metric SAVE_BEST_METRIC] [--save_best_after SAVE_BEST_AFTER] [--benchmark BENCHMARK] [--encoder_mlp_layers [ENCODER_MLP_LAYERS [ENCODER_MLP_LAYERS ...]]] [--encoder_conv_architecture {convnet_simple,convnet_impala,convnet_atari,resnet_impala}] [--encoder_conv_mlp_layers [ENCODER_CONV_MLP_LAYERS [ENCODER_CONV_MLP_LAYERS ...]]] [--use_rnn USE_RNN] [--rnn_size RNN_SIZE] [--rnn_type {gru,lstm}] [--rnn_num_layers RNN_NUM_LAYERS] [--decoder_mlp_layers [DECODER_MLP_LAYERS [DECODER_MLP_LAYERS ...]]] [--nonlinearity {elu,relu,tanh}] [--policy_initialization {orthogonal,xavier_uniform,torch_default}] [--policy_init_gain POLICY_INIT_GAIN] [--actor_critic_share_weights ACTOR_CRITIC_SHARE_WEIGHTS] [--adaptive_stddev ADAPTIVE_STDDEV] [--continuous_tanh_scale CONTINUOUS_TANH_SCALE] [--initial_stddev INITIAL_STDDEV] [--use_env_info_cache USE_ENV_INFO_CACHE] [--env_gpu_actions ENV_GPU_ACTIONS] [--env_gpu_observations ENV_GPU_OBSERVATIONS] [--env_frameskip ENV_FRAMESKIP] [--env_framestack ENV_FRAMESTACK] [--pixel_format PIXEL_FORMAT] [--use_record_episode_statistics USE_RECORD_EPISODE_STATISTICS] [--with_wandb WITH_WANDB] [--wandb_user WANDB_USER] [--wandb_project WANDB_PROJECT] [--wandb_group WANDB_GROUP] [--wandb_job_type WANDB_JOB_TYPE] [--wandb_tags [WANDB_TAGS [WANDB_TAGS ...]]] [--with_pbt WITH_PBT] [--pbt_mix_policies_in_one_env PBT_MIX_POLICIES_IN_ONE_ENV] [--pbt_period_env_steps PBT_PERIOD_ENV_STEPS] [--pbt_start_mutation PBT_START_MUTATION] [--pbt_replace_fraction PBT_REPLACE_FRACTION] [--pbt_mutation_rate PBT_MUTATION_RATE] [--pbt_replace_reward_gap PBT_REPLACE_REWARD_GAP] [--pbt_replace_reward_gap_absolute PBT_REPLACE_REWARD_GAP_ABSOLUTE] [--pbt_optimize_gamma PBT_OPTIMIZE_GAMMA] [--pbt_target_objective PBT_TARGET_OBJECTIVE] [--pbt_perturb_min PBT_PERTURB_MIN] [--pbt_perturb_max PBT_PERTURB_MAX] optional arguments: -h, --help Print the help message (default: False) --algo ALGO Algorithm to use (default: APPO) --env ENV Name of the environment to use (default: None) --experiment EXPERIMENT Unique experiment name. This will also be the name for the experiment folder in the train dir.If the experiment folder with this name aleady exists the experiment will be RESUMED!Any parameters passed from command line that do not match the parameters stored in the experiment config.json file will be overridden. (default: default_experiment) --train_dir TRAIN_DIR Root for all experiments (default: /home/runner/work/sample-factory/sample- factory/train_dir) --restart_behavior {resume,restart,overwrite} How to handle the experiment if the directory with the same name already exists. \"resume\" (default) will resume the experiment, \"restart\" will preserve the existing experiment folder under a different name (with \"old\" suffix) and will start training from scratch, \"overwrite\" will delete the existing experiment folder and start from scratch. This parameter does not have any effect if the experiment directory does not exist. (default: resume) --device {gpu,cpu} CPU training is only recommended for smaller e.g. MLP policies (default: gpu) --seed SEED Set a fixed seed value (default: None) --num_policies NUM_POLICIES Number of policies to train jointly, i.e. for multi- agent environments (default: 1) --async_rl ASYNC_RL Collect experience asynchronously while learning on the previous batch. This is significantly different from standard synchronous actor-critic (or PPO) because not all of the experience will be collected by the latest policy thus increasing policy lag. Negative effects of using async_rl can range from negligible (just grants you throughput boost) to quite serious where you can consider switching it off. It all depends how sensitive your experiment is to policy lag. Envs with complex action spaces and RNN policies tend to be particularly sensitive. (default: True) --serial_mode SERIAL_MODE Enable serial mode: run everything completely synchronously in the same process (default: False) --batched_sampling BATCHED_SAMPLING Batched sampling allows the data to be processed in big batches on the rollout worker.This is especially important for GPU-accelerated vectorized environments such as Megaverse or IsaacGym. As a downside, in batched mode we do not support (for now) some of the features, such as population-based self-play or inactive agents, plus each batched sampler (rollout worker) process only collects data for a single policy. Another issue between batched/non-batched sampling is handling of infos. In batched mode we assume that infos is a single dictionary of lists/tensors containing info for each environment in a vector. If you need some complex info dictionary handling and your environment might return dicts with different keys, on different rollout steps, you probably need non-batched mode. (default: False) --num_batches_to_accumulate NUM_BATCHES_TO_ACCUMULATE This parameter governs the maximum number of training batches the learner can accumulate before further experience collection is stopped. The default value will set this to 2, so if the experience collection is faster than the training, the learner will accumulate enough minibatches for 2 iterations of training but no more. This is a good balance between policy-lag and throughput. When the limit is reached, the learner will notify the actor workers that they ought to stop the experience collection until accumulated minibatches are processed. Set this parameter to 1 to further reduce policy-lag. If the experience collection is very non-uniform, increasing this parameter can increase overall throughput, at the cost of increased policy-lag. (default: 2) --worker_num_splits WORKER_NUM_SPLITS Typically we split a vector of envs into two parts for \"double buffered\" experience collection Set this to 1 to disable double buffering. Set this to 3 for triple buffering! (default: 2) --policy_workers_per_policy POLICY_WORKERS_PER_POLICY Number of policy workers that compute forward pass (per policy) (default: 1) --max_policy_lag MAX_POLICY_LAG Max policy lag in policy versions. Discard all experience that is older than this. (default: 1000) --num_workers NUM_WORKERS Number of parallel environment workers. Should be less than num_envs and should divide num_envs.Use this in async mode. (default: 2) --num_envs_per_worker NUM_ENVS_PER_WORKER Number of envs on a single CPU actor, in high- throughput configurations this should be in 10-30 range for Atari/VizDoomMust be even for double- buffered sampling! (default: 2) --batch_size BATCH_SIZE Minibatch size for SGD (default: 1024) --num_batches_per_epoch NUM_BATCHES_PER_EPOCH This determines the training dataset size for each iteration of training. We collect this many minibatches before performing any SGD. Example: if batch_size=128 and num_batches_per_epoch=2, then learner will process 2*128=256 environment transitions in one training iteration. (default: 1) --num_epochs NUM_EPOCHS Number of training epochs on a dataset of collected experiences of size batch_size x num_batches_per_epoch (default: 1) --rollout ROLLOUT Length of the rollout from each environment in timesteps.Once we collect this many timesteps on actor worker, we send this trajectory to the learner.The length of the rollout will determine how many timesteps are used to calculate bootstrappedMonte- Carlo estimates of discounted rewards, advantages, GAE, or V-trace targets. Shorter rolloutsreduce variance, but the estimates are less precise (bias vs variance tradeoff).For RNN policies, this should be a multiple of --recurrence, so every rollout will be splitinto (n = rollout / recurrence) segments for backpropagation. V-trace algorithm currently requires thatrollout == recurrence, which what you want most of the time anyway.Rollout length is independent from the episode length. Episode length can be both shorter or longer thanrollout, although for PBT training it is currently recommended that rollout << episode_len(see function finalize_trajectory in actor_worker.py) (default: 32) --recurrence RECURRENCE Trajectory length for backpropagation through time. If recurrence=1 there is no backpropagation through time, and experience is shuffled completely randomlyFor V-trace recurrence should be equal to rollout length. (default: 32) --shuffle_minibatches SHUFFLE_MINIBATCHES Whether to randomize and shuffle minibatches between iterations (this is a slow operation when batches are large, disabling this increases learner throughput when training with multiple epochs/minibatches per epoch) (default: False) --gamma GAMMA Discount factor (default: 0.99) --reward_scale REWARD_SCALE Multiply all rewards by this factor before feeding into RL algorithm.Sometimes the overall scale of rewards is too high which makes value estimation a harder regression task.Loss values become too high which requires a smaller learning rate, etc. (default: 1.0) --reward_clip REWARD_CLIP Clip rewards between [-c, c]. Default [-1000, 1000] should mean no clipping for most envs (unless rewards are very large/small) (default: 1000.0) --value_bootstrap VALUE_BOOTSTRAP Bootstrap returns from value estimates if episode is terminated by timeout. More info here: https://github.com/Denys88/rl_games/issues/128 (default: False) --normalize_returns NORMALIZE_RETURNS Whether to use running mean and standard deviation to normalize discounted returns (default: True) --exploration_loss_coeff EXPLORATION_LOSS_COEFF Coefficient for the exploration component of the loss function. (default: 0.003) --value_loss_coeff VALUE_LOSS_COEFF Coefficient for the critic loss (default: 0.5) --kl_loss_coeff KL_LOSS_COEFF Coefficient for fixed KL loss (as used by Schulman et al. in https://arxiv.org/pdf/1707.06347.pdf). Highly recommended for environments with continuous action spaces. (default: 0.0) --exploration_loss {entropy,symmetric_kl} Usually the exploration loss is based on maximizing the entropy of the probability distribution. Note that mathematically maximizing entropy of the categorical probability distribution is exactly the same as minimizing the (regular) KL-divergence between this distribution and a uniform prior. The downside of using the entropy term (or regular asymmetric KL- divergence) is the fact that penalty does not increase as probabilities of some actions approach zero. I.e. numerically, there is almost no difference between an action distribution with a probability epsilon > 0 for some action and an action distribution with a probability = zero for this action. For many tasks the first (epsilon) distribution is preferrable because we keep some (albeit small) amount of exploration, while the second distribution will never explore this action ever again.Unlike the entropy term, symmetric KL divergence between the action distribution and a uniform prior approaches infinity when entropy of the distribution approaches zero, so it can prevent the pathological situations where the agent stops exploring. Empirically, symmetric KL-divergence yielded slightly better results on some problems. (default: entropy) --gae_lambda GAE_LAMBDA Generalized Advantage Estimation discounting (only used when V-trace is False) (default: 0.95) --ppo_clip_ratio PPO_CLIP_RATIO We use unbiased clip(x, 1+e, 1/(1+e)) instead of clip(x, 1+e, 1-e) in the paper (default: 0.1) --ppo_clip_value PPO_CLIP_VALUE Maximum absolute change in value estimate until it is clipped. Sensitive to value magnitude (default: 1.0) --with_vtrace WITH_VTRACE Enables V-trace off-policy correction. If this is True, then GAE is not used (default: False) --vtrace_rho VTRACE_RHO rho_hat clipping parameter of the V-trace algorithm (importance sampling truncation) (default: 1.0) --vtrace_c VTRACE_C c_hat clipping parameter of the V-trace algorithm. Low values for c_hat can reduce variance of the advantage estimates (similar to GAE lambda < 1) (default: 1.0) --optimizer {adam,lamb} Type of optimizer to use (default: adam) --adam_eps ADAM_EPS Adam epsilon parameter (1e-8 to 1e-5 seem to reliably work okay, 1e-3 and up does not work) (default: 1e-06) --adam_beta1 ADAM_BETA1 Adam momentum decay coefficient (default: 0.9) --adam_beta2 ADAM_BETA2 Adam second momentum decay coefficient (default: 0.999) --max_grad_norm MAX_GRAD_NORM Max L2 norm of the gradient vector, set to 0 to disable gradient clipping (default: 4.0) --learning_rate LEARNING_RATE LR (default: 0.0001) --lr_schedule {constant,kl_adaptive_minibatch,kl_adaptive_epoch} Learning rate schedule to use. Constant keeps constant learning rate throughout training.kl_adaptive* schedulers look at --lr_schedule_kl_threshold and if KL-divergence with behavior policyafter the last minibatch/epoch significantly deviates from this threshold, lr is apropriatelyincreased or decreased (default: constant) --lr_schedule_kl_threshold LR_SCHEDULE_KL_THRESHOLD Used with kl_adaptive_* schedulers (default: 0.008) --obs_subtract_mean OBS_SUBTRACT_MEAN Observation preprocessing, mean value to subtract from observation (e.g. 128.0 for 8-bit RGB) (default: 0.0) --obs_scale OBS_SCALE Observation preprocessing, divide observation tensors by this scalar (e.g. 128.0 for 8-bit RGB) (default: 1.0) --normalize_input NORMALIZE_INPUT Whether to use running mean and standard deviation to normalize observations (default: True) --normalize_input_keys [NORMALIZE_INPUT_KEYS [NORMALIZE_INPUT_KEYS ...]] Which observation keys to use for normalization. If None, all observation keys are used (be careful with this!) (default: None) --decorrelate_experience_max_seconds DECORRELATE_EXPERIENCE_MAX_SECONDS Decorrelating experience serves two benefits. First: this is better for learning because samples from workers come from random moments in the episode, becoming more \"i.i.d\".Second, and more important one: this is good for environments with highly non-uniform one-step times, including long and expensive episode resets. If experience is not decorrelatedthen training batches will come in bursts e.g. after a bunch of environments finished resets and many iterations on the learner might be required,which will increase the policy-lag of the new experience collected. The performance of the Sample Factory is best when experience is generated as more-or-lessuniform stream. Try increasing this to 100-200 seconds to smoothen the experience distribution in time right from the beginning (it will eventually spread out and settle anyways) (default: 0) --decorrelate_envs_on_one_worker DECORRELATE_ENVS_ON_ONE_WORKER In addition to temporal decorrelation of worker processes, also decorrelate envs within one worker process. For environments with a fixed episode length it can prevent the reset from happening in the same rollout for all envs simultaneously, which makes experience collection more uniform. (default: True) --actor_worker_gpus [ACTOR_WORKER_GPUS [ACTOR_WORKER_GPUS ...]] By default, actor workers only use CPUs. Changes this if e.g. you need GPU-based rendering on the actors (default: []) --set_workers_cpu_affinity SET_WORKERS_CPU_AFFINITY Whether to assign workers to specific CPU cores or not. The logic is beneficial for most workloads because prevents a lot of context switching.However for some environments it can be better to disable it, to allow one worker to use all cores some of the time. This can be the case for some DMLab environments with very expensive episode resetthat can use parallel CPU cores for level generation. (default: True) --force_envs_single_thread FORCE_ENVS_SINGLE_THREAD Some environments may themselves use parallel libraries such as OpenMP or MKL. Since we parallelize environments on the level of workers, there is no need to keep this parallel semantic.This flag uses threadpoolctl to force libraries such as OpenMP and MKL to use only a single thread within the environment.Enabling this is recommended unless you are running fewer workers than CPU cores. threadpoolctl has caused a bunch of crashes in the past, so this feature is disabled by default at this moment. (default: False) --default_niceness DEFAULT_NICENESS Niceness of the highest priority process (the learner). Values below zero require elevated privileges. (default: 0) --log_to_file LOG_TO_FILE Whether to log to a file (sf_log.txt in the experiment folder) or not. If False, logs to stdout only. It can make sense to disable this in a slow server filesystem environment like NFS. (default: True) --experiment_summaries_interval EXPERIMENT_SUMMARIES_INTERVAL How often in seconds we write avg. statistics about the experiment (reward, episode length, extra stats...) (default: 10) --flush_summaries_interval FLUSH_SUMMARIES_INTERVAL How often do we flush tensorboard summaries (set to higher value for slow NFS-based server filesystems) (default: 30) --stats_avg STATS_AVG How many episodes to average to measure performance (avg. reward etc) (default: 100) --summaries_use_frameskip SUMMARIES_USE_FRAMESKIP Whether to multiply training steps by frameskip when recording summaries, FPS, etc. When this flag is set to True, x-axis for all summaries corresponds to the total number of simulated steps, i.e. with frameskip=4 the x-axis value of 4 million will correspond to 1 million frames observed by the policy. (default: True) --heartbeat_interval HEARTBEAT_INTERVAL How often in seconds components send a heartbeat signal to the runner to verify they are not stuck (default: 20) --heartbeat_reporting_interval HEARTBEAT_REPORTING_INTERVAL How often in seconds the runner checks for heartbeats (default: 180) --train_for_env_steps TRAIN_FOR_ENV_STEPS Stop after all policies are trained for this many env steps (default: 10000000000) --train_for_seconds TRAIN_FOR_SECONDS Stop training after this many seconds (default: 10000000000) --save_every_sec SAVE_EVERY_SEC Checkpointing rate (default: 120) --keep_checkpoints KEEP_CHECKPOINTS Number of model checkpoints to keep (default: 2) --load_checkpoint_kind {latest,best} Whether to load from latest or best checkpoint (default: latest) --save_milestones_sec SAVE_MILESTONES_SEC Save intermediate checkpoints in a separate folder for later evaluation (default=never) (default: -1) --save_best_every_sec SAVE_BEST_EVERY_SEC How often we check if we should save the policy with the best score ever (default: 5) --save_best_metric SAVE_BEST_METRIC Save \"best\" policies based on this metric (just env reward by default) (default: reward) --save_best_after SAVE_BEST_AFTER Start saving \"best\" policies after this many env steps to filter lucky episodes that succeed and dominate the statistics early on (default: 100000) --benchmark BENCHMARK Benchmark mode (default: False) --encoder_mlp_layers [ENCODER_MLP_LAYERS [ENCODER_MLP_LAYERS ...]] In case of MLP encoder, sizes of layers to use. This is ignored if observations are images. (default: [512, 512]) --encoder_conv_architecture {convnet_simple,convnet_impala,convnet_atari,resnet_impala} Architecture of the convolutional encoder. See models.py for details. VizDoom and DMLab examples demonstrate how to define custom architectures. (default: convnet_simple) --encoder_conv_mlp_layers [ENCODER_CONV_MLP_LAYERS [ENCODER_CONV_MLP_LAYERS ...]] Optional fully connected layers after the convolutional encoder head. (default: [512]) --use_rnn USE_RNN Whether to use RNN core in a policy or not (default: True) --rnn_size RNN_SIZE Size of the RNN hidden state in recurrent model (e.g. GRU or LSTM) (default: 512) --rnn_type {gru,lstm} Type of RNN cell to use if use_rnn is True (default: gru) --rnn_num_layers RNN_NUM_LAYERS Number of RNN layers to use if use_rnn is True (default: 1) --decoder_mlp_layers [DECODER_MLP_LAYERS [DECODER_MLP_LAYERS ...]] Optional decoder MLP layers after the policy core. If empty (default) decoder is identity function. (default: []) --nonlinearity {elu,relu,tanh} Type of nonlinearity to use. (default: elu) --policy_initialization {orthogonal,xavier_uniform,torch_default} NN weight initialization (default: orthogonal) --policy_init_gain POLICY_INIT_GAIN Gain parameter of PyTorch initialization schemas (i.e. Xavier) (default: 1.0) --actor_critic_share_weights ACTOR_CRITIC_SHARE_WEIGHTS Whether to share the weights between policy and value function (default: True) --adaptive_stddev ADAPTIVE_STDDEV Only for continuous action distributions, whether stddev is state-dependent or just a single learned parameter (default: True) --continuous_tanh_scale CONTINUOUS_TANH_SCALE Only for continuous action distributions, whether to use tanh squashing and what scale to use. Applies tanh(mu / scale) * scale to distribution means. Experimental. Currently only works with adaptive_stddev=False (TODO). (default: 0.0) --initial_stddev INITIAL_STDDEV Initial value for non-adaptive stddev. Only makes sense for continuous action spaces (default: 1.0) --use_env_info_cache USE_ENV_INFO_CACHE Whether to use cached env info (default: False) --env_gpu_actions ENV_GPU_ACTIONS Set to true if environment expects actions on GPU (i.e. as a GPU-side PyTorch tensor) (default: False) --env_gpu_observations ENV_GPU_OBSERVATIONS Setting this to True together with non-empty --actor_worker_gpus will make observations GPU-side PyTorch tensors. Otherwise data will be on CPU. For CPU-based envs just set --actor_worker_gpus to empty list then this parameter does not matter. (default: True) --env_frameskip ENV_FRAMESKIP Number of frames for action repeat (frame skipping). Setting this to >1 will not add any wrappers that will do frame-skipping, although this can be used in the environment factory function to add these wrappers or to tell the environment itself to skip a desired number of frames i.e. as it is done in VizDoom. FPS metrics will be multiplied by the frameskip value, i.e. 100000FPS with frameskip=4 actually corresponds to 100000/4=25000 samples per second observed by the policy. Frameskip=1 (default) means no frameskip, we process every frame. (default: 1) --env_framestack ENV_FRAMESTACK Frame stacking (only used in Atari, and it is usually set to 4) (default: 1) --pixel_format PIXEL_FORMAT PyTorch expects CHW by default, Ray & TensorFlow expect HWC (default: CHW) --use_record_episode_statistics USE_RECORD_EPISODE_STATISTICS Whether to use gym RecordEpisodeStatistics wrapper to keep track of reward (default: False) --with_wandb WITH_WANDB Enables Weights and Biases integration (default: False) --wandb_user WANDB_USER WandB username (entity). Must be specified from command line! Also see https://docs.wandb.ai/quickstart#1.-set-up-wandb (default: None) --wandb_project WANDB_PROJECT WandB \"Project\" (default: sample_factory) --wandb_group WANDB_GROUP WandB \"Group\" (to group your experiments). By default this is the name of the env. (default: None) --wandb_job_type WANDB_JOB_TYPE WandB job type (default: SF) --wandb_tags [WANDB_TAGS [WANDB_TAGS ...]] Tags can help with finding experiments in WandB web console (default: []) --with_pbt WITH_PBT Enables population-based training (PBT) (default: False) --pbt_mix_policies_in_one_env PBT_MIX_POLICIES_IN_ONE_ENV For multi-agent envs, whether we mix different policies in one env. (default: True) --pbt_period_env_steps PBT_PERIOD_ENV_STEPS Periodically replace the worst policies with the best ones and perturb the hyperparameters (default: 5000000) --pbt_start_mutation PBT_START_MUTATION Allow initial diversification, start PBT after this many env steps (default: 20000000) --pbt_replace_fraction PBT_REPLACE_FRACTION A portion of policies performing worst to be replace by better policies (rounded up) (default: 0.3) --pbt_mutation_rate PBT_MUTATION_RATE Probability that a parameter mutates (default: 0.15) --pbt_replace_reward_gap PBT_REPLACE_REWARD_GAP Relative gap in true reward when replacing weights of the policy with a better performing one (default: 0.1) --pbt_replace_reward_gap_absolute PBT_REPLACE_REWARD_GAP_ABSOLUTE Absolute gap in true reward when replacing weights of the policy with a better performing one (default: 1e-06) --pbt_optimize_gamma PBT_OPTIMIZE_GAMMA Whether to optimize gamma, discount factor, or not (experimental) (default: False) --pbt_target_objective PBT_TARGET_OBJECTIVE Policy stat to optimize with PBT. true_objective (default) is equal to raw env reward if not specified, but can also be any other per-policy stat.For DMlab-30 use value \"dmlab_target_objective\" (which is capped human normalized score) (default: true_objective) --pbt_perturb_min PBT_PERTURB_MIN When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default: 1.1) --pbt_perturb_max PBT_PERTURB_MAX When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default: 1.5)","title":"Full Parameter Reference"},{"location":"02-configuration/configuration/","text":"Configuration \u00b6 Sample Factory experiments are configured via command line parameters. The following command will print the help message for the algorithm-environment combination containing the list of all parameters, their descriptions, and their default values: python -m sf_examples.train_gym_env --env = CartPole-v1 --help (replace train_gym_env with your own training script name and CartPole-v1 with a different environment name to get information about parameters specific to this particular environment). Default parameter values and their help strings are defined in sample_factory/cfg/cfg.py . Besides that, additional parameters can be defined in specific environment integrations, for example in sf_examples/envpool/mujoco/envpool_mujoco_params.py . config.json \u00b6 Once the new experiment is started, a directory containing experiment-related files is created in --train_dir location (or ./train_dir in cwd if --train_dir is not passed from command line). This directory contains a file config.json where all the experiment parameters are saved (including those instantiated from their default values). In addition to that, selected parameter values are printed to the console and thus are saved to sf_log.txt file in the experiment directory. Running an experiment and then stopping it to check the parameter values is a good practice to make sure that the experiment is configured as expected. Key parameters \u00b6 --env (required) full name that uniquely identifies the environment as it is registered in the environment registry (see register_env() function). --experiment a name that uniquely identifies the experiment and the experiment folder. E.g. --experiment=my_experiment . If the experiment folder with the name already exists the experiment (by default) will be resumed ! Resuming experiments after a stop is the default behavior in Sample Factory. When the experiment is resumed from command line are taken into account, unspecified parameters will be loaded from the existing experiment config.json file. If you want to start a new experiment, delete the old experiment folder or change the experiment name. You can also use --restart_behavior=[resume|restart|overwrite] to control this behavior. --train_dir location for all experiments folders, defaults to ./train_dir . --num_workers defaults to number of logical cores in the system, which will give the best throughput in most scenarios. --num_envs_per_worker will greatly affect the performance. Large values (15-30) improve hardware utilization but increase memory usage and policy lag. Must be even for the double-buffered sampling to work. Disable double-buffered sampling by setting --worker_num_splits=1 to use odd number of envs per worker (e.g. 1 env per worker). (Default: 2) A good rule of thumb is to set this to relatively low value (e.g. 4 or 8 for common envs) and then increase it until you see no more performance improvements or you start losing sample efficiency due to the policy lag . --rollout is the length of trajectory collected by each agent. --batch_size is the minibatch size for SGD. --num_batches_per_epoch is the number of minibatches the training batch (dataset) is split into. --num_epochs is the number of epochs on the learner over one training batch (dataset). The above six parameters ( batch_size, num_batches_per_epoch, rollout, num_epochs, num_workers, num_envs_per_worker ) have the biggest influence on the data regime of the RL algorithm and thus on the sample efficiency and the training speed. num_workers , num_envs_per_worker , and rollout define how many samples are collected per iteration (one rollout for all envs), which is sampling_size = num_workers * num_envs_per_worker * rollout (note that this is further multiplied by env's num_agents for multi-agent envs). batch_size and num_batches_per_epoch define how many samples are used for training per iteration. If sampling_size >> batch_size then we will need many iterations of training to go through the data, which will make some experience stale by the time it is used for training ( policy lag ). See Policy Lag for additional information. Evaluation script parameters \u00b6 Evaluation scripts (i.e. sf_examples/atari/enjoy_atari.py ) use the same configuration parameters as training scripts for simplicity, although of course many of them are ignored as they don't affect evaluation. In addition to that, evaluation scripts provide additional parameters, see add_eval_args() in sample_factory/cfg/cfg.py . HuggingFace Hub integration guide provides a good overview of the important parameters such as --save_video , check it out! Full list of parameters \u00b6 Please see the Full Parameter Reference auto-generated using the --help flag for the full list of available command line arguments.","title":"Configuration"},{"location":"02-configuration/configuration/#configuration","text":"Sample Factory experiments are configured via command line parameters. The following command will print the help message for the algorithm-environment combination containing the list of all parameters, their descriptions, and their default values: python -m sf_examples.train_gym_env --env = CartPole-v1 --help (replace train_gym_env with your own training script name and CartPole-v1 with a different environment name to get information about parameters specific to this particular environment). Default parameter values and their help strings are defined in sample_factory/cfg/cfg.py . Besides that, additional parameters can be defined in specific environment integrations, for example in sf_examples/envpool/mujoco/envpool_mujoco_params.py .","title":"Configuration"},{"location":"02-configuration/configuration/#configjson","text":"Once the new experiment is started, a directory containing experiment-related files is created in --train_dir location (or ./train_dir in cwd if --train_dir is not passed from command line). This directory contains a file config.json where all the experiment parameters are saved (including those instantiated from their default values). In addition to that, selected parameter values are printed to the console and thus are saved to sf_log.txt file in the experiment directory. Running an experiment and then stopping it to check the parameter values is a good practice to make sure that the experiment is configured as expected.","title":"config.json"},{"location":"02-configuration/configuration/#key-parameters","text":"--env (required) full name that uniquely identifies the environment as it is registered in the environment registry (see register_env() function). --experiment a name that uniquely identifies the experiment and the experiment folder. E.g. --experiment=my_experiment . If the experiment folder with the name already exists the experiment (by default) will be resumed ! Resuming experiments after a stop is the default behavior in Sample Factory. When the experiment is resumed from command line are taken into account, unspecified parameters will be loaded from the existing experiment config.json file. If you want to start a new experiment, delete the old experiment folder or change the experiment name. You can also use --restart_behavior=[resume|restart|overwrite] to control this behavior. --train_dir location for all experiments folders, defaults to ./train_dir . --num_workers defaults to number of logical cores in the system, which will give the best throughput in most scenarios. --num_envs_per_worker will greatly affect the performance. Large values (15-30) improve hardware utilization but increase memory usage and policy lag. Must be even for the double-buffered sampling to work. Disable double-buffered sampling by setting --worker_num_splits=1 to use odd number of envs per worker (e.g. 1 env per worker). (Default: 2) A good rule of thumb is to set this to relatively low value (e.g. 4 or 8 for common envs) and then increase it until you see no more performance improvements or you start losing sample efficiency due to the policy lag . --rollout is the length of trajectory collected by each agent. --batch_size is the minibatch size for SGD. --num_batches_per_epoch is the number of minibatches the training batch (dataset) is split into. --num_epochs is the number of epochs on the learner over one training batch (dataset). The above six parameters ( batch_size, num_batches_per_epoch, rollout, num_epochs, num_workers, num_envs_per_worker ) have the biggest influence on the data regime of the RL algorithm and thus on the sample efficiency and the training speed. num_workers , num_envs_per_worker , and rollout define how many samples are collected per iteration (one rollout for all envs), which is sampling_size = num_workers * num_envs_per_worker * rollout (note that this is further multiplied by env's num_agents for multi-agent envs). batch_size and num_batches_per_epoch define how many samples are used for training per iteration. If sampling_size >> batch_size then we will need many iterations of training to go through the data, which will make some experience stale by the time it is used for training ( policy lag ). See Policy Lag for additional information.","title":"Key parameters"},{"location":"02-configuration/configuration/#evaluation-script-parameters","text":"Evaluation scripts (i.e. sf_examples/atari/enjoy_atari.py ) use the same configuration parameters as training scripts for simplicity, although of course many of them are ignored as they don't affect evaluation. In addition to that, evaluation scripts provide additional parameters, see add_eval_args() in sample_factory/cfg/cfg.py . HuggingFace Hub integration guide provides a good overview of the important parameters such as --save_video , check it out!","title":"Evaluation script parameters"},{"location":"02-configuration/configuration/#full-list-of-parameters","text":"Please see the Full Parameter Reference auto-generated using the --help flag for the full list of available command line arguments.","title":"Full list of parameters"},{"location":"03-customization/custom-environments/","text":"Custom environments \u00b6 Training agents in your own environment with Sample Factory is straightforward, but if you get stuck feel free to raise an issue on our GitHub Page . We recommend looking at our example environment integrations such as Atari or MuJoCo before using your own environment. Custom environment template \u00b6 In order to integrate your own environment with Sample Factory, the following steps are required: Define entry points for training and evaluation scripts, such as train_custom_env.py and enjoy_custom_env.py . Define a method that creates an instance of your environment, such as make_custom_env() . Override any default parameters that are specific to your environment, this way you can avoid passing them from the command line (optional). Add any custom parameters that will be parsed by Sample Factory alongside the default parameters (optional). We provide the following template, which you can modify to intergrate your environment. We assume your environment conforms to the gym 0.26 API (5-tuple). from typing import Optional from sample_factory.cfg.arguments import parse_full_cfg , parse_sf_args from sample_factory.envs.env_utils import register_env from sample_factory.train import run_rl def make_custom_env_func ( full_env_name : str , cfg = None , env_config = None , render_mode : Optional [ str ] = None ): # see the section below explaining arguments return CustomEnv ( full_env_name , cfg , env_config , render_mode = render_mode ) def register_custom_env_envs (): # register the env in sample-factory's global env registry # after this, you can use the env in the command line using --env=custom_env_name register_env ( \"custom_env_name\" , make_custom_env ) def add_custom_env_args ( _env , p : argparse . ArgumentParser , evaluation = False ): # You can extend the command line arguments here p . add_argument ( \"--custom_argument\" , default = \"value\" , type = str , help = \"\" ) def custom_env_override_defaults ( _env , parser ): # Modify the default arguments when using this env. # These can still be changed from the command line. See configuration guide for more details. parser . set_defaults ( encoder_conv_architecture = \"convnet_atari\" , obs_scale = 255.0 , gamma = 0.99 , learning_rate = 0.00025 , lr_schedule = \"linear_decay\" , adam_eps = 1e-5 , ) def parse_args ( argv = None , evaluation = False ): # parse the command line arguments to build parser , partial_cfg = parse_sf_args ( argv = argv , evaluation = evaluation ) add_custom_env_args ( partial_cfg . env , parser , evaluation = evaluation ) custom_env_override_defaults ( partial_cfg . env , parser ) final_cfg = parse_full_cfg ( parser , argv ) return final_cfg def main (): \"\"\"Script entry point.\"\"\" register_custom_env_envs () cfg = parse_args () status = run_rl ( cfg ) return status if __name__ == \"__main__\" : sys . exit ( main ()) Training can now be started with python train_custom_env.py --env=custom_env_name --experiment=CustomEnv . Note that this train script can be defined in your own codebase, or in the Sample Factory codebase (in case you forked the repo). Environment factory function parameters \u00b6 register_env(\"custom_env_name\", make_custom_env) expects make_custom_env to be a Callable with the following signature: def make_custom_env_func ( full_env_name : str , cfg : Optional [ Config ] = None , env_config : Optional [ AttrDict ] = None , render_mode : Optional [ str ] = None ) -> Env Arguments: * full_env_name : complete name of the environment as passed in the command line with --env * cfg : full system configuration, output of argparser. Normally this is an AttrDict (dictionary where keys can be accessed as attributes) * env_config : AttrDict with additional system information, for example: env_config = AttrDict(worker_index=worker_idx, vector_index=vector_idx, env_id=env_id) Some custom environments will require this information, i.e. env_id is a unique identifier for each environment instance in 0..num_envs-1 range. * render_mode : if not None, environment will be rendered in this mode (e.g. 'human', 'rgb_array'). New parameter required after Gym 0.26. See sample_factory/envs/create_env.py for more details. Evaluation script template \u00b6 The evaluation script template is even more straightforward. Note that we just reuse functions already defined in the training script. import sys from sample_factory.enjoy import enjoy from train_custom_env import parse_args , register_custom_env_envs def main (): \"\"\"Script entry point.\"\"\" register_custom_env_envs () cfg = parse_args ( evaluation = True ) status = enjoy ( cfg ) return status if __name__ == \"__main__\" : sys . exit ( main ()) You can now run evaluation with python enjoy_custom_env.py --env=custom_env_name --experiment=CustomEnv to measure the performance of the trained model, visualize agent's performance, or record a video file. Examples \u00b6 sf_examples/train_custom_env_custom_model.py - integrates an entirely custom toy environment. sf_examples/train_gym_env.py - trains an agent in a Gym environment. Environments registered in gym do not get any special treatment, as it is just another way to define an environment. In this case the environment creation function reduces to gym.make(env_name) . See environment integrations in sf_examples/<env_name> for additional examples.","title":"Custom environments"},{"location":"03-customization/custom-environments/#custom-environments","text":"Training agents in your own environment with Sample Factory is straightforward, but if you get stuck feel free to raise an issue on our GitHub Page . We recommend looking at our example environment integrations such as Atari or MuJoCo before using your own environment.","title":"Custom environments"},{"location":"03-customization/custom-environments/#custom-environment-template","text":"In order to integrate your own environment with Sample Factory, the following steps are required: Define entry points for training and evaluation scripts, such as train_custom_env.py and enjoy_custom_env.py . Define a method that creates an instance of your environment, such as make_custom_env() . Override any default parameters that are specific to your environment, this way you can avoid passing them from the command line (optional). Add any custom parameters that will be parsed by Sample Factory alongside the default parameters (optional). We provide the following template, which you can modify to intergrate your environment. We assume your environment conforms to the gym 0.26 API (5-tuple). from typing import Optional from sample_factory.cfg.arguments import parse_full_cfg , parse_sf_args from sample_factory.envs.env_utils import register_env from sample_factory.train import run_rl def make_custom_env_func ( full_env_name : str , cfg = None , env_config = None , render_mode : Optional [ str ] = None ): # see the section below explaining arguments return CustomEnv ( full_env_name , cfg , env_config , render_mode = render_mode ) def register_custom_env_envs (): # register the env in sample-factory's global env registry # after this, you can use the env in the command line using --env=custom_env_name register_env ( \"custom_env_name\" , make_custom_env ) def add_custom_env_args ( _env , p : argparse . ArgumentParser , evaluation = False ): # You can extend the command line arguments here p . add_argument ( \"--custom_argument\" , default = \"value\" , type = str , help = \"\" ) def custom_env_override_defaults ( _env , parser ): # Modify the default arguments when using this env. # These can still be changed from the command line. See configuration guide for more details. parser . set_defaults ( encoder_conv_architecture = \"convnet_atari\" , obs_scale = 255.0 , gamma = 0.99 , learning_rate = 0.00025 , lr_schedule = \"linear_decay\" , adam_eps = 1e-5 , ) def parse_args ( argv = None , evaluation = False ): # parse the command line arguments to build parser , partial_cfg = parse_sf_args ( argv = argv , evaluation = evaluation ) add_custom_env_args ( partial_cfg . env , parser , evaluation = evaluation ) custom_env_override_defaults ( partial_cfg . env , parser ) final_cfg = parse_full_cfg ( parser , argv ) return final_cfg def main (): \"\"\"Script entry point.\"\"\" register_custom_env_envs () cfg = parse_args () status = run_rl ( cfg ) return status if __name__ == \"__main__\" : sys . exit ( main ()) Training can now be started with python train_custom_env.py --env=custom_env_name --experiment=CustomEnv . Note that this train script can be defined in your own codebase, or in the Sample Factory codebase (in case you forked the repo).","title":"Custom environment template"},{"location":"03-customization/custom-environments/#environment-factory-function-parameters","text":"register_env(\"custom_env_name\", make_custom_env) expects make_custom_env to be a Callable with the following signature: def make_custom_env_func ( full_env_name : str , cfg : Optional [ Config ] = None , env_config : Optional [ AttrDict ] = None , render_mode : Optional [ str ] = None ) -> Env Arguments: * full_env_name : complete name of the environment as passed in the command line with --env * cfg : full system configuration, output of argparser. Normally this is an AttrDict (dictionary where keys can be accessed as attributes) * env_config : AttrDict with additional system information, for example: env_config = AttrDict(worker_index=worker_idx, vector_index=vector_idx, env_id=env_id) Some custom environments will require this information, i.e. env_id is a unique identifier for each environment instance in 0..num_envs-1 range. * render_mode : if not None, environment will be rendered in this mode (e.g. 'human', 'rgb_array'). New parameter required after Gym 0.26. See sample_factory/envs/create_env.py for more details.","title":"Environment factory function parameters"},{"location":"03-customization/custom-environments/#evaluation-script-template","text":"The evaluation script template is even more straightforward. Note that we just reuse functions already defined in the training script. import sys from sample_factory.enjoy import enjoy from train_custom_env import parse_args , register_custom_env_envs def main (): \"\"\"Script entry point.\"\"\" register_custom_env_envs () cfg = parse_args ( evaluation = True ) status = enjoy ( cfg ) return status if __name__ == \"__main__\" : sys . exit ( main ()) You can now run evaluation with python enjoy_custom_env.py --env=custom_env_name --experiment=CustomEnv to measure the performance of the trained model, visualize agent's performance, or record a video file.","title":"Evaluation script template"},{"location":"03-customization/custom-environments/#examples","text":"sf_examples/train_custom_env_custom_model.py - integrates an entirely custom toy environment. sf_examples/train_gym_env.py - trains an agent in a Gym environment. Environments registered in gym do not get any special treatment, as it is just another way to define an environment. In this case the environment creation function reduces to gym.make(env_name) . See environment integrations in sf_examples/<env_name> for additional examples.","title":"Examples"},{"location":"03-customization/custom-models/","text":"Custom models \u00b6 Adding custom models in Sample Factory is simple, but if you get stuck feel free to raise an issue on our GitHub Page . Actor Critic models in Sample Factory \u00b6 Actor Critic models in Sample Factory are composed of three components: Encoder - Process input observations (images, vectors) and map them to a vector. This is the part of the model you will most likely want to customize. Core - Intergrate vectors from one or more encoders, can optionally include a single- or multi-layer LSTM/GRU in a memory-based agent. Decoder - Apply additional layers to the output of the model core before computing the policy and value outputs. Regardless of the component customization, you can use your resulting model in \"shared weights\" or \"separate weights\" regime (either sharing or not sharing the weights between the policy and value networks). This is controlled by the --actor_critic_share_weights=[True|False] command line argument. On top of that, you can register an entire custom Actor Critic model. This can be useful for more complex models, for example centralized critic for multi-agent envs, asymmetric actor-critic where critic observes more information, which can be useful in sim-to-real, and so on. Custom model template \u00b6 The following template demonstrates how different components of the model can be customized. Feel free to combine this with the custom environment template above to create a fully custom environment & model combination. from sample_factory.model.encoder import Encoder from sample_factory.model.decoder import Decoder from sample_factory.model.core import ModelCore from sample_factory.model.actor_critic import ActorCritic from sample_factory.algo.utils.context import global_model_factory class CustomEncoder ( Encoder ): def __init__ ( self , cfg : Config , obs_space : ObsSpace ): super () . __init__ ( cfg ) # build custom encoder architecture ... def forward ( self , obs_dict ): # custom forward logic ... class CustomCore ( ModelCore ): def __init__ ( self , cfg : Config , input_size : int ): super () . __init__ ( cfg ) # build custom core architecture ... def forward ( self , head_output , rnn_states ): # custom forward logic ... class CustomDecoder ( Decoder ): def __init__ ( self , cfg : Config , decoder_input_size : int ): super () . __init__ ( cfg ) # build custom decoder architecture ... def forward ( self , core_output ): # custom forward logic ... class CustomActorCritic ( ActorCritic ): def __init__ ( self , model_factory , obs_space : ObsSpace , action_space : ActionSpace , cfg : Config , ): super () . __init__ ( obs_space , action_space , cfg ) self . encoder = CustomEncoder ( cfg , obs_space ) self . core = CustomCore ( cfg , self . encoder . get_out_size ()) self . decoder = CustomDecoder ( cfg , self . core . get_out_size ()) self . critic_linear = nn . Linear ( self . decoder . get_out_size ()) self . action_parameterization = self . get_action_parameterization ( self . decoder . get_out_size () ) def forward ( self , normalized_obs_dict , rnn_states , values_only = False ): # forward logic ... def register_model_components (): # register custom components with the factory # you can register an entire Actor Critic model global_model_factory () . register_actor_critic_factory ( CustomActorCritic ) # or individual components global_model_factory () . register_encoder_factory ( CustomEncoder ) global_model_factory () . register_core_factory ( CustomCore ) global_model_factory () . register_decoder_factory ( CustomDecoder ) def main (): \"\"\"Script entry point.\"\"\" register_model_components () cfg = parse_args () status = run_rl ( cfg ) return status if __name__ == \"__main__\" : sys . exit ( main ()) Examples \u00b6 Examples of model customizations can be found in: sf_examples/train_custom_env_custom_model.py sf_examples/isaacgym_examples/train_isaacgym.py sf_examples/dmlab/dmlab_model.py sf_examples/vizdoom/doom/doom_model.py","title":"Custom models"},{"location":"03-customization/custom-models/#custom-models","text":"Adding custom models in Sample Factory is simple, but if you get stuck feel free to raise an issue on our GitHub Page .","title":"Custom models"},{"location":"03-customization/custom-models/#actor-critic-models-in-sample-factory","text":"Actor Critic models in Sample Factory are composed of three components: Encoder - Process input observations (images, vectors) and map them to a vector. This is the part of the model you will most likely want to customize. Core - Intergrate vectors from one or more encoders, can optionally include a single- or multi-layer LSTM/GRU in a memory-based agent. Decoder - Apply additional layers to the output of the model core before computing the policy and value outputs. Regardless of the component customization, you can use your resulting model in \"shared weights\" or \"separate weights\" regime (either sharing or not sharing the weights between the policy and value networks). This is controlled by the --actor_critic_share_weights=[True|False] command line argument. On top of that, you can register an entire custom Actor Critic model. This can be useful for more complex models, for example centralized critic for multi-agent envs, asymmetric actor-critic where critic observes more information, which can be useful in sim-to-real, and so on.","title":"Actor Critic models in Sample Factory"},{"location":"03-customization/custom-models/#custom-model-template","text":"The following template demonstrates how different components of the model can be customized. Feel free to combine this with the custom environment template above to create a fully custom environment & model combination. from sample_factory.model.encoder import Encoder from sample_factory.model.decoder import Decoder from sample_factory.model.core import ModelCore from sample_factory.model.actor_critic import ActorCritic from sample_factory.algo.utils.context import global_model_factory class CustomEncoder ( Encoder ): def __init__ ( self , cfg : Config , obs_space : ObsSpace ): super () . __init__ ( cfg ) # build custom encoder architecture ... def forward ( self , obs_dict ): # custom forward logic ... class CustomCore ( ModelCore ): def __init__ ( self , cfg : Config , input_size : int ): super () . __init__ ( cfg ) # build custom core architecture ... def forward ( self , head_output , rnn_states ): # custom forward logic ... class CustomDecoder ( Decoder ): def __init__ ( self , cfg : Config , decoder_input_size : int ): super () . __init__ ( cfg ) # build custom decoder architecture ... def forward ( self , core_output ): # custom forward logic ... class CustomActorCritic ( ActorCritic ): def __init__ ( self , model_factory , obs_space : ObsSpace , action_space : ActionSpace , cfg : Config , ): super () . __init__ ( obs_space , action_space , cfg ) self . encoder = CustomEncoder ( cfg , obs_space ) self . core = CustomCore ( cfg , self . encoder . get_out_size ()) self . decoder = CustomDecoder ( cfg , self . core . get_out_size ()) self . critic_linear = nn . Linear ( self . decoder . get_out_size ()) self . action_parameterization = self . get_action_parameterization ( self . decoder . get_out_size () ) def forward ( self , normalized_obs_dict , rnn_states , values_only = False ): # forward logic ... def register_model_components (): # register custom components with the factory # you can register an entire Actor Critic model global_model_factory () . register_actor_critic_factory ( CustomActorCritic ) # or individual components global_model_factory () . register_encoder_factory ( CustomEncoder ) global_model_factory () . register_core_factory ( CustomCore ) global_model_factory () . register_decoder_factory ( CustomDecoder ) def main (): \"\"\"Script entry point.\"\"\" register_model_components () cfg = parse_args () status = run_rl ( cfg ) return status if __name__ == \"__main__\" : sys . exit ( main ())","title":"Custom model template"},{"location":"03-customization/custom-models/#examples","text":"Examples of model customizations can be found in: sf_examples/train_custom_env_custom_model.py sf_examples/isaacgym_examples/train_isaacgym.py sf_examples/dmlab/dmlab_model.py sf_examples/vizdoom/doom/doom_model.py","title":"Examples"},{"location":"03-customization/custom-multi-agent-environments/","text":"Custom multi-agent environments \u00b6 Multi-agent environments are expected to return lists (or tuples, arrays, tensors) of observations/rewards/etc, one item for every agent. It is expected that a multi-agent env exposes a property or a member variable num_agents that the algorithm uses to allocate the right amount of memory during startup. Multi-agent environments require auto-reset! I.e. they reset a particular agent when the corresponding terminated or truncated flag is True and return the first observation of the next episode (because we have no use for the last observation of the previous episode, we do not act based on it). For simplicity Sample Factory actually treats all environments as multi-agent, i.e. single-agent environments are automatically treated as multi-agent environments with one agent with the use of a wrapper. In rare cases we may deal with an environment that should not be additionally wrapped, i.e. a single-agent version of a multi-agent env may already return lists of length 1. In this case, your environment should define a member variable is_multiagent=True , and Sample Factory will not wrap it. Examples \u00b6 sf_examples/enjoy_custom_multi_env.py - integrates and entirely custom toy example multi-agent env. Use this as a template for your own multi-agent env. sf_examples/isaacgym_examples/train_isaacgym.py - technically IsaacGym is not a multi-agent environment because different agents don't interact. It is a vectorized environment simulating many agents with a single env instance, but is treated as a multi-agent environment by Sample Factory. sf_examples/vizdoom/doom/multiplayer - this is a rather advanced example, here we connect multiple VizDoom instances into a single multi-agent match and expose a multi-agent env interface to Sample Factory. Further reading \u00b6 Multi-agent environments can be combined with multi-policy training and Population Based Training (PBT) . Sometimes it makes sense to disable some of the agents in a multi-agent environment. For example, in a multi-player game some agents might die in the middle of the episode and should not contribute any rollouts until the episode reset. This can be achieved using inactive agents feature .","title":"Custom multi-agent environments"},{"location":"03-customization/custom-multi-agent-environments/#custom-multi-agent-environments","text":"Multi-agent environments are expected to return lists (or tuples, arrays, tensors) of observations/rewards/etc, one item for every agent. It is expected that a multi-agent env exposes a property or a member variable num_agents that the algorithm uses to allocate the right amount of memory during startup. Multi-agent environments require auto-reset! I.e. they reset a particular agent when the corresponding terminated or truncated flag is True and return the first observation of the next episode (because we have no use for the last observation of the previous episode, we do not act based on it). For simplicity Sample Factory actually treats all environments as multi-agent, i.e. single-agent environments are automatically treated as multi-agent environments with one agent with the use of a wrapper. In rare cases we may deal with an environment that should not be additionally wrapped, i.e. a single-agent version of a multi-agent env may already return lists of length 1. In this case, your environment should define a member variable is_multiagent=True , and Sample Factory will not wrap it.","title":"Custom multi-agent environments"},{"location":"03-customization/custom-multi-agent-environments/#examples","text":"sf_examples/enjoy_custom_multi_env.py - integrates and entirely custom toy example multi-agent env. Use this as a template for your own multi-agent env. sf_examples/isaacgym_examples/train_isaacgym.py - technically IsaacGym is not a multi-agent environment because different agents don't interact. It is a vectorized environment simulating many agents with a single env instance, but is treated as a multi-agent environment by Sample Factory. sf_examples/vizdoom/doom/multiplayer - this is a rather advanced example, here we connect multiple VizDoom instances into a single multi-agent match and expose a multi-agent env interface to Sample Factory.","title":"Examples"},{"location":"03-customization/custom-multi-agent-environments/#further-reading","text":"Multi-agent environments can be combined with multi-policy training and Population Based Training (PBT) . Sometimes it makes sense to disable some of the agents in a multi-agent environment. For example, in a multi-player game some agents might die in the middle of the episode and should not contribute any rollouts until the episode reset. This can be achieved using inactive agents feature .","title":"Further reading"},{"location":"04-experiments/experiment-launcher/","text":"Experiment Launcher \u00b6 The simplest way to run experiments is just through command line, see Basic Usage for example. For more complex workflows Sample Factory provides an interface that allows users to run experiments with multiple seeds or hyperparameter combinations with automatic distribution of work across GPUs on a single machine or multiple machines on the cluster. The configuration of such experiments is done through in Python code, i.e. instead of yaml or json files we directly use Python scripts for ultimate flexibility. Launcher scripts \u00b6 Take a look at sf_examples/mujoco/experiments/mujoco_all_envs.py : from sample_factory.launcher.run_description import Experiment , ParamGrid , RunDescription _params = ParamGrid ( [ ( \"seed\" , [ 0 , 1111 , 2222 , 3333 , 4444 , 5555 , 6666 , 7777 , 8888 , 9999 ]), ( \"env\" , [ \"mujoco_ant\" , \"mujoco_halfcheetah\" , \"mujoco_hopper\" , \"mujoco_humanoid\" , \"mujoco_doublependulum\" , \"mujoco_pendulum\" , \"mujoco_reacher\" , \"mujoco_swimmer\" , \"mujoco_walker\" ]), ] ) _experiments = [ Experiment ( \"mujoco_all_envs\" , \"python -m sf_examples.mujoco.train_mujoco --algo=APPO --with_wandb=True --wandb_tags mujoco\" , _params . generate_params ( randomize = False ), ), ] RUN_DESCRIPTION = RunDescription ( \"mujoco_all_envs\" , experiments = _experiments ) This script defines a list of experiments to run. Here we have 10 seeds and 9 environments, so we will run 90 experiments in total with 90 different seed/env combinations. This can be extended in a straightforward way to run hyperparameter searches and so on. The only requirement for such a script is that it defines a RUN_DESCRIPTION variable that references a RunDescription object. This object contains a list of Experiment objects, each of which potentially defines a gridsearch to run. Each experiment object defines a name, a \"base\" command line to run, and a ParamGrid that will generate parameter combinations to be added to the base command line. Take a look at other experiment scripts in sf_examples to see how to define more complex experiments. Note that there's no requirement to use Launcher API to run experiments. You can just run individual experiments from the command line, use WandB hyperparam search features, use Ray Tune or any other tool you like. Launcher API is just a convenient feature for simple workflows available out of the box. Complex hyperparameter configurations \u00b6 The ParamGrid object above can define a cartesian product of parameter lists. In some cases we want searches over pairs (or tuples) of parameters at the same time. For example: _params = ParamGrid ( [ ( \"seed\" , [ 1111 , 2222 , 3333 , 4444 ]), (( \"serial_mode\" , \"async_rl\" ), ([ True , False ], [ False , True ])), (( \"use_rnn\" , \"recurrence\" ), ([ False , 1 ], [ True , 16 ])), ] ) Here we consider parameter pairs (\"serial_mode\", \"async_rl\") and (\"use_rnn\", \"recurrence\") at the same time. If we used a simple grid, we would have to execute useless combinations of parameters such as use_rnn=True, recurrence=1 or use_rnn=False, recurrence=16 (it makes sense to use recurrence > 1 only when using RNNs). RunDescription arguments \u00b6 Launcher script should expose a RunDescription object named RUN_DESCRIPTION that contains a list of experiments to run and some auxiliary parameters. RunDescription parameter reference: class RunDescription : def __init__ ( self , run_name , experiments , experiment_arg_name = \"--experiment\" , experiment_dir_arg_name = \"--train_dir\" , customize_experiment_name = True , param_prefix = \"--\" , ): \"\"\" :param run_name: overall name of the experiment and the name of the root folder :param experiments: a list of Experiment objects to run :param experiment_arg_name: CLI argument of the underlying experiment that determines it's unique name to be generated by the launcher. Default: --experiment :param experiment_dir_arg_name: CLI argument for the root train dir of your experiment. Default: --train_dir :param customize_experiment_name: whether to add a hyperparameter combination to the experiment name :param param_prefix: most experiments will use \"--\" prefix for each parameter, but some apps don't have this prefix, i.e. with Hydra you should set it to empty string. \"\"\" Using a launcher script \u00b6 The script above can be executed using one of several backends. Additional backends are a welcome contribution! Please submit PRs :) \"Local\" backend (multiprocessing) \u00b6 Command line below will run all experiments on a single 4-GPU machine, scheduling 2 experiments per GPU, so running 8 experiments in parallel until all 90 are done. Note how we pass the full path to the launcher script using --run argument. The script should be in your Python path in a way that you should be able to import the module using the path you pass to --run (because this is what the Launcher internally does). python -m sample_factory.launcher.run --run = sf_examples.mujoco.experiments.mujoco_all_envs --backend = processes --max_parallel = 8 --pause_between = 1 --experiments_per_gpu = 2 --num_gpus = 4 Slurm backend \u00b6 The following command will run experiments on a Slurm cluster, creating a separate job for each experiment. python -m sample_factory.launcher.run --run = sf_examples.mujoco.experiments.mujoco_all_envs --backend = slurm --slurm_workdir = ./slurm_isaacgym --experiment_suffix = slurm --slurm_gpus_per_job = 1 --slurm_cpus_per_gpu = 16 --slurm_sbatch_template = ./sample_factory/launcher/slurm/sbatch_timeout.sh --pause_between = 1 --slurm_print_only = False Here we will use 1 GPU and 16 CPUs per job (adjust according to your cluster configuration and experiment config). Note how we also pass --slurm_sbatch_template argument which contains a bash script that will bootstrap a job. In this particular example we use a template that will kill the job if it runs longer than a certain amount of time and then restarts itself (controlled by --slurm_timeout which defaults to 0, i.e. no timeout). Feel free to use your custom template if your job has certain pre-requisites (i.e. installing some packages or activating a Python environment). Please find additional Slurm considerations in How to use Sample Factory on Slurm guide. NGC backend \u00b6 We additionally provide a backend for NGC clusters ( https://ngc.nvidia.com/ ). python -m sample_factory.launcher.run --run = sf_examples.mujoco.experiments.mujoco_all_envs --backend = ngc --ngc_job_template = run_scripts/ngc_job_16g_1gpu.template --ngc_print_only = False --train_dir = /workspace/train_dir Here --ngc_job_template contains information about which Docker image to run plus any additional job bootstrapping. The command will essentially spin a separate VM on the cloud for each job. Point --train_dir to a mounted workspace folder so that you can access results of your experiments (trained models, logs, etc.) Additional CLI examples \u00b6 Local multiprocessing backend: $ python -m sample_factory.launcher.run --run=sf_examples.vizdoom.experiments.paper_doom_battle2_appo --backend=processes --max_parallel=4 --pause_between=10 --experiments_per_gpu=1 --num_gpus=4 Parallelize with Slurm: $ python -m sample_factory.launcher.run --run=megaverse_rl.runs.single_agent --backend=slurm --slurm_workdir=./megaverse_single_agent --experiment_suffix=slurm --pause_between=1 --slurm_gpus_per_job=1 --slurm_cpus_per_gpu=12 --slurm_sbatch_template=./megaverse_rl/slurm/sbatch_template.sh --slurm_print_only=False Parallelize with NGC (https://ngc.nvidia.com/): $ python -m sample_factory.launcher.run --run=rlgpu.run_scripts.dexterous_manipulation --backend=ngc --ngc_job_template=run_scripts/ngc_job_16g_1gpu.template --ngc_print_only=False --train_dir=/workspace/train_dir Command-line reference \u00b6 usage: run.py [-h] [--train_dir TRAIN_DIR] [--run RUN] [--backend {processes,slurm,ngc}] [--pause_between PAUSE_BETWEEN] [--experiment_suffix EXPERIMENT_SUFFIX] # Multiprocessing backend: [--num_gpus NUM_GPUS] [--experiments_per_gpu EXPERIMENTS_PER_GPU] [--max_parallel MAX_PARALLEL] # Slurm-related: [--slurm_gpus_per_job SLURM_GPUS_PER_JOB] [--slurm_cpus_per_gpu SLURM_CPUS_PER_GPU] [--slurm_print_only SLURM_PRINT_ONLY] [--slurm_workdir SLURM_WORKDIR] [--slurm_partition SLURM_PARTITION] [--slurm_sbatch_template SLURM_SBATCH_TEMPLATE] # NGC-related [--ngc_job_template NGC_JOB_TEMPLATE] [--ngc_print_only NGC_PRINT_ONLY] Arguments: -h, --help show this help message and exit --train_dir TRAIN_DIR Directory for sub-experiments --run RUN Name of the python module that describes the run, e.g. sf_examples.vizdoom.experiments.doom_basic --backend {processes,slurm,ngc} --pause_between PAUSE_BETWEEN Pause in seconds between processes --experiment_suffix EXPERIMENT_SUFFIX Append this to the name of the experiment dir Multiprocessing backend: --num_gpus NUM_GPUS How many GPUs to use (only for local multiprocessing) --experiments_per_gpu EXPERIMENTS_PER_GPU How many experiments can we squeeze on a single GPU (-1 for not altering CUDA_VISIBLE_DEVICES at all) --max_parallel MAX_PARALLEL Maximum simultaneous experiments (only for local multiprocessing) Slurm-related: --slurm_gpus_per_job SLURM_GPUS_PER_JOB GPUs in a single SLURM process --slurm_cpus_per_gpu SLURM_CPUS_PER_GPU Max allowed number of CPU cores per allocated GPU --slurm_print_only SLURM_PRINT_ONLY Just print commands to the console without executing --slurm_workdir SLURM_WORKDIR Optional workdir. Used by slurm launcher to store logfiles etc. --slurm_partition SLURM_PARTITION Adds slurm partition, i.e. for \"gpu\" it will add \"-p gpu\" to sbatch command line --slurm_sbatch_template SLURM_SBATCH_TEMPLATE Commands to run before the actual experiment (i.e. activate conda env, etc.) Example: https://github.com/alex-petrenko/megaverse/blob/master/megaverse_rl/slurm/sbatch_template.sh (typically a shell script) --slurm_timeout SLURM_TIMEOUT Time to run jobs before timing out job and requeuing the job. Defaults to 0, which does not time out the job NGC-related: --ngc_job_template NGC_JOB_TEMPLATE NGC command line template, specifying instance type, docker container, etc. --ngc_print_only NGC_PRINT_ONLY Just print commands to the console without executing","title":"Experiment Launcher"},{"location":"04-experiments/experiment-launcher/#experiment-launcher","text":"The simplest way to run experiments is just through command line, see Basic Usage for example. For more complex workflows Sample Factory provides an interface that allows users to run experiments with multiple seeds or hyperparameter combinations with automatic distribution of work across GPUs on a single machine or multiple machines on the cluster. The configuration of such experiments is done through in Python code, i.e. instead of yaml or json files we directly use Python scripts for ultimate flexibility.","title":"Experiment Launcher"},{"location":"04-experiments/experiment-launcher/#launcher-scripts","text":"Take a look at sf_examples/mujoco/experiments/mujoco_all_envs.py : from sample_factory.launcher.run_description import Experiment , ParamGrid , RunDescription _params = ParamGrid ( [ ( \"seed\" , [ 0 , 1111 , 2222 , 3333 , 4444 , 5555 , 6666 , 7777 , 8888 , 9999 ]), ( \"env\" , [ \"mujoco_ant\" , \"mujoco_halfcheetah\" , \"mujoco_hopper\" , \"mujoco_humanoid\" , \"mujoco_doublependulum\" , \"mujoco_pendulum\" , \"mujoco_reacher\" , \"mujoco_swimmer\" , \"mujoco_walker\" ]), ] ) _experiments = [ Experiment ( \"mujoco_all_envs\" , \"python -m sf_examples.mujoco.train_mujoco --algo=APPO --with_wandb=True --wandb_tags mujoco\" , _params . generate_params ( randomize = False ), ), ] RUN_DESCRIPTION = RunDescription ( \"mujoco_all_envs\" , experiments = _experiments ) This script defines a list of experiments to run. Here we have 10 seeds and 9 environments, so we will run 90 experiments in total with 90 different seed/env combinations. This can be extended in a straightforward way to run hyperparameter searches and so on. The only requirement for such a script is that it defines a RUN_DESCRIPTION variable that references a RunDescription object. This object contains a list of Experiment objects, each of which potentially defines a gridsearch to run. Each experiment object defines a name, a \"base\" command line to run, and a ParamGrid that will generate parameter combinations to be added to the base command line. Take a look at other experiment scripts in sf_examples to see how to define more complex experiments. Note that there's no requirement to use Launcher API to run experiments. You can just run individual experiments from the command line, use WandB hyperparam search features, use Ray Tune or any other tool you like. Launcher API is just a convenient feature for simple workflows available out of the box.","title":"Launcher scripts"},{"location":"04-experiments/experiment-launcher/#complex-hyperparameter-configurations","text":"The ParamGrid object above can define a cartesian product of parameter lists. In some cases we want searches over pairs (or tuples) of parameters at the same time. For example: _params = ParamGrid ( [ ( \"seed\" , [ 1111 , 2222 , 3333 , 4444 ]), (( \"serial_mode\" , \"async_rl\" ), ([ True , False ], [ False , True ])), (( \"use_rnn\" , \"recurrence\" ), ([ False , 1 ], [ True , 16 ])), ] ) Here we consider parameter pairs (\"serial_mode\", \"async_rl\") and (\"use_rnn\", \"recurrence\") at the same time. If we used a simple grid, we would have to execute useless combinations of parameters such as use_rnn=True, recurrence=1 or use_rnn=False, recurrence=16 (it makes sense to use recurrence > 1 only when using RNNs).","title":"Complex hyperparameter configurations"},{"location":"04-experiments/experiment-launcher/#rundescription-arguments","text":"Launcher script should expose a RunDescription object named RUN_DESCRIPTION that contains a list of experiments to run and some auxiliary parameters. RunDescription parameter reference: class RunDescription : def __init__ ( self , run_name , experiments , experiment_arg_name = \"--experiment\" , experiment_dir_arg_name = \"--train_dir\" , customize_experiment_name = True , param_prefix = \"--\" , ): \"\"\" :param run_name: overall name of the experiment and the name of the root folder :param experiments: a list of Experiment objects to run :param experiment_arg_name: CLI argument of the underlying experiment that determines it's unique name to be generated by the launcher. Default: --experiment :param experiment_dir_arg_name: CLI argument for the root train dir of your experiment. Default: --train_dir :param customize_experiment_name: whether to add a hyperparameter combination to the experiment name :param param_prefix: most experiments will use \"--\" prefix for each parameter, but some apps don't have this prefix, i.e. with Hydra you should set it to empty string. \"\"\"","title":"RunDescription arguments"},{"location":"04-experiments/experiment-launcher/#using-a-launcher-script","text":"The script above can be executed using one of several backends. Additional backends are a welcome contribution! Please submit PRs :)","title":"Using a launcher script"},{"location":"04-experiments/experiment-launcher/#local-backend-multiprocessing","text":"Command line below will run all experiments on a single 4-GPU machine, scheduling 2 experiments per GPU, so running 8 experiments in parallel until all 90 are done. Note how we pass the full path to the launcher script using --run argument. The script should be in your Python path in a way that you should be able to import the module using the path you pass to --run (because this is what the Launcher internally does). python -m sample_factory.launcher.run --run = sf_examples.mujoco.experiments.mujoco_all_envs --backend = processes --max_parallel = 8 --pause_between = 1 --experiments_per_gpu = 2 --num_gpus = 4","title":"\"Local\" backend (multiprocessing)"},{"location":"04-experiments/experiment-launcher/#slurm-backend","text":"The following command will run experiments on a Slurm cluster, creating a separate job for each experiment. python -m sample_factory.launcher.run --run = sf_examples.mujoco.experiments.mujoco_all_envs --backend = slurm --slurm_workdir = ./slurm_isaacgym --experiment_suffix = slurm --slurm_gpus_per_job = 1 --slurm_cpus_per_gpu = 16 --slurm_sbatch_template = ./sample_factory/launcher/slurm/sbatch_timeout.sh --pause_between = 1 --slurm_print_only = False Here we will use 1 GPU and 16 CPUs per job (adjust according to your cluster configuration and experiment config). Note how we also pass --slurm_sbatch_template argument which contains a bash script that will bootstrap a job. In this particular example we use a template that will kill the job if it runs longer than a certain amount of time and then restarts itself (controlled by --slurm_timeout which defaults to 0, i.e. no timeout). Feel free to use your custom template if your job has certain pre-requisites (i.e. installing some packages or activating a Python environment). Please find additional Slurm considerations in How to use Sample Factory on Slurm guide.","title":"Slurm backend"},{"location":"04-experiments/experiment-launcher/#ngc-backend","text":"We additionally provide a backend for NGC clusters ( https://ngc.nvidia.com/ ). python -m sample_factory.launcher.run --run = sf_examples.mujoco.experiments.mujoco_all_envs --backend = ngc --ngc_job_template = run_scripts/ngc_job_16g_1gpu.template --ngc_print_only = False --train_dir = /workspace/train_dir Here --ngc_job_template contains information about which Docker image to run plus any additional job bootstrapping. The command will essentially spin a separate VM on the cloud for each job. Point --train_dir to a mounted workspace folder so that you can access results of your experiments (trained models, logs, etc.)","title":"NGC backend"},{"location":"04-experiments/experiment-launcher/#additional-cli-examples","text":"Local multiprocessing backend: $ python -m sample_factory.launcher.run --run=sf_examples.vizdoom.experiments.paper_doom_battle2_appo --backend=processes --max_parallel=4 --pause_between=10 --experiments_per_gpu=1 --num_gpus=4 Parallelize with Slurm: $ python -m sample_factory.launcher.run --run=megaverse_rl.runs.single_agent --backend=slurm --slurm_workdir=./megaverse_single_agent --experiment_suffix=slurm --pause_between=1 --slurm_gpus_per_job=1 --slurm_cpus_per_gpu=12 --slurm_sbatch_template=./megaverse_rl/slurm/sbatch_template.sh --slurm_print_only=False Parallelize with NGC (https://ngc.nvidia.com/): $ python -m sample_factory.launcher.run --run=rlgpu.run_scripts.dexterous_manipulation --backend=ngc --ngc_job_template=run_scripts/ngc_job_16g_1gpu.template --ngc_print_only=False --train_dir=/workspace/train_dir","title":"Additional CLI examples"},{"location":"04-experiments/experiment-launcher/#command-line-reference","text":"usage: run.py [-h] [--train_dir TRAIN_DIR] [--run RUN] [--backend {processes,slurm,ngc}] [--pause_between PAUSE_BETWEEN] [--experiment_suffix EXPERIMENT_SUFFIX] # Multiprocessing backend: [--num_gpus NUM_GPUS] [--experiments_per_gpu EXPERIMENTS_PER_GPU] [--max_parallel MAX_PARALLEL] # Slurm-related: [--slurm_gpus_per_job SLURM_GPUS_PER_JOB] [--slurm_cpus_per_gpu SLURM_CPUS_PER_GPU] [--slurm_print_only SLURM_PRINT_ONLY] [--slurm_workdir SLURM_WORKDIR] [--slurm_partition SLURM_PARTITION] [--slurm_sbatch_template SLURM_SBATCH_TEMPLATE] # NGC-related [--ngc_job_template NGC_JOB_TEMPLATE] [--ngc_print_only NGC_PRINT_ONLY] Arguments: -h, --help show this help message and exit --train_dir TRAIN_DIR Directory for sub-experiments --run RUN Name of the python module that describes the run, e.g. sf_examples.vizdoom.experiments.doom_basic --backend {processes,slurm,ngc} --pause_between PAUSE_BETWEEN Pause in seconds between processes --experiment_suffix EXPERIMENT_SUFFIX Append this to the name of the experiment dir Multiprocessing backend: --num_gpus NUM_GPUS How many GPUs to use (only for local multiprocessing) --experiments_per_gpu EXPERIMENTS_PER_GPU How many experiments can we squeeze on a single GPU (-1 for not altering CUDA_VISIBLE_DEVICES at all) --max_parallel MAX_PARALLEL Maximum simultaneous experiments (only for local multiprocessing) Slurm-related: --slurm_gpus_per_job SLURM_GPUS_PER_JOB GPUs in a single SLURM process --slurm_cpus_per_gpu SLURM_CPUS_PER_GPU Max allowed number of CPU cores per allocated GPU --slurm_print_only SLURM_PRINT_ONLY Just print commands to the console without executing --slurm_workdir SLURM_WORKDIR Optional workdir. Used by slurm launcher to store logfiles etc. --slurm_partition SLURM_PARTITION Adds slurm partition, i.e. for \"gpu\" it will add \"-p gpu\" to sbatch command line --slurm_sbatch_template SLURM_SBATCH_TEMPLATE Commands to run before the actual experiment (i.e. activate conda env, etc.) Example: https://github.com/alex-petrenko/megaverse/blob/master/megaverse_rl/slurm/sbatch_template.sh (typically a shell script) --slurm_timeout SLURM_TIMEOUT Time to run jobs before timing out job and requeuing the job. Defaults to 0, which does not time out the job NGC-related: --ngc_job_template NGC_JOB_TEMPLATE NGC command line template, specifying instance type, docker container, etc. --ngc_print_only NGC_PRINT_ONLY Just print commands to the console without executing","title":"Command-line reference"},{"location":"04-experiments/slurm-details/","text":"Sample Factory on Slurm \u00b6 This section contains instructions for running Sample Factory experiments using Slurm. Setting up \u00b6 Login to your Slurm login node using ssh with your username and password. Start an interactive job with srun to install files to your NFS. srun -c40 --gres=gpu:1 --pty bash Note that you may get a message groups: cannot find name for group ID XXXX which is not an error. Install Miniconda: Download installer using wget from https://docs.conda.io/en/latest/miniconda.html#linux-installers Run the installer with bash {Miniconda...sh} Make new conda environment conda create --name sf2 then conda activate sf2 Download Sample Factory and install dependencies, for example: git clone https://github.com/alex-petrenko/sample-factory.git cd sample-factory git checkout sf2 pip install -e . # install additional env dependencies here if needed Necessary scripts in Sample Factory \u00b6 To run a custom launcher script for Sample Factory on slurm, you may need to write your own slurm_sbatch_template and/or launcher script. slurm_sbatch_template is a bash script that run by slurm before your python script. It includes commands to activate your conda environment etc. See an example at ./sample_factory/launcher/slurm/sbatch_timeout.sh . Variables in the bash script can be added in sample_factory.launcher.run_slurm . The launcher script controls the Python command slurm will run. Examples are located in sf_examples . You can run multiple experiments with different parameters using ParamGrid . Timeout script \u00b6 If your slurm cluster has time limits for jobs, you can use the sbatch_timeout.sh bash script to launch jobs that timeout and requeue themselves before the time limit. The time limit can be set with the --slurm_timeout command line argument. It defaults to 0 which runs the job with no time limit. It is recommended the timeout be set to slightly less than the time limit of your job. For example, if the time limit is 24 hours, you should set --slurm_timeout=23h Running launcher scripts on Slurm \u00b6 Activate your conda environment conda activate sf2 then cd sample-factory Run your launcher script - an example mujoco launcher (replace run, slurm_sbatch_template, and slurm_workdir with appropriate values) python -m sample_factory.launcher.run --run=sf_examples.mujoco.experiments.mujoco_all_envs --backend=slurm --slurm_workdir=./slurm_mujoco --experiment_suffix=slurm --slurm_gpus_per_job=1 --slurm_cpus_per_gpu=16 --slurm_sbatch_template=./sample_factory/launcher/slurm/sbatch_timeout.sh --pause_between=1 --slurm_print_only=False The slurm_gpus_per_job and slurm_cpus_per_gpu determine the resources allocated to each job. You can view the jobs without running them by setting slurm_print_only=True . You can view the status of your jobs on nodes or the queue with squeue and view the outputs of your experiments with tail -f {slurm_workdir}/*.out . Cancel your jobs with scancel {job_id}","title":"Sample Factory on Slurm"},{"location":"04-experiments/slurm-details/#sample-factory-on-slurm","text":"This section contains instructions for running Sample Factory experiments using Slurm.","title":"Sample Factory on Slurm"},{"location":"04-experiments/slurm-details/#setting-up","text":"Login to your Slurm login node using ssh with your username and password. Start an interactive job with srun to install files to your NFS. srun -c40 --gres=gpu:1 --pty bash Note that you may get a message groups: cannot find name for group ID XXXX which is not an error. Install Miniconda: Download installer using wget from https://docs.conda.io/en/latest/miniconda.html#linux-installers Run the installer with bash {Miniconda...sh} Make new conda environment conda create --name sf2 then conda activate sf2 Download Sample Factory and install dependencies, for example: git clone https://github.com/alex-petrenko/sample-factory.git cd sample-factory git checkout sf2 pip install -e . # install additional env dependencies here if needed","title":"Setting up"},{"location":"04-experiments/slurm-details/#necessary-scripts-in-sample-factory","text":"To run a custom launcher script for Sample Factory on slurm, you may need to write your own slurm_sbatch_template and/or launcher script. slurm_sbatch_template is a bash script that run by slurm before your python script. It includes commands to activate your conda environment etc. See an example at ./sample_factory/launcher/slurm/sbatch_timeout.sh . Variables in the bash script can be added in sample_factory.launcher.run_slurm . The launcher script controls the Python command slurm will run. Examples are located in sf_examples . You can run multiple experiments with different parameters using ParamGrid .","title":"Necessary scripts in Sample Factory"},{"location":"04-experiments/slurm-details/#timeout-script","text":"If your slurm cluster has time limits for jobs, you can use the sbatch_timeout.sh bash script to launch jobs that timeout and requeue themselves before the time limit. The time limit can be set with the --slurm_timeout command line argument. It defaults to 0 which runs the job with no time limit. It is recommended the timeout be set to slightly less than the time limit of your job. For example, if the time limit is 24 hours, you should set --slurm_timeout=23h","title":"Timeout script"},{"location":"04-experiments/slurm-details/#running-launcher-scripts-on-slurm","text":"Activate your conda environment conda activate sf2 then cd sample-factory Run your launcher script - an example mujoco launcher (replace run, slurm_sbatch_template, and slurm_workdir with appropriate values) python -m sample_factory.launcher.run --run=sf_examples.mujoco.experiments.mujoco_all_envs --backend=slurm --slurm_workdir=./slurm_mujoco --experiment_suffix=slurm --slurm_gpus_per_job=1 --slurm_cpus_per_gpu=16 --slurm_sbatch_template=./sample_factory/launcher/slurm/sbatch_timeout.sh --pause_between=1 --slurm_print_only=False The slurm_gpus_per_job and slurm_cpus_per_gpu determine the resources allocated to each job. You can view the jobs without running them by setting slurm_print_only=True . You can view the status of your jobs on nodes or the queue with squeue and view the outputs of your experiments with tail -f {slurm_workdir}/*.out . Cancel your jobs with scancel {job_id}","title":"Running launcher scripts on Slurm"},{"location":"05-monitoring/custom-metrics/","text":"Custom Summaries \u00b6 Environment-specific info \u00b6 It is often useful to monitor custom training metrics, i.e. certain environment-specific aspects of agent's performance. You can add custom monitored metrics by adding info[\"episode_extra_stats\"] = { ... } to the environment's info dictionary returned from the step() function on the last step of the episode. See sf_examples/dmlab/wrappers/reward_shaping.py for example. Here we add information about agent's performance on individual levels in DMLab-30. Custom metrics \u00b6 You can add completely custom metrics that are calculated based on other metrics or the RL algorithm state. To do this, add a custom algo observer that overrides extra_summaries() function. See sf_examples/dmlab/train_dmlab.py where we define DmlabExtraSummariesObserver that aggregates custom environment metrics to produce a single \"Human-normalized score\" summary.","title":"Custom Summaries"},{"location":"05-monitoring/custom-metrics/#custom-summaries","text":"","title":"Custom Summaries"},{"location":"05-monitoring/custom-metrics/#environment-specific-info","text":"It is often useful to monitor custom training metrics, i.e. certain environment-specific aspects of agent's performance. You can add custom monitored metrics by adding info[\"episode_extra_stats\"] = { ... } to the environment's info dictionary returned from the step() function on the last step of the episode. See sf_examples/dmlab/wrappers/reward_shaping.py for example. Here we add information about agent's performance on individual levels in DMLab-30.","title":"Environment-specific info"},{"location":"05-monitoring/custom-metrics/#custom-metrics","text":"You can add completely custom metrics that are calculated based on other metrics or the RL algorithm state. To do this, add a custom algo observer that overrides extra_summaries() function. See sf_examples/dmlab/train_dmlab.py where we define DmlabExtraSummariesObserver that aggregates custom environment metrics to produce a single \"Human-normalized score\" summary.","title":"Custom metrics"},{"location":"05-monitoring/metrics-reference/","text":"Metrics Reference \u00b6 General information \u00b6 Each experiment will have at least the following groups of metrics on Tensorboard/Wandb: len perf policy_stats reward stats train Plus new sections (groups) are created for each custom metric with key in <group_name>/<metric_name> format (see Custom Metrics section). Summaries such as len , perf , reward are averaged over the last 100 data points to filter noise (this can be changed by --stats_avg=N argument). These summaries are written to Tensorboard/Wandb every --experiment_summaries_interval seconds (10 seconds by default). train summaries are not averaged and just represent the values from the latest minibatch on the learner. The reporting rate for train summaries is decayed over time to reduce the size of the log files. The schedule is controlled by summary_rate_decay_seconds variable in learner.py . len \u00b6 len/len , len/len_max , len/len_min are simply episode lengths measured after frameskip . If your environment uses frameskip=4 and the reported episode length is 400, it means that 400 environment steps were simulated but the agent actually observed only 100 frames. perf \u00b6 perf/_fps and perf/_sample_throughput represent throughput as measured in different parts of the algorithm. perf/_sample_throughput is the number of observations processed (or actions generated) by the inference worker, i.e. pure sampling throughput measured before frameskipping is taken into account. perf/_fps is the number of observations/actions processed by the learner and measured after frameskipping. For example with frameskip=4, perf/_sample_throughput will be 4 times smaller than perf/_fps . If this is not the case, it means that the learner had to throw away some trajectories which can happen for multiple reasons, for example if the trajectories were too stale and exceeded --max_policy_lag . policy_stats \u00b6 By default this section only contains the true_objective metrics: policy_stats/avg_true_objective , policy_stats/avg_true_objective_max , policy_stats/avg_true_objective_min . This will reflect the true_objective value if the environment returns one in the info dictionary (see PBT for more details). If true_objective is not specified these metrics should be equal to the scalar environment reward. policy_stats will also contain any custom metrics (see Custom metrics ) that are not in <group_name>/<metric_name> format. reward \u00b6 reward/reward , reward/reward_max , reward/reward_min are the raw scalar environment rewards, reported before any scaling ( --reward_scale ) or normalization is applied. stats \u00b6 stats/avg_request_count - how many requests from the rollout workers are processed per inference step. The correpondence between this number and the actual inference batch size depends on training configuration, this is mostly an internal metric for debugging purposes. stats/gpu_cache_learner , stats/gpu_cache_policy_worker , stats/gpu_mem_learner , stats/gpu_mem_policy_worker , stats/gpu_mem_policy_worker , stats/master_process_memory_mb , stats/memory_learner , stats/memory_policy_worker - a group of metrics to keep track of RAM and VRAM usage, mostly used to detect and debug memory leaks. stats/step_policy , stats/wait_policy - performance debugging metrics for the inference worker, respectively the time spent on the last inference step and the time spent waiting for new observations from the rollout workers, both in seconds. train \u00b6 This is perhaps the most useful section of metrics, many parameters can be used to debug RL training issues. Metrics are listed and explained below in the alphabetical order in which they appear in Tensorboard. train/actual_lr - the actual learning rate used by the learner, which can be different from the configuration parameter if the adaptive learning rate is enabled. train/adam_max_second_moment - the maximum value of the second moment of the Adam optimizer. Sometimes spikes in this metric can be used to detect training instability. train/adv_max , train/adv_min , train/adv_std - the maximum, minimum, standard deviation of the advantage values. \"Mean\" value is not reported because it is always zero (we use advantage normalization by default). train/entropy - the entropy of the actions probability distribution. train/exploration_loss - exploration loss (if any). See --exploration_loss argument for more details. train/fraction_clipped - fraction of minibatch samples that were clipped by the PPO loss. This value growing too large is often a sign of training instability (i.e. learning rate is too high). train/grad_norm - the L2 norm of the gradient of the loss function after gradient clipping. train/kl_divergence - the average KL-divergence between the policy that collected the experience and the latest copy of the policy on the learner. This value growing or spiking is often concerning and can be a sign of training instability. train/kl_divergence_max - max KL value in the whole minibatch. train/kl_loss - value of the KL loss (if any). See --kl_loss_coeff argument for more details. train/loss - the total loss function value. train/lr - the learning rate used by the learner (can be changed by PBT algorithm even if there is no lr scheduler). train/max_abs_logprob - the maximum absolute value of the log probability of any action in the minibatch under the latest policy. If this reaches hundreds or thousands (extremely improbable) it might be a sign that the distributions fluctuate too much, although it can also happen with very complex action distributions, i.e. Tuple action distributions. train/measurements_running_mean , train/measurements_running_std - in this particular example the environment provides the additional observation space called \"measurements\" and these values report the statistics of this observation space. train/num_sgd_steps - number of SGD steps performed on the current trajectories dataset when the summaries are recorded. This can range from 1 to --num_epochs * --num_batches_per_epoch . train/obs_running_mean , train/obs_running_std - the running mean and standard deviation of the observations, reported when --normalize_input is enabled. train/policy_loss - policy gradient loss component of the total loss. train/ratio_max , train/ratio_mean , train/ratio_min - action probability ratio between the latest policy and the policy that collected the experience. Min/max/mean are across the minibatch. train/returns_running_mean , train/returns_running_std - the running mean and standard deviation of bootstrapped discounted returns, reported when --normalize_returns is enabled. train/same_policy_fraction - fraction of samples in the minibatch that come from the same policy. This can be less than 1.0 in multi-policy (i.e. PBT) workflows when we change the policy controlling the agent mid-episode. train/valids_fraction - fraction of samples in the minibatch that are valid. Samples can be invalid if they come from a different policy or if they are too old exceeding --max_policy_lag . In most cases both train/same_policy_fraction and train/valids_fraction should be close to 1.0. train/value - discounted return as predicted by the value function. train/value_delta , train/value_delta_max - how much the value estimate changed between the current critic and the critic at the moment when the experience was collected. Similar to train/ratio... metrics, but for the value function. train/value_loss - value function loss component of the total loss. train/version_diff_avg , train/version_diff_max , train/version_diff_min - policy lag measured in policy versions (SGD steps) between the policy that collected the experience and the latest policy on the learner.","title":"Metrics Reference"},{"location":"05-monitoring/metrics-reference/#metrics-reference","text":"","title":"Metrics Reference"},{"location":"05-monitoring/metrics-reference/#general-information","text":"Each experiment will have at least the following groups of metrics on Tensorboard/Wandb: len perf policy_stats reward stats train Plus new sections (groups) are created for each custom metric with key in <group_name>/<metric_name> format (see Custom Metrics section). Summaries such as len , perf , reward are averaged over the last 100 data points to filter noise (this can be changed by --stats_avg=N argument). These summaries are written to Tensorboard/Wandb every --experiment_summaries_interval seconds (10 seconds by default). train summaries are not averaged and just represent the values from the latest minibatch on the learner. The reporting rate for train summaries is decayed over time to reduce the size of the log files. The schedule is controlled by summary_rate_decay_seconds variable in learner.py .","title":"General information"},{"location":"05-monitoring/metrics-reference/#len","text":"len/len , len/len_max , len/len_min are simply episode lengths measured after frameskip . If your environment uses frameskip=4 and the reported episode length is 400, it means that 400 environment steps were simulated but the agent actually observed only 100 frames.","title":"len"},{"location":"05-monitoring/metrics-reference/#perf","text":"perf/_fps and perf/_sample_throughput represent throughput as measured in different parts of the algorithm. perf/_sample_throughput is the number of observations processed (or actions generated) by the inference worker, i.e. pure sampling throughput measured before frameskipping is taken into account. perf/_fps is the number of observations/actions processed by the learner and measured after frameskipping. For example with frameskip=4, perf/_sample_throughput will be 4 times smaller than perf/_fps . If this is not the case, it means that the learner had to throw away some trajectories which can happen for multiple reasons, for example if the trajectories were too stale and exceeded --max_policy_lag .","title":"perf"},{"location":"05-monitoring/metrics-reference/#policy_stats","text":"By default this section only contains the true_objective metrics: policy_stats/avg_true_objective , policy_stats/avg_true_objective_max , policy_stats/avg_true_objective_min . This will reflect the true_objective value if the environment returns one in the info dictionary (see PBT for more details). If true_objective is not specified these metrics should be equal to the scalar environment reward. policy_stats will also contain any custom metrics (see Custom metrics ) that are not in <group_name>/<metric_name> format.","title":"policy_stats"},{"location":"05-monitoring/metrics-reference/#reward","text":"reward/reward , reward/reward_max , reward/reward_min are the raw scalar environment rewards, reported before any scaling ( --reward_scale ) or normalization is applied.","title":"reward"},{"location":"05-monitoring/metrics-reference/#stats","text":"stats/avg_request_count - how many requests from the rollout workers are processed per inference step. The correpondence between this number and the actual inference batch size depends on training configuration, this is mostly an internal metric for debugging purposes. stats/gpu_cache_learner , stats/gpu_cache_policy_worker , stats/gpu_mem_learner , stats/gpu_mem_policy_worker , stats/gpu_mem_policy_worker , stats/master_process_memory_mb , stats/memory_learner , stats/memory_policy_worker - a group of metrics to keep track of RAM and VRAM usage, mostly used to detect and debug memory leaks. stats/step_policy , stats/wait_policy - performance debugging metrics for the inference worker, respectively the time spent on the last inference step and the time spent waiting for new observations from the rollout workers, both in seconds.","title":"stats"},{"location":"05-monitoring/metrics-reference/#train","text":"This is perhaps the most useful section of metrics, many parameters can be used to debug RL training issues. Metrics are listed and explained below in the alphabetical order in which they appear in Tensorboard. train/actual_lr - the actual learning rate used by the learner, which can be different from the configuration parameter if the adaptive learning rate is enabled. train/adam_max_second_moment - the maximum value of the second moment of the Adam optimizer. Sometimes spikes in this metric can be used to detect training instability. train/adv_max , train/adv_min , train/adv_std - the maximum, minimum, standard deviation of the advantage values. \"Mean\" value is not reported because it is always zero (we use advantage normalization by default). train/entropy - the entropy of the actions probability distribution. train/exploration_loss - exploration loss (if any). See --exploration_loss argument for more details. train/fraction_clipped - fraction of minibatch samples that were clipped by the PPO loss. This value growing too large is often a sign of training instability (i.e. learning rate is too high). train/grad_norm - the L2 norm of the gradient of the loss function after gradient clipping. train/kl_divergence - the average KL-divergence between the policy that collected the experience and the latest copy of the policy on the learner. This value growing or spiking is often concerning and can be a sign of training instability. train/kl_divergence_max - max KL value in the whole minibatch. train/kl_loss - value of the KL loss (if any). See --kl_loss_coeff argument for more details. train/loss - the total loss function value. train/lr - the learning rate used by the learner (can be changed by PBT algorithm even if there is no lr scheduler). train/max_abs_logprob - the maximum absolute value of the log probability of any action in the minibatch under the latest policy. If this reaches hundreds or thousands (extremely improbable) it might be a sign that the distributions fluctuate too much, although it can also happen with very complex action distributions, i.e. Tuple action distributions. train/measurements_running_mean , train/measurements_running_std - in this particular example the environment provides the additional observation space called \"measurements\" and these values report the statistics of this observation space. train/num_sgd_steps - number of SGD steps performed on the current trajectories dataset when the summaries are recorded. This can range from 1 to --num_epochs * --num_batches_per_epoch . train/obs_running_mean , train/obs_running_std - the running mean and standard deviation of the observations, reported when --normalize_input is enabled. train/policy_loss - policy gradient loss component of the total loss. train/ratio_max , train/ratio_mean , train/ratio_min - action probability ratio between the latest policy and the policy that collected the experience. Min/max/mean are across the minibatch. train/returns_running_mean , train/returns_running_std - the running mean and standard deviation of bootstrapped discounted returns, reported when --normalize_returns is enabled. train/same_policy_fraction - fraction of samples in the minibatch that come from the same policy. This can be less than 1.0 in multi-policy (i.e. PBT) workflows when we change the policy controlling the agent mid-episode. train/valids_fraction - fraction of samples in the minibatch that are valid. Samples can be invalid if they come from a different policy or if they are too old exceeding --max_policy_lag . In most cases both train/same_policy_fraction and train/valids_fraction should be close to 1.0. train/value - discounted return as predicted by the value function. train/value_delta , train/value_delta_max - how much the value estimate changed between the current critic and the critic at the moment when the experience was collected. Similar to train/ratio... metrics, but for the value function. train/value_loss - value function loss component of the total loss. train/version_diff_avg , train/version_diff_max , train/version_diff_min - policy lag measured in policy versions (SGD steps) between the policy that collected the experience and the latest policy on the learner.","title":"train"},{"location":"05-monitoring/tensorboard/","text":"Tensorboard \u00b6 Sample Factory uses Tensorboard summaries. Run Tensorboard to monitor any running or finished experiments: tensorboard --logdir = <your_train_dir> --port = 6006 Monitoring multiple experiments \u00b6 Additionally, we provide a helper script that has nice command line interface to monitor select experiment folders using wildcards: python -m sample_factory.utils.tb --dir = ./train_dir '*name_mask*' '*another*mask*' Here --dir parameter is the root directory with experiments, and the script will recursively search for experiment folders that match the masks. Monitoring experiments started by the Launcher \u00b6 Launcher API is a convenient way to start multiple experiments in parallel. Such groups of experiments can be monitored with a single Tensorboard command, just specify --logdir pointing to the root directory with experiments.","title":"Tensorboard"},{"location":"05-monitoring/tensorboard/#tensorboard","text":"Sample Factory uses Tensorboard summaries. Run Tensorboard to monitor any running or finished experiments: tensorboard --logdir = <your_train_dir> --port = 6006","title":"Tensorboard"},{"location":"05-monitoring/tensorboard/#monitoring-multiple-experiments","text":"Additionally, we provide a helper script that has nice command line interface to monitor select experiment folders using wildcards: python -m sample_factory.utils.tb --dir = ./train_dir '*name_mask*' '*another*mask*' Here --dir parameter is the root directory with experiments, and the script will recursively search for experiment folders that match the masks.","title":"Monitoring multiple experiments"},{"location":"05-monitoring/tensorboard/#monitoring-experiments-started-by-the-launcher","text":"Launcher API is a convenient way to start multiple experiments in parallel. Such groups of experiments can be monitored with a single Tensorboard command, just specify --logdir pointing to the root directory with experiments.","title":"Monitoring experiments started by the Launcher"},{"location":"05-monitoring/wandb/","text":"Weights and Biases \u00b6 Sample Factory also supports experiment monitoring with Weights and Biases. In order to setup WandB locally run wandb login in the terminal ( https://docs.wandb.ai/quickstart#1.-set-up-wandb ) Example command line to run an experiment with WandB monitoring: python -m sf_examples.vizdoom.train_vizdoom --env=doom_basic --experiment=DoomBasic --train_dir=./train_dir --num_workers=20 --num_envs_per_worker=16 --train_for_env_steps=1000000 \\\\ --with_wandb=True --wandb_user=<your_wandb_user> --wandb_tags test doom appo A total list of WandB settings: --with_wandb: Enables Weights and Biases integration (default: False) --wandb_user: WandB username (entity). Must be specified from command line! Also see https://docs.wandb.ai/quickstart#1.-set-up-wandb (default: None) --wandb_project: WandB \"Project\" (default: sample_factory) --wandb_group: WandB \"Group\" (to group your experiments). By default this is the name of the env. (default: None) --wandb_job_type: WandB job type (default: SF) --wandb_tags: [WANDB_TAGS [WANDB_TAGS ...]] Tags can help with finding experiments in WandB web console (default: []) Once the experiment is started the link to the monitored session is going to be available in the logs (or you can find it by searching in Wandb Web console).","title":"Weights and Biases"},{"location":"05-monitoring/wandb/#weights-and-biases","text":"Sample Factory also supports experiment monitoring with Weights and Biases. In order to setup WandB locally run wandb login in the terminal ( https://docs.wandb.ai/quickstart#1.-set-up-wandb ) Example command line to run an experiment with WandB monitoring: python -m sf_examples.vizdoom.train_vizdoom --env=doom_basic --experiment=DoomBasic --train_dir=./train_dir --num_workers=20 --num_envs_per_worker=16 --train_for_env_steps=1000000 \\\\ --with_wandb=True --wandb_user=<your_wandb_user> --wandb_tags test doom appo A total list of WandB settings: --with_wandb: Enables Weights and Biases integration (default: False) --wandb_user: WandB username (entity). Must be specified from command line! Also see https://docs.wandb.ai/quickstart#1.-set-up-wandb (default: None) --wandb_project: WandB \"Project\" (default: sample_factory) --wandb_group: WandB \"Group\" (to group your experiments). By default this is the name of the env. (default: None) --wandb_job_type: WandB job type (default: SF) --wandb_tags: [WANDB_TAGS [WANDB_TAGS ...]] Tags can help with finding experiments in WandB web console (default: []) Once the experiment is started the link to the monitored session is going to be available in the logs (or you can find it by searching in Wandb Web console).","title":"Weights and Biases"},{"location":"06-architecture/message-passing/","text":"Event Loops, Signals, and Slots \u00b6 Sample Factory uses a custom mechanism for communication between components inspired by Qt's signals and slots. Unlike in Qt, signals and slots can be used not only across threads, but also across processes. The implementation of this mechanism is available as a separate repository here . The main idea can be summarised as follows: Application is a collection of EventLoop s. Each EventLoop is an infinite loop that occupies a thread or a process. Logic of the system is implemented in EventLoopObject components that live on EventLoop s. Each EventLoop can support multiple EventLoopObject s. Components (i.e. EventLoopObject s) can emit signals. A signal \"message\" contains a name of the signal and the payload (arbitrary data). Components can also connect to signals emitted by other components by specifying a slot function to be called when the signal is received by the EventLoop. The majority of communication between components is done via signals and slots. Some examples: Rollout workers emit \"p{policy_id}_trajectories\" signal when a new trajectory is available, and Batcher's on_new_trajectories() slot is connected to this signal. Inference workers emit \"advance{rollout_worker_idx}\" signal when actions are ready for the next rollout step, and RolloutWorker's advance_rollouts() slot is connected to this signal. Implementation details \u00b6 There's no argument validation for signals and slots. If you connect a slot to a signal with a different signature, it will fail at runtime. This can also be used to your advantage by allowing to propagate arbitrary data as payload with appropriate runtime checks. Signals can be connected to slots only before the processes are spawned, i.e. only during system initialization. This is mostly done by the Runner in connect_components() . It is currently impossible to connect a slot to a signal if emitter and receiver objects belong to event loops already running in different processes (although it should be possible to implement this feature). Connect signals to slots during system initialization. Signal-slot mechanism in the current implementation can't implement a message passing protocol where only a single copy of the signal is received by the subscribers. Signals are always delivered to all connected slots. Use a FIFO multiprocessing queue if you want only one receiver to receive the signal. For example, RolloutWorkers explicitly push requests for new actions into queues corresponding to a policy that controls the agent, and this queue can be processed by any of the multiple InferenceWorkers: inference_queues[policy_id].put(policy_request) Please see https://github.com/alex-petrenko/signal-slot for more information. Multiprocessing queues \u00b6 At the core of the signal-slot mechanism are the queues that are used to pass messages between processes. Python provides a default implementation multiprocessing.Queue , which turns out to be rather slow. Sample Factory uses a custom queue implementation written in C++ using POSIX API that is significantly faster: https://github.com/alex-petrenko/faster-fifo","title":"Event Loops, Signals, and Slots"},{"location":"06-architecture/message-passing/#event-loops-signals-and-slots","text":"Sample Factory uses a custom mechanism for communication between components inspired by Qt's signals and slots. Unlike in Qt, signals and slots can be used not only across threads, but also across processes. The implementation of this mechanism is available as a separate repository here . The main idea can be summarised as follows: Application is a collection of EventLoop s. Each EventLoop is an infinite loop that occupies a thread or a process. Logic of the system is implemented in EventLoopObject components that live on EventLoop s. Each EventLoop can support multiple EventLoopObject s. Components (i.e. EventLoopObject s) can emit signals. A signal \"message\" contains a name of the signal and the payload (arbitrary data). Components can also connect to signals emitted by other components by specifying a slot function to be called when the signal is received by the EventLoop. The majority of communication between components is done via signals and slots. Some examples: Rollout workers emit \"p{policy_id}_trajectories\" signal when a new trajectory is available, and Batcher's on_new_trajectories() slot is connected to this signal. Inference workers emit \"advance{rollout_worker_idx}\" signal when actions are ready for the next rollout step, and RolloutWorker's advance_rollouts() slot is connected to this signal.","title":"Event Loops, Signals, and Slots"},{"location":"06-architecture/message-passing/#implementation-details","text":"There's no argument validation for signals and slots. If you connect a slot to a signal with a different signature, it will fail at runtime. This can also be used to your advantage by allowing to propagate arbitrary data as payload with appropriate runtime checks. Signals can be connected to slots only before the processes are spawned, i.e. only during system initialization. This is mostly done by the Runner in connect_components() . It is currently impossible to connect a slot to a signal if emitter and receiver objects belong to event loops already running in different processes (although it should be possible to implement this feature). Connect signals to slots during system initialization. Signal-slot mechanism in the current implementation can't implement a message passing protocol where only a single copy of the signal is received by the subscribers. Signals are always delivered to all connected slots. Use a FIFO multiprocessing queue if you want only one receiver to receive the signal. For example, RolloutWorkers explicitly push requests for new actions into queues corresponding to a policy that controls the agent, and this queue can be processed by any of the multiple InferenceWorkers: inference_queues[policy_id].put(policy_request) Please see https://github.com/alex-petrenko/signal-slot for more information.","title":"Implementation details"},{"location":"06-architecture/message-passing/#multiprocessing-queues","text":"At the core of the signal-slot mechanism are the queues that are used to pass messages between processes. Python provides a default implementation multiprocessing.Queue , which turns out to be rather slow. Sample Factory uses a custom queue implementation written in C++ using POSIX API that is significantly faster: https://github.com/alex-petrenko/faster-fifo","title":"Multiprocessing queues"},{"location":"06-architecture/overview/","text":"Architecture Overview \u00b6 While a basic implementation of an RL algorithm can fit in a single file , a high-throughput RL system requires a rather sophisticated architecture. This document describes the high-level design of Sample Factory. The following diagram shows the main components of the system and the data flow between them. Please see sections below for more details. High-level design \u00b6 At the core of Sample Factory structure is the idea that RL training can be split into multiple largely independent components, each one of them focusing on a specific task. This enables a modular design where these components can be accelerated/parallelized independently, allowing us to achieve the maximum performance on any RL task. Components interact asynchronously by sending and receving messages (aka signals, see a dedicated section on message passing ). Typically separate components live on different event loops in different processes, although the system is agnostic of whether this is true and it is thus possible to run multiple (or even all components) on a single event loop in a single process. Instead of explicitly sending the data between components (i.e. by serializing observations and sending them across processes), we choose to send the data through shared memory buffers. Each time a component needs to send data to another component, it writes the data to a shared memory buffer and sends a signal containing the buffer ID (essentially a pointer to data). This massively reduces the overhead of message passing. Components \u00b6 Each component is dedicated to a specific task and can be seen as a data processing engine (i.e. each component gets some input by receiving signals, executes a computation, and broadcasts the results by emitting its own signals). These are the main components of Sample Factory: Rollout Workers are responsible for environment simulation. Rollout workers receive actions from the policy, do environment step() and produce observations after each step and full trajectories after --rollout steps. Inference Workers receive observations and hidden states and produce actions. The policy on each inference worker is updated after each SGD step on the learner. Batcher receives trajectories from rollout workers, puts them together and produces datasets of data for the learner. Learner gets batches of data from the batcher, splits them into minibatches and does --num_epochs of stochastic gradient descent. After each SGD step the updated weights are written to shared memory buffers and the corresponding signal is broadcasted. Runner is a component that bootstraps the whole system, receives all sorts of statistics from other components and takes care of logging and summary writing. Sampler , although technically its own component that can send and receive signals, in the typical configuration is nothing more than a thin wrapper around Rollout/Inference workers and serves as an interface to the rest of the system. (Although this interface allows us to create alternative samplers i.e. single-process synchronous JAX-optimized sampler is an idea) Rollout Workers \u00b6 The number of rollout workers is controlled by --num_workers . Each rollout worker can simulate one or multiple environments serially in the same process. The number of environments per worker is controlled by --num_envs_per_worker . Each rollout worker contains >= 1 of VectorEnvRunner objects, the number of which is controlled by --worker_num_splits . The default value of this parameter is 2, which enables double-buffered sampling . The number of envs on each VectorEnvRunner is thus num_envs_per_worker // worker_num_splits and therefore --num_envs_per_worker must be divisible by --worker_num_splits . Inference Workers \u00b6 Each policy (see multi-policy training ) has >= 1 corresponding inference workers which generate actions for the agents controlled by this policy. The number of inference workers is controlled by --policy_workers_per_policy . Batcher \u00b6 There's typically a single batcher per policy in the system. The batcher receives trajectories from rollout workers and puts them together into a dataset available for training. In batched sampling mode this is pretty much a no-op, the batcher just passes the data through. In non-batched sampling mode this is a non-trivial process, since rollouts from different workers finish asynchronously and need to be put in the contiguous tensor for minibatch SGD. Although batcher is it's own component, in the default configuration we run it in the same process as the learner (but in a separate thread) in order to minimize the number of CUDA contexts and thus VRAM usage. Learner \u00b6 There's typically a single learner per policy in the system. Trajectory datasets flow in and updated parameters flow out. Terminology \u00b6 Some terminology used in the codebase and in the further documentation: rollout or trajectory is a sequence of observations, actions, rewards, etc. produced by a single agent. dataset (or training batch or sometimes just batch ) is a collection of trajectories produced by >=1 agents. Datasets are split into minibatches and >=1 epochs of SGD are performed. Minibatch size is determined by --batch_size and number of epochs is determined by --num_epochs . Dataset size is batch_size * num_batches_per_epoch , and in total batch_size * num_batches_per_epoch * num_epochs SGD steps are performed on each dataset (sorry for the obvious confusion between \"batch\" and \"minibatch\" terms, the parameter names are kept largely for legacy reasons). signals are messages sent between components. Signals are connected to slots , which are functions that are called when a signal is received. This mechanism is inspired by Qt's signals and slots (see the dedicated section on message passing ). shared memory buffers are PyTorch tensors shared between processes, created with share_memory_() method.","title":"Architecture Overview"},{"location":"06-architecture/overview/#architecture-overview","text":"While a basic implementation of an RL algorithm can fit in a single file , a high-throughput RL system requires a rather sophisticated architecture. This document describes the high-level design of Sample Factory. The following diagram shows the main components of the system and the data flow between them. Please see sections below for more details.","title":"Architecture Overview"},{"location":"06-architecture/overview/#high-level-design","text":"At the core of Sample Factory structure is the idea that RL training can be split into multiple largely independent components, each one of them focusing on a specific task. This enables a modular design where these components can be accelerated/parallelized independently, allowing us to achieve the maximum performance on any RL task. Components interact asynchronously by sending and receving messages (aka signals, see a dedicated section on message passing ). Typically separate components live on different event loops in different processes, although the system is agnostic of whether this is true and it is thus possible to run multiple (or even all components) on a single event loop in a single process. Instead of explicitly sending the data between components (i.e. by serializing observations and sending them across processes), we choose to send the data through shared memory buffers. Each time a component needs to send data to another component, it writes the data to a shared memory buffer and sends a signal containing the buffer ID (essentially a pointer to data). This massively reduces the overhead of message passing.","title":"High-level design"},{"location":"06-architecture/overview/#components","text":"Each component is dedicated to a specific task and can be seen as a data processing engine (i.e. each component gets some input by receiving signals, executes a computation, and broadcasts the results by emitting its own signals). These are the main components of Sample Factory: Rollout Workers are responsible for environment simulation. Rollout workers receive actions from the policy, do environment step() and produce observations after each step and full trajectories after --rollout steps. Inference Workers receive observations and hidden states and produce actions. The policy on each inference worker is updated after each SGD step on the learner. Batcher receives trajectories from rollout workers, puts them together and produces datasets of data for the learner. Learner gets batches of data from the batcher, splits them into minibatches and does --num_epochs of stochastic gradient descent. After each SGD step the updated weights are written to shared memory buffers and the corresponding signal is broadcasted. Runner is a component that bootstraps the whole system, receives all sorts of statistics from other components and takes care of logging and summary writing. Sampler , although technically its own component that can send and receive signals, in the typical configuration is nothing more than a thin wrapper around Rollout/Inference workers and serves as an interface to the rest of the system. (Although this interface allows us to create alternative samplers i.e. single-process synchronous JAX-optimized sampler is an idea)","title":"Components"},{"location":"06-architecture/overview/#rollout-workers","text":"The number of rollout workers is controlled by --num_workers . Each rollout worker can simulate one or multiple environments serially in the same process. The number of environments per worker is controlled by --num_envs_per_worker . Each rollout worker contains >= 1 of VectorEnvRunner objects, the number of which is controlled by --worker_num_splits . The default value of this parameter is 2, which enables double-buffered sampling . The number of envs on each VectorEnvRunner is thus num_envs_per_worker // worker_num_splits and therefore --num_envs_per_worker must be divisible by --worker_num_splits .","title":"Rollout Workers"},{"location":"06-architecture/overview/#inference-workers","text":"Each policy (see multi-policy training ) has >= 1 corresponding inference workers which generate actions for the agents controlled by this policy. The number of inference workers is controlled by --policy_workers_per_policy .","title":"Inference Workers"},{"location":"06-architecture/overview/#batcher","text":"There's typically a single batcher per policy in the system. The batcher receives trajectories from rollout workers and puts them together into a dataset available for training. In batched sampling mode this is pretty much a no-op, the batcher just passes the data through. In non-batched sampling mode this is a non-trivial process, since rollouts from different workers finish asynchronously and need to be put in the contiguous tensor for minibatch SGD. Although batcher is it's own component, in the default configuration we run it in the same process as the learner (but in a separate thread) in order to minimize the number of CUDA contexts and thus VRAM usage.","title":"Batcher"},{"location":"06-architecture/overview/#learner","text":"There's typically a single learner per policy in the system. Trajectory datasets flow in and updated parameters flow out.","title":"Learner"},{"location":"06-architecture/overview/#terminology","text":"Some terminology used in the codebase and in the further documentation: rollout or trajectory is a sequence of observations, actions, rewards, etc. produced by a single agent. dataset (or training batch or sometimes just batch ) is a collection of trajectories produced by >=1 agents. Datasets are split into minibatches and >=1 epochs of SGD are performed. Minibatch size is determined by --batch_size and number of epochs is determined by --num_epochs . Dataset size is batch_size * num_batches_per_epoch , and in total batch_size * num_batches_per_epoch * num_epochs SGD steps are performed on each dataset (sorry for the obvious confusion between \"batch\" and \"minibatch\" terms, the parameter names are kept largely for legacy reasons). signals are messages sent between components. Signals are connected to slots , which are functions that are called when a signal is received. This mechanism is inspired by Qt's signals and slots (see the dedicated section on message passing ). shared memory buffers are PyTorch tensors shared between processes, created with share_memory_() method.","title":"Terminology"},{"location":"07-advanced-topics/batched-non-batched/","text":"Batched/Non-Batched Sampling \u00b6 Sample Factory has two different implementations of the RolloutWorker: batched and non-batched. You can switch between them using --batched_sampling=[True|False] argument. Non-Batched Sampling \u00b6 Non-batched sampling is the default mode. It makes very few assumptions about the environment and is the most flexible. We call it non-batched because it treats each agent in each environment independently and processes trajectories one by one. One advantage of this approach is that we can control each agent in each environment with any policy which can be very useful for multi-agent, self-play, and PBT setups. A downside of this mode is that we have to batch individual observations from rollout workers before we can do inference on the GPU (because GPUs are most efficient with big batches). This makes non-batched mode very inefficient for vectorized environments like IsaacGym . Batched Sampling \u00b6 Batched mode is perfect for massively vectorized environments like IsaacGym or EnvPool . It assumes that the observations are available in one large tensor that we can directly give to the inference worker for processing. For GPU-accelerated environments we can sample thousands of observations in a single tensor that is already on GPU and thus achieve the maximum possible throughput. It is common that batched mode is used with a single rollout worker --num_workers=1 and a single inference worker --num_inference_workers=1 and the parallelization of environment simulation is handled by the environment itself. Although PBT can be used with batched sampling, we do not support controlling individual agents with different policies in this mode. For regular CPU-based envs (Atari, VizDoom, Mujoco) the difference between batched and non-batched sampling is negligible, either mode should work fine. Observations \u00b6 In Sample Factory for simplicity all environments have dictionary observations (or converted to dictionary with a single key obs ). In non-batched mode even with multi-agent envs each agent thus provides a separate observation dictionary. In batched mode we want to work with big tensors, so instead of a list of dictionaries we have a single dictionary with a tensor of observations for each key.","title":"Batched/Non-Batched Sampling"},{"location":"07-advanced-topics/batched-non-batched/#batchednon-batched-sampling","text":"Sample Factory has two different implementations of the RolloutWorker: batched and non-batched. You can switch between them using --batched_sampling=[True|False] argument.","title":"Batched/Non-Batched Sampling"},{"location":"07-advanced-topics/batched-non-batched/#non-batched-sampling","text":"Non-batched sampling is the default mode. It makes very few assumptions about the environment and is the most flexible. We call it non-batched because it treats each agent in each environment independently and processes trajectories one by one. One advantage of this approach is that we can control each agent in each environment with any policy which can be very useful for multi-agent, self-play, and PBT setups. A downside of this mode is that we have to batch individual observations from rollout workers before we can do inference on the GPU (because GPUs are most efficient with big batches). This makes non-batched mode very inefficient for vectorized environments like IsaacGym .","title":"Non-Batched Sampling"},{"location":"07-advanced-topics/batched-non-batched/#batched-sampling","text":"Batched mode is perfect for massively vectorized environments like IsaacGym or EnvPool . It assumes that the observations are available in one large tensor that we can directly give to the inference worker for processing. For GPU-accelerated environments we can sample thousands of observations in a single tensor that is already on GPU and thus achieve the maximum possible throughput. It is common that batched mode is used with a single rollout worker --num_workers=1 and a single inference worker --num_inference_workers=1 and the parallelization of environment simulation is handled by the environment itself. Although PBT can be used with batched sampling, we do not support controlling individual agents with different policies in this mode. For regular CPU-based envs (Atari, VizDoom, Mujoco) the difference between batched and non-batched sampling is negligible, either mode should work fine.","title":"Batched Sampling"},{"location":"07-advanced-topics/batched-non-batched/#observations","text":"In Sample Factory for simplicity all environments have dictionary observations (or converted to dictionary with a single key obs ). In non-batched mode even with multi-agent envs each agent thus provides a separate observation dictionary. In batched mode we want to work with big tensors, so instead of a list of dictionaries we have a single dictionary with a tensor of observations for each key.","title":"Observations"},{"location":"07-advanced-topics/double-buffered/","text":"Double-Buffered Sampling \u00b6 Sample Factory supports accelerated sampling regime called double-buffered sampling which can be enabled by setting --worker_num_splits=2 (default value) and --num_envs_per_worker to a multiple of 2. Note that this feature is independent of sync/async or batched/non-batched mode and can be used in any configuration. Explanation \u00b6 Experience collection in RL is normally a sequential process. We can't collect the next observation until we generate an action based on the current observation. This means that for CPU-based environments we can't use our CPU cores when we're waiting for the inference to finish. This is a waste of resources and it slows down training. Double-buffered sampling solves this problem by simulating 2*N environments serially in the same process. While we're waiting for the inference to finish on the first N environments, we can already collect observations from the next N environments. The diagram below shows how this works: Additionally, take a look at this animation that demonstrates double-buffered sampling: https://www.youtube.com/watch?v=0AyaeLqXQc4","title":"Double-Buffered Sampling"},{"location":"07-advanced-topics/double-buffered/#double-buffered-sampling","text":"Sample Factory supports accelerated sampling regime called double-buffered sampling which can be enabled by setting --worker_num_splits=2 (default value) and --num_envs_per_worker to a multiple of 2. Note that this feature is independent of sync/async or batched/non-batched mode and can be used in any configuration.","title":"Double-Buffered Sampling"},{"location":"07-advanced-topics/double-buffered/#explanation","text":"Experience collection in RL is normally a sequential process. We can't collect the next observation until we generate an action based on the current observation. This means that for CPU-based environments we can't use our CPU cores when we're waiting for the inference to finish. This is a waste of resources and it slows down training. Double-buffered sampling solves this problem by simulating 2*N environments serially in the same process. While we're waiting for the inference to finish on the first N environments, we can already collect observations from the next N environments. The diagram below shows how this works: Additionally, take a look at this animation that demonstrates double-buffered sampling: https://www.youtube.com/watch?v=0AyaeLqXQc4","title":"Explanation"},{"location":"07-advanced-topics/inactive-agents/","text":"Inactive Agents \u00b6 Sometimes it makes sense to disable some of the agents in a multi-agent environment. For example, in a multi-player game some agents might die in the middle of the episode and should not contribute any rollouts until the episode reset. In order to disable (deactivate) the agent, add info[\"is_active\"] = False in the env.step() call, i.e. the agent's info dict should contain is_active key with False value. Absent is_active key or is_active=True is treated as active agent. When the agent is deactivated in the middle of the rollout, the inactive part of the rollout is treated as invalid data by the learner (similar to any other invalid data, i.e. experience that exceeds --max_policy_lag ). We carefully mask this invalid data on the learner for loss & advantages calculations. Therefore any inactive data makes the effective batch size smaller, so we decrease the learning rate accordingly, otherwise batches with >90% invalid data would produce very noisy parameter updates. It is generally advised that the portion of inactive data ( train/valids_fraction on Tensorboard/WandB) does not exceed 50%, otherwise it may seriously affect training dynamics and requires careful tuning. There are also alternative ways to treat inactive agents, for example just feeding them some special observation (e.g. all zeros) and zero rewards until the episode reset. Inactive agents are currently only supported in non-batched sampling mode ( --batched_sampling=False ). Examples \u00b6 sf_examples/train_custom_multi_env.py - shows how to use inactive agents in a custom multi-agent environment. Inactive agents are a new feature, suggestions & contributions are welcome!","title":"Inactive Agents"},{"location":"07-advanced-topics/inactive-agents/#inactive-agents","text":"Sometimes it makes sense to disable some of the agents in a multi-agent environment. For example, in a multi-player game some agents might die in the middle of the episode and should not contribute any rollouts until the episode reset. In order to disable (deactivate) the agent, add info[\"is_active\"] = False in the env.step() call, i.e. the agent's info dict should contain is_active key with False value. Absent is_active key or is_active=True is treated as active agent. When the agent is deactivated in the middle of the rollout, the inactive part of the rollout is treated as invalid data by the learner (similar to any other invalid data, i.e. experience that exceeds --max_policy_lag ). We carefully mask this invalid data on the learner for loss & advantages calculations. Therefore any inactive data makes the effective batch size smaller, so we decrease the learning rate accordingly, otherwise batches with >90% invalid data would produce very noisy parameter updates. It is generally advised that the portion of inactive data ( train/valids_fraction on Tensorboard/WandB) does not exceed 50%, otherwise it may seriously affect training dynamics and requires careful tuning. There are also alternative ways to treat inactive agents, for example just feeding them some special observation (e.g. all zeros) and zero rewards until the episode reset. Inactive agents are currently only supported in non-batched sampling mode ( --batched_sampling=False ).","title":"Inactive Agents"},{"location":"07-advanced-topics/inactive-agents/#examples","text":"sf_examples/train_custom_multi_env.py - shows how to use inactive agents in a custom multi-agent environment. Inactive agents are a new feature, suggestions & contributions are welcome!","title":"Examples"},{"location":"07-advanced-topics/multi-policy-training/","text":"Multi-Policy Training \u00b6 Sample Factory supports training multiple policies at the same time with --num_policies=N , where N is the number of policies to train. Single-agent environments \u00b6 Multi-policy training with single-agent environments is equivalent to just running multiple experiments with different seeds. We actually recommend running separate experiments in this case because experiment monitoring is easier this way. Multi-agent environments \u00b6 In multi-agent and self-play environments it can be beneficial to train multiple policies at once to avoid overfitting to a single opponent (i.e. self). If this is the desired training mode, it is important that we control multiple agents in the same environment with different policies. This is controlled by the argument --pbt_mix_policies_in_one_env , which is set to True by default. Although this argument has --pbt prefix, it actually applies regardless of whether we're training with PBT or not. If --pbt_mix_policies_in_one_env=True , then we will periodically randomly resample policies controlling each agent in the environment. This is implemented in sample_factory.algo.utils.agent_policy_mapping . Feel free to fork the repo and modify this class to create your own custom mapping. Exposing agent_policy_mapping through API to allow custom mapping is an obvious improvement, and contributions here are welcome! Implementation details \u00b6 GPU mapping \u00b6 On a multi-GPU machine we assign each policy to a separate GPU. Or, if we have fewer GPUs than policies, we will fill the GPUs with policies until we run out of GPUs, and then start reusing GPUs. For example, on a 2-GPU machine 4-policy training will look like this: GPU 0: policy 0, policy 2 GPU 1: policy 1, policy 3 Multi-policy training in different modes \u00b6 All features of multi-policy training (including mixing different policies in one env) are only supported with asynchronous ( --async_rl=True ) non-batched ( --batched_sampling=False ) training. In synchronous mode we can still use multi-policy training, but the mapping between agents and policies is fixed and deterministic because we need to guarantee the same amount of experience for all policies. In batched mode we can also use multi-policy training, but mixing policies in one environment is not supported. This would defeat the purpose of batched mode where we want to directly transfer a large vector of observations on the GPU and do inference. Arbitrary mapping between agents and policies would make this significantly slower and more complicated. That said, it might still make a lot of sense to use multi-policy training in batched mode/sync mode in the context of Population-Based Training, i.e. to optimize hyperparameters of agents in the population. See Population-Based Training for more details.","title":"Multi-Policy Training"},{"location":"07-advanced-topics/multi-policy-training/#multi-policy-training","text":"Sample Factory supports training multiple policies at the same time with --num_policies=N , where N is the number of policies to train.","title":"Multi-Policy Training"},{"location":"07-advanced-topics/multi-policy-training/#single-agent-environments","text":"Multi-policy training with single-agent environments is equivalent to just running multiple experiments with different seeds. We actually recommend running separate experiments in this case because experiment monitoring is easier this way.","title":"Single-agent environments"},{"location":"07-advanced-topics/multi-policy-training/#multi-agent-environments","text":"In multi-agent and self-play environments it can be beneficial to train multiple policies at once to avoid overfitting to a single opponent (i.e. self). If this is the desired training mode, it is important that we control multiple agents in the same environment with different policies. This is controlled by the argument --pbt_mix_policies_in_one_env , which is set to True by default. Although this argument has --pbt prefix, it actually applies regardless of whether we're training with PBT or not. If --pbt_mix_policies_in_one_env=True , then we will periodically randomly resample policies controlling each agent in the environment. This is implemented in sample_factory.algo.utils.agent_policy_mapping . Feel free to fork the repo and modify this class to create your own custom mapping. Exposing agent_policy_mapping through API to allow custom mapping is an obvious improvement, and contributions here are welcome!","title":"Multi-agent environments"},{"location":"07-advanced-topics/multi-policy-training/#implementation-details","text":"","title":"Implementation details"},{"location":"07-advanced-topics/multi-policy-training/#gpu-mapping","text":"On a multi-GPU machine we assign each policy to a separate GPU. Or, if we have fewer GPUs than policies, we will fill the GPUs with policies until we run out of GPUs, and then start reusing GPUs. For example, on a 2-GPU machine 4-policy training will look like this: GPU 0: policy 0, policy 2 GPU 1: policy 1, policy 3","title":"GPU mapping"},{"location":"07-advanced-topics/multi-policy-training/#multi-policy-training-in-different-modes","text":"All features of multi-policy training (including mixing different policies in one env) are only supported with asynchronous ( --async_rl=True ) non-batched ( --batched_sampling=False ) training. In synchronous mode we can still use multi-policy training, but the mapping between agents and policies is fixed and deterministic because we need to guarantee the same amount of experience for all policies. In batched mode we can also use multi-policy training, but mixing policies in one environment is not supported. This would defeat the purpose of batched mode where we want to directly transfer a large vector of observations on the GPU and do inference. Arbitrary mapping between agents and policies would make this significantly slower and more complicated. That said, it might still make a lot of sense to use multi-policy training in batched mode/sync mode in the context of Population-Based Training, i.e. to optimize hyperparameters of agents in the population. See Population-Based Training for more details.","title":"Multi-policy training in different modes"},{"location":"07-advanced-topics/normalizations/","text":"Observation and Return Normalization \u00b6 Neural networks are known to perform better when the input data and prediction targets are normalized. Sample Factory provides normalization for observations and returns. Normalization works by collecting running mean and standard deviation statistics of the data for the entire training process, and then using these statistics to normalize the data to approximately zero mean and unit variance. This way of normalization has proven more effective than per-batch normalization since it changes the statistics slowly and smoothly as observations and returns change, and thus allows the networks to adapt to the new statistics. Observation normalization \u00b6 Enable observation normalization by setting --normalize_input to True . If your environment provides dictionary observations, you can specify which keys to normalize by setting --normalize_input_keys key1 key2 key3 (defaults to all keys). Observation normalization in some cases can massively improve sample efficiency. I.e. below is VizDoom \"doom_basic\" environment with (blue) and without (orange) observation normalization: Return normalization \u00b6 Enable return normalization by setting --normalize_returns to True . In addition to stabilizing training and reducing the critic loss, return normalization eliminates the need for careful tuning of reward scaling. In the example below we train an agent on a relatively complex continuous control task, and the version with return normalization not only trains faster, but also consistently reaches much higher reward. There's no guarantee that normalization of observations and returns will always help, experiment with your environment to see if it helps. Advantage normalization \u00b6 In addition to observation and return normalization, Sample Factory also normalizes advantages. Unlike observation and return normalization, advantage normalization is not based on running statistics, but instead uses per-batch statistics. We found that this configuration performs well in many different domains.","title":"Observation and Return Normalization"},{"location":"07-advanced-topics/normalizations/#observation-and-return-normalization","text":"Neural networks are known to perform better when the input data and prediction targets are normalized. Sample Factory provides normalization for observations and returns. Normalization works by collecting running mean and standard deviation statistics of the data for the entire training process, and then using these statistics to normalize the data to approximately zero mean and unit variance. This way of normalization has proven more effective than per-batch normalization since it changes the statistics slowly and smoothly as observations and returns change, and thus allows the networks to adapt to the new statistics.","title":"Observation and Return Normalization"},{"location":"07-advanced-topics/normalizations/#observation-normalization","text":"Enable observation normalization by setting --normalize_input to True . If your environment provides dictionary observations, you can specify which keys to normalize by setting --normalize_input_keys key1 key2 key3 (defaults to all keys). Observation normalization in some cases can massively improve sample efficiency. I.e. below is VizDoom \"doom_basic\" environment with (blue) and without (orange) observation normalization:","title":"Observation normalization"},{"location":"07-advanced-topics/normalizations/#return-normalization","text":"Enable return normalization by setting --normalize_returns to True . In addition to stabilizing training and reducing the critic loss, return normalization eliminates the need for careful tuning of reward scaling. In the example below we train an agent on a relatively complex continuous control task, and the version with return normalization not only trains faster, but also consistently reaches much higher reward. There's no guarantee that normalization of observations and returns will always help, experiment with your environment to see if it helps.","title":"Return normalization"},{"location":"07-advanced-topics/normalizations/#advantage-normalization","text":"In addition to observation and return normalization, Sample Factory also normalizes advantages. Unlike observation and return normalization, advantage normalization is not based on running statistics, but instead uses per-batch statistics. We found that this configuration performs well in many different domains.","title":"Advantage normalization"},{"location":"07-advanced-topics/observer/","text":"Observer Interface \u00b6 Sample Factory version 2 introduces a new feature: you can wrap the RL algorithm with a custom Observer object which allows you to interact with the RL training process in an arbitrary way. AlgoObserver \u00b6 The AlgoObserver interface is defined as follows: class AlgoObserver : def on_init ( self , runner : Runner ) -> None : \"\"\"Called after ctor, but before signal-slots are connected or any processes are started.\"\"\" pass def on_connect_components ( self , runner : Runner ) -> None : \"\"\"Connect additional signal-slot pairs in the observers if needed.\"\"\" pass def on_start ( self , runner : Runner ) -> None : \"\"\"Called right after sampling/learning processes are started.\"\"\" pass def on_training_step ( self , runner : Runner , training_iteration_since_resume : int ) -> None : \"\"\"Called after each training step.\"\"\" pass def extra_summaries ( self , runner : Runner , policy_id : PolicyID , env_steps : int , writer : SummaryWriter ) -> None : pass def on_stop ( self , runner : Runner ) -> None : pass Define your own class derived from AlgoObserver (i.e. MyObserver ) and register it before starting the training process: runner . register_observer ( MyObserver ()) Our DMLab integration provides an example of how to use AlgoObserver to implement custom summaries that aggregate information from multiple custom metrics (see sf_examples/dmlab/train_dmlab.py ). AlgoObserver is a new feature and further suggestions/extensions are welcome!","title":"Observer Interface"},{"location":"07-advanced-topics/observer/#observer-interface","text":"Sample Factory version 2 introduces a new feature: you can wrap the RL algorithm with a custom Observer object which allows you to interact with the RL training process in an arbitrary way.","title":"Observer Interface"},{"location":"07-advanced-topics/observer/#algoobserver","text":"The AlgoObserver interface is defined as follows: class AlgoObserver : def on_init ( self , runner : Runner ) -> None : \"\"\"Called after ctor, but before signal-slots are connected or any processes are started.\"\"\" pass def on_connect_components ( self , runner : Runner ) -> None : \"\"\"Connect additional signal-slot pairs in the observers if needed.\"\"\" pass def on_start ( self , runner : Runner ) -> None : \"\"\"Called right after sampling/learning processes are started.\"\"\" pass def on_training_step ( self , runner : Runner , training_iteration_since_resume : int ) -> None : \"\"\"Called after each training step.\"\"\" pass def extra_summaries ( self , runner : Runner , policy_id : PolicyID , env_steps : int , writer : SummaryWriter ) -> None : pass def on_stop ( self , runner : Runner ) -> None : pass Define your own class derived from AlgoObserver (i.e. MyObserver ) and register it before starting the training process: runner . register_observer ( MyObserver ()) Our DMLab integration provides an example of how to use AlgoObserver to implement custom summaries that aggregate information from multiple custom metrics (see sf_examples/dmlab/train_dmlab.py ). AlgoObserver is a new feature and further suggestions/extensions are welcome!","title":"AlgoObserver"},{"location":"07-advanced-topics/passing-info/","text":"Passing Info from RL Algo to Env \u00b6 Custom summary metrics provide a way to pass information from the RL environment to the training system (i.e. success rate, etc.) In some RL workflows it might be desirable to also pass information in the opposide direction: from the RL algorithm to the environment. This can enable, for example, curriculum learning based on the number of training steps consumed by the agent (or any other metric of the training progress). We provide a way to do this by passing a training_info dictionary to the environment. In order to do this, your environment needs to implement TrainingInfoInterface . class TrainingInfoInterface : def __init__ ( self ): self . training_info : Dict [ str , Any ] = dict () def set_training_info ( self , training_info ): \"\"\" Send the training information to the environment, i.e. number of training steps so far. Some environments rely on that i.e. to implement curricula. :param training_info: dictionary containing information about the current training session. Guaranteed to contain 'approx_total_training_steps' (approx because it lags a bit behind due to multiprocess synchronization) \"\"\" self . training_info = training_info Currently we only pass approx_total_training_steps to the environment which should be enough for simple curricula. Feel free to fork the repo and add more information to this dictionary by modifying _propagate_training_info() in runner.py . This is a new feature and further suggestions/extensions are welcome. Note that if your environment uses a chain of wrappers (e.g. env = Wrapper3(Wrapper2(Wrapper1(env))) ), then it is sufficient that any Wrapper in the chain implements TrainingInfoInterface . Sample Factory will unwrap the outer wrappers until it finds the first one that implements TrainingInfoInterface . Additional notes on curriculum learning \u00b6 Curriculum based on the training progress is not the only way to implement curriculum learning. In most cases, you can actually do it without knowing anything about the outer training loop. An alternative approach is to implement curriculum based on the agent's performance in the current environment instance, i.e. by averaging episodic statistics over the last N episodes. This way the resulting curriculum is more smooth and stochastic, which can actually create more robust policies, since different environment instances can be at different levels of difficulty and thus produce more diverse data. We used this approach to train our agents against bots in the original Sample Factory paper .","title":"Passing Info from RL Algo to Env"},{"location":"07-advanced-topics/passing-info/#passing-info-from-rl-algo-to-env","text":"Custom summary metrics provide a way to pass information from the RL environment to the training system (i.e. success rate, etc.) In some RL workflows it might be desirable to also pass information in the opposide direction: from the RL algorithm to the environment. This can enable, for example, curriculum learning based on the number of training steps consumed by the agent (or any other metric of the training progress). We provide a way to do this by passing a training_info dictionary to the environment. In order to do this, your environment needs to implement TrainingInfoInterface . class TrainingInfoInterface : def __init__ ( self ): self . training_info : Dict [ str , Any ] = dict () def set_training_info ( self , training_info ): \"\"\" Send the training information to the environment, i.e. number of training steps so far. Some environments rely on that i.e. to implement curricula. :param training_info: dictionary containing information about the current training session. Guaranteed to contain 'approx_total_training_steps' (approx because it lags a bit behind due to multiprocess synchronization) \"\"\" self . training_info = training_info Currently we only pass approx_total_training_steps to the environment which should be enough for simple curricula. Feel free to fork the repo and add more information to this dictionary by modifying _propagate_training_info() in runner.py . This is a new feature and further suggestions/extensions are welcome. Note that if your environment uses a chain of wrappers (e.g. env = Wrapper3(Wrapper2(Wrapper1(env))) ), then it is sufficient that any Wrapper in the chain implements TrainingInfoInterface . Sample Factory will unwrap the outer wrappers until it finds the first one that implements TrainingInfoInterface .","title":"Passing Info from RL Algo to Env"},{"location":"07-advanced-topics/passing-info/#additional-notes-on-curriculum-learning","text":"Curriculum based on the training progress is not the only way to implement curriculum learning. In most cases, you can actually do it without knowing anything about the outer training loop. An alternative approach is to implement curriculum based on the agent's performance in the current environment instance, i.e. by averaging episodic statistics over the last N episodes. This way the resulting curriculum is more smooth and stochastic, which can actually create more robust policies, since different environment instances can be at different levels of difficulty and thus produce more diverse data. We used this approach to train our agents against bots in the original Sample Factory paper .","title":"Additional notes on curriculum learning"},{"location":"07-advanced-topics/pbt/","text":"Population-Based Training \u00b6 Sample Factory contains an implementation of the Population-Based Training algorithm. See PBT paper and original Sample Factory paper for more details. PBT is a hyperparameter optimization algorithm that can be used to train RL agents. Instead of manually tuning all hyperparameters, you can let an optimization method do it for you. This can include not only learning parameters (e.g. learning rate, entropy coefficient), but also environment parameters (e.g. reward function coefficients). It is common in RL to have a sophisticated (shaped) reward function which guides the exploration process. As a result such reward function can distract the agent from the actual final goal. PBT allows you to optimize with respect to some sparse final objective (which we call \"true_objective\") while still using a shaped reward function. Theoretically the algorithm should find hyperparameters (including shaping coefficients) that lead to the best final objective. This can be, for example, directly optimizing for just winning a match in a multiplayer game, which would be very difficult to do with just regular RL because of the sparsity of such objective. This type of PBT algorithm is implemented in the FTW agent by DeepMind. Algorithm \u00b6 PBT works similar to a genetic algorithm. A population of agents is trained simultaneously with roughly the following approach: Each agent is assigned a set of hyperparameters (e.g. learning rate, entropy coefficient, reward function coefficients, etc.) Each agent is trained for a fixed number of steps (e.g. 5M steps) At the end of this meta-training epoch, the performance of all agents is ranked: Agents with top K % of performance are unchanged, we just keep training them Agents with bottom K % of performance are replaced by a copy of a random top-K % agent with mutated hyperparameters. Agents in the middle keep their weights but also get mutated hyperparameters. Proceed to the next meta-training epoch. Current version of PBT is implemented for a single machine. The perfect setup is a multi-GPU server that can train multiple agents at the same time. For example, we can train a population of 8 agents on a 4-GPU machine, training 2 agents on each GPU. PBT is perfect for multiplayer game scenarios where training a population of agents against one another yields much more robust results compared to self-play with a single policy. Providing \"True Objective\" to PBT \u00b6 In order to optimize for a true objective, you need to return it from the environment. Just add it to the info dictionary returned by the environment at the last step of the episode, e.g.: def step ( self , action ): info = {} ... info [ 'true_objective' ] = self . compute_true_objective () return obs , reward , terminated , truncated , info In the absence of true_objective in the info dictionary, PBT will use the regular reward as the objective. Learning parameters optimized by PBT \u00b6 See population_based_training.py : HYPERPARAMS_TO_TUNE = { \"learning_rate\" , \"exploration_loss_coeff\" , \"value_loss_coeff\" , \"max_grad_norm\" , \"ppo_clip_ratio\" , \"ppo_clip_value\" , # gamma can be added with a CLI parameter (--pbt_optimize_gamma=True) } During training current learning parameters are saved in f\"policy_{policy_id:02d}_cfg.json\" files in the experiment directory. Optimizing environment parameters \u00b6 Besides learning parameters we can also optimize parameters of the environment with respect to some \"true objective\". In order to do that, your environment should implement RewardShapingInterface interface in addition to gym.Env interface. class RewardShapingInterface : def get_default_reward_shaping ( self ) -> Optional [ Dict [ str , Any ]]: \"\"\"Should return a dictionary of string:float key-value pairs defining the current reward shaping scheme.\"\"\" raise NotImplementedError def set_reward_shaping ( self , reward_shaping : Dict [ str , Any ], agent_idx : int | slice ) -> None : \"\"\" Sets the new reward shaping scheme. :param reward_shaping dictionary of string-float key-value pairs :param agent_idx: integer agent index (for multi-agent envs). Can be a slice if we're training in batched mode (set a single reward shaping scheme for a range of agents) \"\"\" raise NotImplementedError Any parameters in the dictionary returned by get_default_reward_shaping will be optimized by PBT. Note that although the dictionary is called \"reward shaping\", it can be used to optimize any environment parameters. It is important that none of these parameters should directly affect the objective calculation, otherwise all PBT will do is increase the coefficients all the way to infinity. An example of how this can be used. Suppose your shaped reward function contains a term for picking up a weapon in a game like Quake or VizDoom. If the true objective is 1.0 for winning the game and 0.0 otherwise then PBT can optimize these weapon preference coefficients to maximize success. But if true objective is not specified (so just the env reward itself is used as objective), then you can just increase the coefficients to increase the reward unboundedly. Configuring PBT \u00b6 Please see Configuration parameter reference . Parameters with --pbt_ prefix are related to PBT. Use --with_pbt=True to enable PBT. It is important also to set --num_policies to the number of agents in the population. Command-line examples \u00b6 Training on DMLab-30 with a 4-agent population on a 4-GPU machine: python -m sf_examples.dmlab.train_dmlab --env = dmlab_30 --train_for_env_steps = 10000000000 --algo = APPO --gamma = 0 .99 --use_rnn = True --num_workers = 90 --num_envs_per_worker = 12 --num_epochs = 1 --rollout = 32 --recurrence = 32 --batch_size = 2048 --benchmark = False --max_grad_norm = 0 .0 --dmlab_renderer = software --decorrelate_experience_max_seconds = 120 --reset_timeout_seconds = 300 --encoder_conv_architecture = resnet_impala --encoder_conv_mlp_layers = 512 --nonlinearity = relu --rnn_type = lstm --dmlab_extended_action_set = True --num_policies = 4 --pbt_replace_reward_gap = 0 .05 --pbt_replace_reward_gap_absolute = 5 .0 --pbt_period_env_steps = 10000000 --pbt_start_mutation = 100000000 --with_pbt = True --experiment = dmlab_30_resnet_4pbt_w90_v12 --dmlab_one_task_per_worker = True --set_workers_cpu_affinity = True --max_policy_lag = 35 --pbt_target_objective = dmlab_target_objective --dmlab30_dataset = ~/datasets/brady_konkle_oliva2008 --dmlab_use_level_cache = True --dmlab_level_cache_path = /home/user/dmlab_cache PBT for VizDoom (8 agents, 4-GPU machine): python -m sf_examples.vizdoom.train_vizdoom --env = doom_deathmatch_bots --train_for_seconds = 3600000 --algo = APPO --use_rnn = True --gamma = 0 .995 --env_frameskip = 2 --num_workers = 80 --num_envs_per_worker = 24 --num_policies = 8 --batch_size = 2048 --res_w = 128 --res_h = 72 --wide_aspect_ratio = False --with_pbt = True --pbt_period_env_steps = 5000000 --experiment = doom_deathmatch_bots","title":"Population-Based Training"},{"location":"07-advanced-topics/pbt/#population-based-training","text":"Sample Factory contains an implementation of the Population-Based Training algorithm. See PBT paper and original Sample Factory paper for more details. PBT is a hyperparameter optimization algorithm that can be used to train RL agents. Instead of manually tuning all hyperparameters, you can let an optimization method do it for you. This can include not only learning parameters (e.g. learning rate, entropy coefficient), but also environment parameters (e.g. reward function coefficients). It is common in RL to have a sophisticated (shaped) reward function which guides the exploration process. As a result such reward function can distract the agent from the actual final goal. PBT allows you to optimize with respect to some sparse final objective (which we call \"true_objective\") while still using a shaped reward function. Theoretically the algorithm should find hyperparameters (including shaping coefficients) that lead to the best final objective. This can be, for example, directly optimizing for just winning a match in a multiplayer game, which would be very difficult to do with just regular RL because of the sparsity of such objective. This type of PBT algorithm is implemented in the FTW agent by DeepMind.","title":"Population-Based Training"},{"location":"07-advanced-topics/pbt/#algorithm","text":"PBT works similar to a genetic algorithm. A population of agents is trained simultaneously with roughly the following approach: Each agent is assigned a set of hyperparameters (e.g. learning rate, entropy coefficient, reward function coefficients, etc.) Each agent is trained for a fixed number of steps (e.g. 5M steps) At the end of this meta-training epoch, the performance of all agents is ranked: Agents with top K % of performance are unchanged, we just keep training them Agents with bottom K % of performance are replaced by a copy of a random top-K % agent with mutated hyperparameters. Agents in the middle keep their weights but also get mutated hyperparameters. Proceed to the next meta-training epoch. Current version of PBT is implemented for a single machine. The perfect setup is a multi-GPU server that can train multiple agents at the same time. For example, we can train a population of 8 agents on a 4-GPU machine, training 2 agents on each GPU. PBT is perfect for multiplayer game scenarios where training a population of agents against one another yields much more robust results compared to self-play with a single policy.","title":"Algorithm"},{"location":"07-advanced-topics/pbt/#providing-true-objective-to-pbt","text":"In order to optimize for a true objective, you need to return it from the environment. Just add it to the info dictionary returned by the environment at the last step of the episode, e.g.: def step ( self , action ): info = {} ... info [ 'true_objective' ] = self . compute_true_objective () return obs , reward , terminated , truncated , info In the absence of true_objective in the info dictionary, PBT will use the regular reward as the objective.","title":"Providing \"True Objective\" to PBT"},{"location":"07-advanced-topics/pbt/#learning-parameters-optimized-by-pbt","text":"See population_based_training.py : HYPERPARAMS_TO_TUNE = { \"learning_rate\" , \"exploration_loss_coeff\" , \"value_loss_coeff\" , \"max_grad_norm\" , \"ppo_clip_ratio\" , \"ppo_clip_value\" , # gamma can be added with a CLI parameter (--pbt_optimize_gamma=True) } During training current learning parameters are saved in f\"policy_{policy_id:02d}_cfg.json\" files in the experiment directory.","title":"Learning parameters optimized by PBT"},{"location":"07-advanced-topics/pbt/#optimizing-environment-parameters","text":"Besides learning parameters we can also optimize parameters of the environment with respect to some \"true objective\". In order to do that, your environment should implement RewardShapingInterface interface in addition to gym.Env interface. class RewardShapingInterface : def get_default_reward_shaping ( self ) -> Optional [ Dict [ str , Any ]]: \"\"\"Should return a dictionary of string:float key-value pairs defining the current reward shaping scheme.\"\"\" raise NotImplementedError def set_reward_shaping ( self , reward_shaping : Dict [ str , Any ], agent_idx : int | slice ) -> None : \"\"\" Sets the new reward shaping scheme. :param reward_shaping dictionary of string-float key-value pairs :param agent_idx: integer agent index (for multi-agent envs). Can be a slice if we're training in batched mode (set a single reward shaping scheme for a range of agents) \"\"\" raise NotImplementedError Any parameters in the dictionary returned by get_default_reward_shaping will be optimized by PBT. Note that although the dictionary is called \"reward shaping\", it can be used to optimize any environment parameters. It is important that none of these parameters should directly affect the objective calculation, otherwise all PBT will do is increase the coefficients all the way to infinity. An example of how this can be used. Suppose your shaped reward function contains a term for picking up a weapon in a game like Quake or VizDoom. If the true objective is 1.0 for winning the game and 0.0 otherwise then PBT can optimize these weapon preference coefficients to maximize success. But if true objective is not specified (so just the env reward itself is used as objective), then you can just increase the coefficients to increase the reward unboundedly.","title":"Optimizing environment parameters"},{"location":"07-advanced-topics/pbt/#configuring-pbt","text":"Please see Configuration parameter reference . Parameters with --pbt_ prefix are related to PBT. Use --with_pbt=True to enable PBT. It is important also to set --num_policies to the number of agents in the population.","title":"Configuring PBT"},{"location":"07-advanced-topics/pbt/#command-line-examples","text":"Training on DMLab-30 with a 4-agent population on a 4-GPU machine: python -m sf_examples.dmlab.train_dmlab --env = dmlab_30 --train_for_env_steps = 10000000000 --algo = APPO --gamma = 0 .99 --use_rnn = True --num_workers = 90 --num_envs_per_worker = 12 --num_epochs = 1 --rollout = 32 --recurrence = 32 --batch_size = 2048 --benchmark = False --max_grad_norm = 0 .0 --dmlab_renderer = software --decorrelate_experience_max_seconds = 120 --reset_timeout_seconds = 300 --encoder_conv_architecture = resnet_impala --encoder_conv_mlp_layers = 512 --nonlinearity = relu --rnn_type = lstm --dmlab_extended_action_set = True --num_policies = 4 --pbt_replace_reward_gap = 0 .05 --pbt_replace_reward_gap_absolute = 5 .0 --pbt_period_env_steps = 10000000 --pbt_start_mutation = 100000000 --with_pbt = True --experiment = dmlab_30_resnet_4pbt_w90_v12 --dmlab_one_task_per_worker = True --set_workers_cpu_affinity = True --max_policy_lag = 35 --pbt_target_objective = dmlab_target_objective --dmlab30_dataset = ~/datasets/brady_konkle_oliva2008 --dmlab_use_level_cache = True --dmlab_level_cache_path = /home/user/dmlab_cache PBT for VizDoom (8 agents, 4-GPU machine): python -m sf_examples.vizdoom.train_vizdoom --env = doom_deathmatch_bots --train_for_seconds = 3600000 --algo = APPO --use_rnn = True --gamma = 0 .995 --env_frameskip = 2 --num_workers = 80 --num_envs_per_worker = 24 --num_policies = 8 --batch_size = 2048 --res_w = 128 --res_h = 72 --wide_aspect_ratio = False --with_pbt = True --pbt_period_env_steps = 5000000 --experiment = doom_deathmatch_bots","title":"Command-line examples"},{"location":"07-advanced-topics/policy-lag/","text":"Policy Lag \u00b6 Policy lag is the discrepancy between the policy that is used to collect samples and the policy that we train on this data. Policy gradient algorithms (like PPO) are considered on-policy methods and typically suffer sample efficiency losses when the policy lag is large (experience is stale, \"off-policy\"). In practice PPO is pretty robust to slightly off-policy data, but it is important to keep the policy lag under control. Although this is not rigorous, we can measure the policy lag in the number of SGD steps between the policy that collected the data ( behavior policy ) and the trained policy ( target policy ). Sources of policy lag \u00b6 There are following main sources of policy lag: Multiple updates on the same data. This is the most common source of policy lag inherent to almost all policy gradient implementations. If --num_batches_per_epoch > 1 and/or --num_epochs > 1 then we need to do multiple SGD steps before we finish training on the sampled data, causing the lag in the later epochs. Collecting more experience per sampling iteration (one rollout for all agents) than we use for one iteration of training ( num_workers * num_envs_per_worker * rollout >> batch_size * num_batches_per_epoch ). In this case we will inevitably have some trajectories (or parts of trajectories) collected by the policy that is already outdated by the time we use them for training. Async sampling. With asynchronous sampling we collect new data while we are training on the old data, which will inevitably cause some amount of lag. This is the smallest source of lag since we update the policy on the inference worker as soon as new weights are available. Estimating and measuring policy lag \u00b6 Policy lag for a particular RL experiment configuration is roughly proportional to the following value: Lag ~~ (num_epochs * num_workers * num_envs_per_worker * agents_per_env * rollout) / batch_size Sample Factory reports empirical policy lag in two different ways. Policy lag measurements printed to the console \u00b6 [2022-11-30 19:48:19,509][07580] Updated weights for policy 0, policy_version 926 (0.0015) [2022-11-30 19:48:21,166][07494] Fps is (10 sec: 22528.2, 60 sec: 20377.6, 300 sec: 20377.6). Total num frames: 3829760. Throughput: 0: 5085.4. Samples: 203415. Policy #0 lag: (min: 0.0, avg: 1.9, max: 5.0) [2022-11-30 19:48:21,166][07494] Avg episode reward: [(0, '0.824')] Here message Policy #0 lag: (min: 0.0, avg: 1.9, max: 5.0) contains the minimum, average and maximum policy lag for transitions encountered in the last minibatch processed by the learner at the moment of printing. This can correspond to a minibatch in earlier or later epochs, so these values might fluctuate, but looking at 5-10 consecutive printouts should give a good idea of the policy lag. Policy lag measurements in Tensorboard or Weights & Biases \u00b6 train/version_diff_avg , train/version_diff_max , train/version_diff_min metrics represent policy lag values measured in policy versions (SGD steps). See Metrics Reference . Minimizing policy lag \u00b6 Policy lag can usually be traded off for higher throughput or sample efficiency (i.e. by doing many epochs of SGD on the same data). But large values of policy lag can cause instability in training. Each task will have its own sweet spot when it comes to configuration and policy lag. Very roughly speaking, policy lag < 20-30 SGD steps is usually fine, but significantly larger values might be a reason to reconsider the configuration. Empirically, LSTM/GRU policies and environments with very complex action spaces tend to be more sensitive to policy lag. For RNNs this is true because not only the action distributions, but also the hidden states change between the behavior and target policies. With complex action spaces (i.e. tuple, multi-discrete) small changes to the policy can cause large changes to probabilities of individual actions. Following configuration options can be used to minimize policy lag: Increase batch_size , decrease num_epochs , num_batches_per_epoch , num_workers , num_envs_per_worker , rollout , num_batches_per_epoch (see the formula above). Switch to synchronous sampling ( --async_rl=False ). Note that this will likely increase the training time. Achieving zero policy lag (A2C) \u00b6 It is possible to achieve zero policy lag by using --async_rl=False and --num_batches_per_epoch=1 and --num_epochs=1 . This will turn PPO into the algorithm known as A2C (Advantage Actor-Critic) which always trains on the most recent data. This should typically yield stable training, although might not be the best option in terms of throughput or sample efficiency.","title":"Policy Lag"},{"location":"07-advanced-topics/policy-lag/#policy-lag","text":"Policy lag is the discrepancy between the policy that is used to collect samples and the policy that we train on this data. Policy gradient algorithms (like PPO) are considered on-policy methods and typically suffer sample efficiency losses when the policy lag is large (experience is stale, \"off-policy\"). In practice PPO is pretty robust to slightly off-policy data, but it is important to keep the policy lag under control. Although this is not rigorous, we can measure the policy lag in the number of SGD steps between the policy that collected the data ( behavior policy ) and the trained policy ( target policy ).","title":"Policy Lag"},{"location":"07-advanced-topics/policy-lag/#sources-of-policy-lag","text":"There are following main sources of policy lag: Multiple updates on the same data. This is the most common source of policy lag inherent to almost all policy gradient implementations. If --num_batches_per_epoch > 1 and/or --num_epochs > 1 then we need to do multiple SGD steps before we finish training on the sampled data, causing the lag in the later epochs. Collecting more experience per sampling iteration (one rollout for all agents) than we use for one iteration of training ( num_workers * num_envs_per_worker * rollout >> batch_size * num_batches_per_epoch ). In this case we will inevitably have some trajectories (or parts of trajectories) collected by the policy that is already outdated by the time we use them for training. Async sampling. With asynchronous sampling we collect new data while we are training on the old data, which will inevitably cause some amount of lag. This is the smallest source of lag since we update the policy on the inference worker as soon as new weights are available.","title":"Sources of policy lag"},{"location":"07-advanced-topics/policy-lag/#estimating-and-measuring-policy-lag","text":"Policy lag for a particular RL experiment configuration is roughly proportional to the following value: Lag ~~ (num_epochs * num_workers * num_envs_per_worker * agents_per_env * rollout) / batch_size Sample Factory reports empirical policy lag in two different ways.","title":"Estimating and measuring policy lag"},{"location":"07-advanced-topics/policy-lag/#policy-lag-measurements-printed-to-the-console","text":"[2022-11-30 19:48:19,509][07580] Updated weights for policy 0, policy_version 926 (0.0015) [2022-11-30 19:48:21,166][07494] Fps is (10 sec: 22528.2, 60 sec: 20377.6, 300 sec: 20377.6). Total num frames: 3829760. Throughput: 0: 5085.4. Samples: 203415. Policy #0 lag: (min: 0.0, avg: 1.9, max: 5.0) [2022-11-30 19:48:21,166][07494] Avg episode reward: [(0, '0.824')] Here message Policy #0 lag: (min: 0.0, avg: 1.9, max: 5.0) contains the minimum, average and maximum policy lag for transitions encountered in the last minibatch processed by the learner at the moment of printing. This can correspond to a minibatch in earlier or later epochs, so these values might fluctuate, but looking at 5-10 consecutive printouts should give a good idea of the policy lag.","title":"Policy lag measurements printed to the console"},{"location":"07-advanced-topics/policy-lag/#policy-lag-measurements-in-tensorboard-or-weights-biases","text":"train/version_diff_avg , train/version_diff_max , train/version_diff_min metrics represent policy lag values measured in policy versions (SGD steps). See Metrics Reference .","title":"Policy lag measurements in Tensorboard or Weights &amp; Biases"},{"location":"07-advanced-topics/policy-lag/#minimizing-policy-lag","text":"Policy lag can usually be traded off for higher throughput or sample efficiency (i.e. by doing many epochs of SGD on the same data). But large values of policy lag can cause instability in training. Each task will have its own sweet spot when it comes to configuration and policy lag. Very roughly speaking, policy lag < 20-30 SGD steps is usually fine, but significantly larger values might be a reason to reconsider the configuration. Empirically, LSTM/GRU policies and environments with very complex action spaces tend to be more sensitive to policy lag. For RNNs this is true because not only the action distributions, but also the hidden states change between the behavior and target policies. With complex action spaces (i.e. tuple, multi-discrete) small changes to the policy can cause large changes to probabilities of individual actions. Following configuration options can be used to minimize policy lag: Increase batch_size , decrease num_epochs , num_batches_per_epoch , num_workers , num_envs_per_worker , rollout , num_batches_per_epoch (see the formula above). Switch to synchronous sampling ( --async_rl=False ). Note that this will likely increase the training time.","title":"Minimizing policy lag"},{"location":"07-advanced-topics/policy-lag/#achieving-zero-policy-lag-a2c","text":"It is possible to achieve zero policy lag by using --async_rl=False and --num_batches_per_epoch=1 and --num_epochs=1 . This will turn PPO into the algorithm known as A2C (Advantage Actor-Critic) which always trains on the most recent data. This should typically yield stable training, although might not be the best option in terms of throughput or sample efficiency.","title":"Achieving zero policy lag (A2C)"},{"location":"07-advanced-topics/profiling/","text":"Profiling \u00b6 It is virtually impossible to optimize any system without measuring its performance and identifying the bottlenecks. This guide will show you how to profile your RL workload in different regimes. Profiling with the built-in \"Timing\" tool \u00b6 Sample Factory provides a simple class called Timing (see sample_factory/utils/timing.py ) that can be used for high-level profiling to get a rough idea of where the compute cycles are spent. Core hotspots are already instrumented, but if you'd like to see a more elaborate picture, you can use the Timing class in your own code like this: import time timing = Timing ( name = \"MyProfile\" ) # add_time() will accumulate time spent in the block # this is the most commonly used method with timing . add_time ( \"hotspot\" ): # do something ... # measure time spent in a subsection of code # when we build the timing report, we'll generate a tree corresponding to the nesting with timing . add_time ( \"subsection1\" ): # do something ... with timing . add_time ( \"subsection2\" ): # do something ... # instead of accumulating time, this will measure the last time the block was executed with timing . timeit ( \"hotspot2\" ): # do something ... # this will measure the average time spent in the block with timing . time_avg ( \"hotspot3\" ): # do something ... # this will print the timing report print ( timing ) Example: profiling an asynchronous workload \u00b6 Let's take a look at a typical RL workload: training an agent in a VizDoom pixel-based environment. We use the following command line and run it on a 6-core laptop with hyperthreading: python -m sf_examples.vizdoom.train_vizdoom --env = doom_benchmark --env_frameskip = 4 --train_for_env_steps = 4000000 \\\\ --use_rnn = True --num_workers = 12 --num_envs_per_worker = 16 --num_policies = 1 --num_epochs = 1 --rollout = 32 --recurrence = 32 \\\\ --batch_size = 2048 --experiment = profiling --benchmark = True --decorrelate_envs_on_one_worker = False --res_w = 128 --res_h = 72 \\\\ --wide_aspect_ratio = True --policy_workers_per_policy = 1 --worker_num_splits = 2 --batched_sampling = False \\\\ --serial_mode = False --async_rl = True --policy_workers_per_policy = 1 If we wait for this experiment to finish (in this case, after training for 4M env steps), we'll get the following timing report: [2022-11-25 01:36:52,563][15762] Batcher 0 profile tree view: batching: 10.1365, releasing_batches: 0.0136 [2022-11-25 01:36:52,564][15762] InferenceWorker_p0-w0 profile tree view: wait_policy: 0.0022 wait_policy_total: 93.7697 update_model: 2.3025 weight_update: 0.0015 one_step: 0.0034 handle_policy_step: 105.2299 deserialize: 7.4926, stack: 0.6621, obs_to_device_normalize: 29.3540, forward: 38.4143, send_messages: 5.9522 prepare_outputs: 18.2651 to_cpu: 11.2702 [2022-11-25 01:36:52,564][15762] Learner 0 profile tree view: misc: 0.0024, prepare_batch: 8.0517 train: 28.5942 epoch_init: 0.0037, minibatch_init: 0.0038, losses_postprocess: 0.1654, kl_divergence: 0.2093, after_optimizer: 12.5617 calculate_losses: 10.2242 losses_init: 0.0021, forward_head: 0.4746, bptt_initial: 7.5225, tail: 0.3432, advantages_returns: 0.0976, losses: 0.7113 bptt: 0.9616 bptt_forward_core: 0.9263 update: 5.0903 clip: 0.8172 [2022-11-25 01:36:52,564][15762] RolloutWorker_w0 profile tree view: wait_for_trajectories: 0.0767, enqueue_policy_requests: 5.3569, env_step: 170.3642, overhead: 10.1567, complete_rollouts: 0.3764 save_policy_outputs: 6.6260 split_output_tensors: 3.0167 [2022-11-25 01:36:52,564][15762] RolloutWorker_w11 profile tree view: wait_for_trajectories: 0.0816, enqueue_policy_requests: 5.5298, env_step: 169.3195, overhead: 10.2944, complete_rollouts: 0.3914 save_policy_outputs: 6.7380 split_output_tensors: 3.1037 [2022-11-25 01:36:52,564][15762] Loop Runner_EvtLoop terminating... [2022-11-25 01:36:52,565][15762] Runner profile tree view: main_loop: 217.4041 [2022-11-25 01:36:52,565][15762] Collected {0: 4014080}, FPS: 18463.7 First thing to notice here: instead of a single report we have reports from all different types of components of our system: Batcher, InferenceWorker, Learner, RolloutWorker, Runner (main loop). There are 12 rollout workers but we see only 0 th (first) and 11 th (last) workers in the report - that's just to save space, reports from all other workers will be very similar. Total training time took 217 seconds at ~18400FPS (actual FPS reported during training was ~21000FPS, but this final number takes initialization time into account). Each individual report is a tree view of the time spent in different hotspots. For example, learner profile looks like this: train: 28.5942 epoch_init: 0.0037, minibatch_init: 0.0038, losses_postprocess: 0.1654, kl_divergence: 0.2093, after_optimizer: 12.5617 calculate_losses: 10.2242 losses_init: 0.0021, forward_head: 0.4746, bptt_initial: 7.5225, tail: 0.3432, advantages_returns: 0.0976, losses: 0.7113 bptt: 0.9616 bptt_forward_core: 0.9263 update: 5.0903 clip: 0.8172 train is the highest-level profiler context. On the next line we print all sub-profiles that don't have any sub-profiles of their own. In this case, epoch_init , minibatch_init , etc. After that, one by one, we print all sub-profiles that have sub-profiles of their own. Let's take a look at individual component reports: Runner (main loop) does not actually do any heavy work other than reporting summaries, so we can ignore it. It is here mostly to give us the total time from experiment start to finish. Batcher is responsible for batching trajectories from rollout workers and feeding them to the learner. In this case it only took 10 seconds and since it was done in parallel to all other work, we can ignore it for the most part, it's pretty fast. Learner's main hotspots took only 8 and 28 seconds. Again, considering that it was done in parallel to all other work, and the time is pretty insignificant compared to the total time of 217 seconds, we can safely say that it's not the bottleneck. InferenceWorker's overall time is 105 seconds, which is significant. We can see that the main hotspots are forward (actual forward pass) and obs_to_device_normalize (normalizing the observations and transferring them to GPU). In order to increase throughtput we might want to make our model faster (i.e. by making it smaller) or disable normalization (parameter --normalize_input=False , see config reference). Note however that both of these measures may hurt sample efficiency. RolloutWorkers that simulate the environment are the main culprits here. The majority of time is taken by env_step (stepping through the environment), ~170 seconds. Overall, we can say that this workload is heavily dominated by CPU-based simulation. If you're in a similar situation you might want to consider instrumenting your code deeper (i.e. using Timing or other tool) to measure hotspots in your environment and attempt to optimize it. Notes on GPU profiling \u00b6 Profiling GPU-based workloads can be misleading because GPU kernels are asynchronous and sometimes we can see a lot of time spent in sections after the ones we expect to be the hotspots. In the example above, the learner's after_optimizer: 12.5617 is significantly longer than update: 5.0903 where the actual backward pass happens. Thus one should not rely too heavily on timing your code for GPU profiling. Take a look at CUDA profiling, i.e. here is a Pytorch tutorial . Also check out this tutorial for some advanced RL profiling techniques. Profiling with standard Python profilers (cProfile or yappi) \u00b6 In most RL workloads in Sample Factory it can be difficult to use standard profiling tools because the full application consists of many processes and threads and in the author's experience standard tools struggle to organise traces from multiple processes into a single coherent report (if the reader knows of a good tool for this, please let the author know ). However, using serial mode we can force Sample Factory to execute everything in one process! This can be very useful for finding bottlenecks in your environment implementation without the need for manual instrumentation. The following command will run the entire experiment in a single process: python -m sf_examples.mujoco.train_mujoco --env = mujoco_ant --serial_mode = True --async_rl = False Note that we enable synchronous RL mode as well, it's easier to debug this way and asynchronicity does not make sense when we're not using multiple processes. Moreover for some workloads it is actually optimal to run everything in a single process! This is true for GPU-accelerated environments such as IsaacGym or Brax. When env simulation, inference, and learning are all done on one GPU it is not necessarily beneficial to run these tasks in separate processes. In this case we can profile Sample Factory like any other Python application. For example, PyCharm has a nice visualizer for profiling results generated by cProfile or yappi . If we run training in IsaacGym in serial mode under PyCharm's profiler: python -m sf_examples.isaacgym_examples.train_isaacgym --env = Ant --experiment = igeAnt we get the following report which can be explored to find hotspots in different parts of the code:","title":"Profiling"},{"location":"07-advanced-topics/profiling/#profiling","text":"It is virtually impossible to optimize any system without measuring its performance and identifying the bottlenecks. This guide will show you how to profile your RL workload in different regimes.","title":"Profiling"},{"location":"07-advanced-topics/profiling/#profiling-with-the-built-in-timing-tool","text":"Sample Factory provides a simple class called Timing (see sample_factory/utils/timing.py ) that can be used for high-level profiling to get a rough idea of where the compute cycles are spent. Core hotspots are already instrumented, but if you'd like to see a more elaborate picture, you can use the Timing class in your own code like this: import time timing = Timing ( name = \"MyProfile\" ) # add_time() will accumulate time spent in the block # this is the most commonly used method with timing . add_time ( \"hotspot\" ): # do something ... # measure time spent in a subsection of code # when we build the timing report, we'll generate a tree corresponding to the nesting with timing . add_time ( \"subsection1\" ): # do something ... with timing . add_time ( \"subsection2\" ): # do something ... # instead of accumulating time, this will measure the last time the block was executed with timing . timeit ( \"hotspot2\" ): # do something ... # this will measure the average time spent in the block with timing . time_avg ( \"hotspot3\" ): # do something ... # this will print the timing report print ( timing )","title":"Profiling with the built-in \"Timing\" tool"},{"location":"07-advanced-topics/profiling/#example-profiling-an-asynchronous-workload","text":"Let's take a look at a typical RL workload: training an agent in a VizDoom pixel-based environment. We use the following command line and run it on a 6-core laptop with hyperthreading: python -m sf_examples.vizdoom.train_vizdoom --env = doom_benchmark --env_frameskip = 4 --train_for_env_steps = 4000000 \\\\ --use_rnn = True --num_workers = 12 --num_envs_per_worker = 16 --num_policies = 1 --num_epochs = 1 --rollout = 32 --recurrence = 32 \\\\ --batch_size = 2048 --experiment = profiling --benchmark = True --decorrelate_envs_on_one_worker = False --res_w = 128 --res_h = 72 \\\\ --wide_aspect_ratio = True --policy_workers_per_policy = 1 --worker_num_splits = 2 --batched_sampling = False \\\\ --serial_mode = False --async_rl = True --policy_workers_per_policy = 1 If we wait for this experiment to finish (in this case, after training for 4M env steps), we'll get the following timing report: [2022-11-25 01:36:52,563][15762] Batcher 0 profile tree view: batching: 10.1365, releasing_batches: 0.0136 [2022-11-25 01:36:52,564][15762] InferenceWorker_p0-w0 profile tree view: wait_policy: 0.0022 wait_policy_total: 93.7697 update_model: 2.3025 weight_update: 0.0015 one_step: 0.0034 handle_policy_step: 105.2299 deserialize: 7.4926, stack: 0.6621, obs_to_device_normalize: 29.3540, forward: 38.4143, send_messages: 5.9522 prepare_outputs: 18.2651 to_cpu: 11.2702 [2022-11-25 01:36:52,564][15762] Learner 0 profile tree view: misc: 0.0024, prepare_batch: 8.0517 train: 28.5942 epoch_init: 0.0037, minibatch_init: 0.0038, losses_postprocess: 0.1654, kl_divergence: 0.2093, after_optimizer: 12.5617 calculate_losses: 10.2242 losses_init: 0.0021, forward_head: 0.4746, bptt_initial: 7.5225, tail: 0.3432, advantages_returns: 0.0976, losses: 0.7113 bptt: 0.9616 bptt_forward_core: 0.9263 update: 5.0903 clip: 0.8172 [2022-11-25 01:36:52,564][15762] RolloutWorker_w0 profile tree view: wait_for_trajectories: 0.0767, enqueue_policy_requests: 5.3569, env_step: 170.3642, overhead: 10.1567, complete_rollouts: 0.3764 save_policy_outputs: 6.6260 split_output_tensors: 3.0167 [2022-11-25 01:36:52,564][15762] RolloutWorker_w11 profile tree view: wait_for_trajectories: 0.0816, enqueue_policy_requests: 5.5298, env_step: 169.3195, overhead: 10.2944, complete_rollouts: 0.3914 save_policy_outputs: 6.7380 split_output_tensors: 3.1037 [2022-11-25 01:36:52,564][15762] Loop Runner_EvtLoop terminating... [2022-11-25 01:36:52,565][15762] Runner profile tree view: main_loop: 217.4041 [2022-11-25 01:36:52,565][15762] Collected {0: 4014080}, FPS: 18463.7 First thing to notice here: instead of a single report we have reports from all different types of components of our system: Batcher, InferenceWorker, Learner, RolloutWorker, Runner (main loop). There are 12 rollout workers but we see only 0 th (first) and 11 th (last) workers in the report - that's just to save space, reports from all other workers will be very similar. Total training time took 217 seconds at ~18400FPS (actual FPS reported during training was ~21000FPS, but this final number takes initialization time into account). Each individual report is a tree view of the time spent in different hotspots. For example, learner profile looks like this: train: 28.5942 epoch_init: 0.0037, minibatch_init: 0.0038, losses_postprocess: 0.1654, kl_divergence: 0.2093, after_optimizer: 12.5617 calculate_losses: 10.2242 losses_init: 0.0021, forward_head: 0.4746, bptt_initial: 7.5225, tail: 0.3432, advantages_returns: 0.0976, losses: 0.7113 bptt: 0.9616 bptt_forward_core: 0.9263 update: 5.0903 clip: 0.8172 train is the highest-level profiler context. On the next line we print all sub-profiles that don't have any sub-profiles of their own. In this case, epoch_init , minibatch_init , etc. After that, one by one, we print all sub-profiles that have sub-profiles of their own. Let's take a look at individual component reports: Runner (main loop) does not actually do any heavy work other than reporting summaries, so we can ignore it. It is here mostly to give us the total time from experiment start to finish. Batcher is responsible for batching trajectories from rollout workers and feeding them to the learner. In this case it only took 10 seconds and since it was done in parallel to all other work, we can ignore it for the most part, it's pretty fast. Learner's main hotspots took only 8 and 28 seconds. Again, considering that it was done in parallel to all other work, and the time is pretty insignificant compared to the total time of 217 seconds, we can safely say that it's not the bottleneck. InferenceWorker's overall time is 105 seconds, which is significant. We can see that the main hotspots are forward (actual forward pass) and obs_to_device_normalize (normalizing the observations and transferring them to GPU). In order to increase throughtput we might want to make our model faster (i.e. by making it smaller) or disable normalization (parameter --normalize_input=False , see config reference). Note however that both of these measures may hurt sample efficiency. RolloutWorkers that simulate the environment are the main culprits here. The majority of time is taken by env_step (stepping through the environment), ~170 seconds. Overall, we can say that this workload is heavily dominated by CPU-based simulation. If you're in a similar situation you might want to consider instrumenting your code deeper (i.e. using Timing or other tool) to measure hotspots in your environment and attempt to optimize it.","title":"Example: profiling an asynchronous workload"},{"location":"07-advanced-topics/profiling/#notes-on-gpu-profiling","text":"Profiling GPU-based workloads can be misleading because GPU kernels are asynchronous and sometimes we can see a lot of time spent in sections after the ones we expect to be the hotspots. In the example above, the learner's after_optimizer: 12.5617 is significantly longer than update: 5.0903 where the actual backward pass happens. Thus one should not rely too heavily on timing your code for GPU profiling. Take a look at CUDA profiling, i.e. here is a Pytorch tutorial . Also check out this tutorial for some advanced RL profiling techniques.","title":"Notes on GPU profiling"},{"location":"07-advanced-topics/profiling/#profiling-with-standard-python-profilers-cprofile-or-yappi","text":"In most RL workloads in Sample Factory it can be difficult to use standard profiling tools because the full application consists of many processes and threads and in the author's experience standard tools struggle to organise traces from multiple processes into a single coherent report (if the reader knows of a good tool for this, please let the author know ). However, using serial mode we can force Sample Factory to execute everything in one process! This can be very useful for finding bottlenecks in your environment implementation without the need for manual instrumentation. The following command will run the entire experiment in a single process: python -m sf_examples.mujoco.train_mujoco --env = mujoco_ant --serial_mode = True --async_rl = False Note that we enable synchronous RL mode as well, it's easier to debug this way and asynchronicity does not make sense when we're not using multiple processes. Moreover for some workloads it is actually optimal to run everything in a single process! This is true for GPU-accelerated environments such as IsaacGym or Brax. When env simulation, inference, and learning are all done on one GPU it is not necessarily beneficial to run these tasks in separate processes. In this case we can profile Sample Factory like any other Python application. For example, PyCharm has a nice visualizer for profiling results generated by cProfile or yappi . If we run training in IsaacGym in serial mode under PyCharm's profiler: python -m sf_examples.isaacgym_examples.train_isaacgym --env = Ant --experiment = igeAnt we get the following report which can be explored to find hotspots in different parts of the code:","title":"Profiling with standard Python profilers (cProfile or yappi)"},{"location":"07-advanced-topics/serial-mode/","text":"Serial Mode \u00b6 Debugging an asynchronous system can be hard. In order to streamline debugging and development process, we provide a way to run all components of Sample Factory in a single process. Enable serial mode by setting --serial_mode to True . Serial regime is achieved via signal-slot mechanism. Components interact by sending and receiving signals. Thus they actually don't care whether they are running in the same process or in multiple processes. This allows us to put them all on the same event loop in the main process. Applications \u00b6 The main use case for serial mode is debugging and development. If you're debugging your environment code, or any part of SF codebase, it is almost always easier to do it in serial mode. That said, for highly-vectorized GPU-accelerated environments it can be beneficial to run the whole system in serial mode, which is exactly what we do by default with IsaacGym . One advantage of serial mode is that we minimize the number of CUDA contexts and thus VRAM usage.","title":"Serial Mode"},{"location":"07-advanced-topics/serial-mode/#serial-mode","text":"Debugging an asynchronous system can be hard. In order to streamline debugging and development process, we provide a way to run all components of Sample Factory in a single process. Enable serial mode by setting --serial_mode to True . Serial regime is achieved via signal-slot mechanism. Components interact by sending and receiving signals. Thus they actually don't care whether they are running in the same process or in multiple processes. This allows us to put them all on the same event loop in the main process.","title":"Serial Mode"},{"location":"07-advanced-topics/serial-mode/#applications","text":"The main use case for serial mode is debugging and development. If you're debugging your environment code, or any part of SF codebase, it is almost always easier to do it in serial mode. That said, for highly-vectorized GPU-accelerated environments it can be beneficial to run the whole system in serial mode, which is exactly what we do by default with IsaacGym . One advantage of serial mode is that we minimize the number of CUDA contexts and thus VRAM usage.","title":"Applications"},{"location":"07-advanced-topics/sync-async/","text":"Synchronous/Asynchronous RL \u00b6 Since version 2.0 Sample Factory supports two training modes: synchronous and asynchronous. You can switch between them by setting --async_rl=False or --async_rl=True in the command line. Synchronous RL \u00b6 In synchronous mode we collect trajectories from all environments until we have just enough data to fill a dataset (or training batch ) on which the learner will perform one or more epochs of SGD. We operate synchronously: the system either collects the experience or trains the policy, not both at the same time. Rollout and Inference workers wait for the learner to finish the training before they start collecting more data. Asynchronous RL \u00b6 In asynchronous (default) mode we collect trajectories all the time and train the policy in the background. Once we have enough data to fill a dataset, we immediately start collecting new trajectories and will keep doing so until --num_batches_to_accumulate training batches are accumulated. Pros and Cons \u00b6 There is no clear winner between the two modes. Try both regimes and see which one works better for you. Async mode is often faster because we allow more computation to happen in parallel. As a tradeoff it introduces more policy-lag because some of the experience is collected by older versions of the policy (for example when we collect experience during training). So async mode enables faster training but might cost sample efficiency in some setups, for example LSTM/GRU training is usually more susceptible to policy-lag than non-recurrent policies. Sync mode has more strict requirements for the system configuration because we're looking to collect the exact amount of data to fill a training batch. Example: we have --num_workers=16 , --num_envs_per_worker=8 , --rollout=32 . This means in one iteration we collect 16 * 8 * 32 = 4096 steps of experience. Sync mode requires that training batch size is a multiple of 4096. This would work with --batch_size=4096 and --num_batches_per_epoch=1 or --batch_size=2048 and --num_batches_per_epoch=2 , but not with --batch_size=512 and --num_batches_per_epoch=3 . TLDR : sync mode provides less flexibility in the training configuration. In async mode we can do pretty much anything. For multi-policy and PBT setups we recommend using async mode. Async mode allows different policies to collect different amounts of experience per iteration, which allows us to use arbitrary mapping between agents and policies. Visualization \u00b6 The following animations may provide further insight into the difference between the two modes. Sync RL: https://www.youtube.com/watch?v=FHRG0lHVa54 Async RL: https://www.youtube.com/watch?v=ML2WAQNpF90 Note that the \"Sync RL\" animation is not 100% accurate to how SF works, we actually still do collect the experience asynchronously within the rollout, but then pause during training. \"Sync RL\" animation is closer to how a traditional RL implementation operates (e.g. OpenAI Baselines) and the comparison between the two shows why Sample Factory is often much faster. Vectorized environments \u00b6 In GPU-accelerated environments like IsaacGym async mode does not provide a significant speedup because we do everything on the same device anyway. For these environments it is recommended to use sync mode for maximum sample efficiency. This animation demonstrates how synchronous learning works in a vectorized environment like IsaacGym: https://www.youtube.com/watch?v=EyUyDs4AA1Y","title":"Synchronous/Asynchronous RL"},{"location":"07-advanced-topics/sync-async/#synchronousasynchronous-rl","text":"Since version 2.0 Sample Factory supports two training modes: synchronous and asynchronous. You can switch between them by setting --async_rl=False or --async_rl=True in the command line.","title":"Synchronous/Asynchronous RL"},{"location":"07-advanced-topics/sync-async/#synchronous-rl","text":"In synchronous mode we collect trajectories from all environments until we have just enough data to fill a dataset (or training batch ) on which the learner will perform one or more epochs of SGD. We operate synchronously: the system either collects the experience or trains the policy, not both at the same time. Rollout and Inference workers wait for the learner to finish the training before they start collecting more data.","title":"Synchronous RL"},{"location":"07-advanced-topics/sync-async/#asynchronous-rl","text":"In asynchronous (default) mode we collect trajectories all the time and train the policy in the background. Once we have enough data to fill a dataset, we immediately start collecting new trajectories and will keep doing so until --num_batches_to_accumulate training batches are accumulated.","title":"Asynchronous RL"},{"location":"07-advanced-topics/sync-async/#pros-and-cons","text":"There is no clear winner between the two modes. Try both regimes and see which one works better for you. Async mode is often faster because we allow more computation to happen in parallel. As a tradeoff it introduces more policy-lag because some of the experience is collected by older versions of the policy (for example when we collect experience during training). So async mode enables faster training but might cost sample efficiency in some setups, for example LSTM/GRU training is usually more susceptible to policy-lag than non-recurrent policies. Sync mode has more strict requirements for the system configuration because we're looking to collect the exact amount of data to fill a training batch. Example: we have --num_workers=16 , --num_envs_per_worker=8 , --rollout=32 . This means in one iteration we collect 16 * 8 * 32 = 4096 steps of experience. Sync mode requires that training batch size is a multiple of 4096. This would work with --batch_size=4096 and --num_batches_per_epoch=1 or --batch_size=2048 and --num_batches_per_epoch=2 , but not with --batch_size=512 and --num_batches_per_epoch=3 . TLDR : sync mode provides less flexibility in the training configuration. In async mode we can do pretty much anything. For multi-policy and PBT setups we recommend using async mode. Async mode allows different policies to collect different amounts of experience per iteration, which allows us to use arbitrary mapping between agents and policies.","title":"Pros and Cons"},{"location":"07-advanced-topics/sync-async/#visualization","text":"The following animations may provide further insight into the difference between the two modes. Sync RL: https://www.youtube.com/watch?v=FHRG0lHVa54 Async RL: https://www.youtube.com/watch?v=ML2WAQNpF90 Note that the \"Sync RL\" animation is not 100% accurate to how SF works, we actually still do collect the experience asynchronously within the rollout, but then pause during training. \"Sync RL\" animation is closer to how a traditional RL implementation operates (e.g. OpenAI Baselines) and the comparison between the two shows why Sample Factory is often much faster.","title":"Visualization"},{"location":"07-advanced-topics/sync-async/#vectorized-environments","text":"In GPU-accelerated environments like IsaacGym async mode does not provide a significant speedup because we do everything on the same device anyway. For these environments it is recommended to use sync mode for maximum sample efficiency. This animation demonstrates how synchronous learning works in a vectorized environment like IsaacGym: https://www.youtube.com/watch?v=EyUyDs4AA1Y","title":"Vectorized environments"},{"location":"08-miscellaneous/tests/","text":"Tests \u00b6 To run unit tests install prereqiusites and execute the following command from the root of the repo: pip install -e . [ dev ] make test Consider installing VizDoom for a more comprehensive set of tests. These tests are executed after each commit/PR by Github Actions. Test CI based on Github Actions \u00b6 We build a test CI system based on Github Actions which will automatically run unit tests on different operating systems (currently Linux and macOS) with different python versions (currently 3.8, 3.9, 3.10) when you submit PRs or merge to the main branch. The test workflow is defined in .github/workflows/test-ci.yml .","title":"Tests"},{"location":"08-miscellaneous/tests/#tests","text":"To run unit tests install prereqiusites and execute the following command from the root of the repo: pip install -e . [ dev ] make test Consider installing VizDoom for a more comprehensive set of tests. These tests are executed after each commit/PR by Github Actions.","title":"Tests"},{"location":"08-miscellaneous/tests/#test-ci-based-on-github-actions","text":"We build a test CI system based on Github Actions which will automatically run unit tests on different operating systems (currently Linux and macOS) with different python versions (currently 3.8, 3.9, 3.10) when you submit PRs or merge to the main branch. The test workflow is defined in .github/workflows/test-ci.yml .","title":"Test CI based on Github Actions"},{"location":"08-miscellaneous/v1-to-v2/","text":"v1 to v2 Migration \u00b6 The repository has changed very significantly from the original Sample Factory v1, which makes it pretty much impossible to track all the changes. Perhaps the most obvious change that will affect everyone is that we removed generic entry points such as train_appo.py and enjoy_appo.py . As a consequence, now there's no difference between how \"custom\" and \"built-in\" environments are handled (custom envs are first-class citizens now). See train/enjoy scripts in sf_examples to see how to use the new API. If you have any custom code that registers custom environments of model architectures, please check Custom Environments and Custom Models for the new API. Some configuration parameters were renamed: --ppo_epochs -> --num_epochs --num_batches_per_iteration -> --num_batches_per_epoch --num_minibatches_to_accumulate -> --num_batches_to_accumulate (also changed semantically, check the cfg reference) Runner class we used to launch groups of experiments such as hyperparameter searches got renamed to Launcher . The name Runner now refers to an entirely different concept, a class that handles the main loop of the algorithm. Entities ActorWorker and PolicyWorker were renamed to RolloutWorker and InferenceWorker respectively. Due to the gravity of the changes it is difficult to provide a comprehensive migration guide. If you recently updated your codebase to use Sample Factory v2.0+, please consider sharing your experience and contribute to this guide! :)","title":"v1 to v2 Migration"},{"location":"08-miscellaneous/v1-to-v2/#v1-to-v2-migration","text":"The repository has changed very significantly from the original Sample Factory v1, which makes it pretty much impossible to track all the changes. Perhaps the most obvious change that will affect everyone is that we removed generic entry points such as train_appo.py and enjoy_appo.py . As a consequence, now there's no difference between how \"custom\" and \"built-in\" environments are handled (custom envs are first-class citizens now). See train/enjoy scripts in sf_examples to see how to use the new API. If you have any custom code that registers custom environments of model architectures, please check Custom Environments and Custom Models for the new API. Some configuration parameters were renamed: --ppo_epochs -> --num_epochs --num_batches_per_iteration -> --num_batches_per_epoch --num_minibatches_to_accumulate -> --num_batches_to_accumulate (also changed semantically, check the cfg reference) Runner class we used to launch groups of experiments such as hyperparameter searches got renamed to Launcher . The name Runner now refers to an entirely different concept, a class that handles the main loop of the algorithm. Entities ActorWorker and PolicyWorker were renamed to RolloutWorker and InferenceWorker respectively. Due to the gravity of the changes it is difficult to provide a comprehensive migration guide. If you recently updated your codebase to use Sample Factory v2.0+, please consider sharing your experience and contribute to this guide! :)","title":"v1 to v2 Migration"},{"location":"09-environment-integrations/atari/","text":"Atari \u00b6 Installation \u00b6 Install Sample Factory with Atari dependencies with PyPI: pip install sample-factory[atari] Running Experiments \u00b6 Run Atari experiments with the scripts in sf_examples.atari . The default parameters have been chosen to match CleanRL's configuration (see reports below) and are not tuned for throughput. (TODO: also provide parameters that result in the fastest training). To train a model in the BreakoutNoFrameskip-v4 environment: python -m sf_examples.atari.train_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\" To visualize the training results, use the enjoy_atari script: python -m sf_examples.atari.enjoy_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\" Multiple experiments can be run in parallel with the launcher module. atari_envs is an example launcher script that runs atari envs with 4 seeds. python -m sample_factory.launcher.run --run=sf_examples.atari.experiments.atari_envs --backend=processes --max_parallel=8 --pause_between=1 --experiments_per_gpu=10000 --num_gpus=1 List of Supported Environments \u00b6 Specify the environment to run with the --env command line parameter. The following Atari v4 environments are supported out of the box. Various APPO models trained on Atari environments are uploaded to the HuggingFace Hub. The models have all been trained for 2 billion steps with 3 seeds per experiment. Videos of the agents after training can be found on the HuggingFace Hub. Atari Command Line Parameter Atari Environment name Model Checkpooints atari_alien AlienNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_amidar AmidarNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_assault AssaultNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_asterix AsterixNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_asteroid AsteroidsNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_atlantis AtlantisNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_bankheist BankHeistNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_battlezone BattleZoneNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_beamrider BeamRiderNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_berzerk BerzerkNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_bowling BowlingNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_boxing BoxingNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_breakout BreakoutNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_centipede CentipedeNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_choppercommand ChopperCommandNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_crazyclimber CrazyClimberNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_defender DefenderNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_demonattack DemonAttackNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_doubledunk DoubleDunkNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_enduro EnduroNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_fishingderby FishingDerbyNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_freeway FreewayNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_frostbite FrostbiteNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_gopher GopherNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_gravitar GravitarNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_hero HeroNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_icehockey IceHockeyNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_jamesbond JamesbondNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_kangaroo KangarooNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_krull KrullNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_kongfumaster KungFuMasterNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_montezuma MontezumaRevengeNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_mspacman MsPacmanNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_namethisgame NameThisGameNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_phoenix PhoenixNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_pitfall PitfallNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_pong PongNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_privateye PrivateEyeNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_qbert QbertNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_riverraid RiverraidNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_roadrunner RoadRunnerNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_robotank RobotankNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_seaquest SeaquestNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_skiing SkiingNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_solaris SolarisNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_spaceinvaders SpaceInvadersNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_stargunner StarGunnerNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_surround SurroundNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_tennis TennisNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_timepilot TimePilotNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_tutankham TutankhamNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_upndown UpNDownNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_venture VentureNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_videopinball VideoPinballNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_wizardofwor WizardOfWorNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_yarsrevenge YarsRevengeNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_zaxxon ZaxxonNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints Reports \u00b6 Sample Factory was benchmarked on Atari against CleanRL and Baselines. Sample Factory was able to achieve similar sample efficiency as CleanRL and Baselines using the same parameters. https://wandb.ai/wmfrank/atari-benchmark/reports/Atari-Sample-Factory2-Baselines-CleanRL--VmlldzoyMzEyNjIw","title":"Atari"},{"location":"09-environment-integrations/atari/#atari","text":"","title":"Atari"},{"location":"09-environment-integrations/atari/#installation","text":"Install Sample Factory with Atari dependencies with PyPI: pip install sample-factory[atari]","title":"Installation"},{"location":"09-environment-integrations/atari/#running-experiments","text":"Run Atari experiments with the scripts in sf_examples.atari . The default parameters have been chosen to match CleanRL's configuration (see reports below) and are not tuned for throughput. (TODO: also provide parameters that result in the fastest training). To train a model in the BreakoutNoFrameskip-v4 environment: python -m sf_examples.atari.train_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\" To visualize the training results, use the enjoy_atari script: python -m sf_examples.atari.enjoy_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\" Multiple experiments can be run in parallel with the launcher module. atari_envs is an example launcher script that runs atari envs with 4 seeds. python -m sample_factory.launcher.run --run=sf_examples.atari.experiments.atari_envs --backend=processes --max_parallel=8 --pause_between=1 --experiments_per_gpu=10000 --num_gpus=1","title":"Running Experiments"},{"location":"09-environment-integrations/atari/#list-of-supported-environments","text":"Specify the environment to run with the --env command line parameter. The following Atari v4 environments are supported out of the box. Various APPO models trained on Atari environments are uploaded to the HuggingFace Hub. The models have all been trained for 2 billion steps with 3 seeds per experiment. Videos of the agents after training can be found on the HuggingFace Hub. Atari Command Line Parameter Atari Environment name Model Checkpooints atari_alien AlienNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_amidar AmidarNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_assault AssaultNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_asterix AsterixNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_asteroid AsteroidsNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_atlantis AtlantisNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_bankheist BankHeistNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_battlezone BattleZoneNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_beamrider BeamRiderNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_berzerk BerzerkNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_bowling BowlingNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_boxing BoxingNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_breakout BreakoutNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_centipede CentipedeNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_choppercommand ChopperCommandNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_crazyclimber CrazyClimberNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_defender DefenderNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_demonattack DemonAttackNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_doubledunk DoubleDunkNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_enduro EnduroNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_fishingderby FishingDerbyNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_freeway FreewayNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_frostbite FrostbiteNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_gopher GopherNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_gravitar GravitarNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_hero HeroNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_icehockey IceHockeyNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_jamesbond JamesbondNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_kangaroo KangarooNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_krull KrullNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_kongfumaster KungFuMasterNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_montezuma MontezumaRevengeNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_mspacman MsPacmanNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_namethisgame NameThisGameNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_phoenix PhoenixNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_pitfall PitfallNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_pong PongNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_privateye PrivateEyeNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_qbert QbertNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_riverraid RiverraidNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_roadrunner RoadRunnerNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_robotank RobotankNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_seaquest SeaquestNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_skiing SkiingNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_solaris SolarisNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_spaceinvaders SpaceInvadersNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_stargunner StarGunnerNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_surround SurroundNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_tennis TennisNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_timepilot TimePilotNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_tutankham TutankhamNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_upndown UpNDownNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_venture VentureNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_videopinball VideoPinballNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_wizardofwor WizardOfWorNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_yarsrevenge YarsRevengeNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_zaxxon ZaxxonNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints","title":"List of Supported Environments"},{"location":"09-environment-integrations/atari/#reports","text":"Sample Factory was benchmarked on Atari against CleanRL and Baselines. Sample Factory was able to achieve similar sample efficiency as CleanRL and Baselines using the same parameters. https://wandb.ai/wmfrank/atari-benchmark/reports/Atari-Sample-Factory2-Baselines-CleanRL--VmlldzoyMzEyNjIw","title":"Reports"},{"location":"09-environment-integrations/dmlab/","text":"DeepMind Lab \u00b6 Installation \u00b6 Installation DeepMind Lab can be time consuming. If you are on a Linux system, we provide a prebuilt wheel . Either pip install deepmind_lab-1.0-py3-none-any.whl Or alternatively, DMLab can be compiled from source by following the instructions on the DMLab Github . pip install dm_env To train on DMLab-30 you will need brady_konkle_oliva2008 dataset . To significantly speed up training on DMLab-30 consider downloading our dataset of pre-generated environment layouts (see paper for details). Command lines for running experiments with these datasets are provided in the sections below. Running Experiments \u00b6 Run DMLab experiments with the scripts in sf_examples.dmlab . Example of training in the DMLab watermaze environment for 1B environment steps python -m sf_examples.dmlab.train_dmlab --env=dmlab_watermaze --train_for_env_steps=1000000000 --gamma=0.99 --use_rnn=True --num_workers=90 --num_envs_per_worker=12 --num_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --benchmark=False --max_grad_norm=0.0 --dmlab_renderer=software --decorrelate_experience_max_seconds=120 --reset_timeout_seconds=300 --encoder_conv_architecture=resnet_impala --encoder_conv_mlp_layers=512 --nonlinearity=relu --rnn_type=lstm --dmlab_extended_action_set=True --num_policies=1 --experiment=dmlab_watermaze_resnet_w90_v12 --set_workers_cpu_affinity=True --max_policy_lag=35 --dmlab30_dataset=~/datasets/brady_konkle_oliva2008 --dmlab_use_level_cache=True --dmlab_level_cache_path=/home/user/dmlab_cache DMLab-30 run on a 36-core server with 4 GPUs using Population-Based Training with 4 agents: python -m sf_examples.dmlab.train_dmlab --env=dmlab_30 --train_for_env_steps=10000000000 --gamma=0.99 --use_rnn=True --num_workers=90 --num_envs_per_worker=12 --num_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --benchmark=False --max_grad_norm=0.0 --dmlab_renderer=software --decorrelate_experience_max_seconds=120 --reset_timeout_seconds=300 --encoder_conv_architecture=resnet_impala --encoder_conv_mlp_layers=512 --nonlinearity=relu --rnn_type=lstm --dmlab_extended_action_set=True --num_policies=4 --pbt_replace_reward_gap=0.05 --pbt_replace_reward_gap_absolute=5.0 --pbt_period_env_steps=10000000 --pbt_start_mutation=100000000 --with_pbt=True --experiment=dmlab_30_resnet_4pbt_w90_v12 --dmlab_one_task_per_worker=True --set_workers_cpu_affinity=True --max_policy_lag=35 --pbt_target_objective=dmlab_target_objective --dmlab30_dataset=~/datasets/brady_konkle_oliva2008 --dmlab_use_level_cache=True --dmlab_level_cache_path=/home/user/dmlab_cache Models \u00b6 DMLab Command Line Parameter DMLab Environment name Model Checkpooints dmlab_30 DMLab-30 \ud83e\udd17 Hub DMLab30 checkpoints","title":"DeepMind Lab"},{"location":"09-environment-integrations/dmlab/#deepmind-lab","text":"","title":"DeepMind Lab"},{"location":"09-environment-integrations/dmlab/#installation","text":"Installation DeepMind Lab can be time consuming. If you are on a Linux system, we provide a prebuilt wheel . Either pip install deepmind_lab-1.0-py3-none-any.whl Or alternatively, DMLab can be compiled from source by following the instructions on the DMLab Github . pip install dm_env To train on DMLab-30 you will need brady_konkle_oliva2008 dataset . To significantly speed up training on DMLab-30 consider downloading our dataset of pre-generated environment layouts (see paper for details). Command lines for running experiments with these datasets are provided in the sections below.","title":"Installation"},{"location":"09-environment-integrations/dmlab/#running-experiments","text":"Run DMLab experiments with the scripts in sf_examples.dmlab . Example of training in the DMLab watermaze environment for 1B environment steps python -m sf_examples.dmlab.train_dmlab --env=dmlab_watermaze --train_for_env_steps=1000000000 --gamma=0.99 --use_rnn=True --num_workers=90 --num_envs_per_worker=12 --num_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --benchmark=False --max_grad_norm=0.0 --dmlab_renderer=software --decorrelate_experience_max_seconds=120 --reset_timeout_seconds=300 --encoder_conv_architecture=resnet_impala --encoder_conv_mlp_layers=512 --nonlinearity=relu --rnn_type=lstm --dmlab_extended_action_set=True --num_policies=1 --experiment=dmlab_watermaze_resnet_w90_v12 --set_workers_cpu_affinity=True --max_policy_lag=35 --dmlab30_dataset=~/datasets/brady_konkle_oliva2008 --dmlab_use_level_cache=True --dmlab_level_cache_path=/home/user/dmlab_cache DMLab-30 run on a 36-core server with 4 GPUs using Population-Based Training with 4 agents: python -m sf_examples.dmlab.train_dmlab --env=dmlab_30 --train_for_env_steps=10000000000 --gamma=0.99 --use_rnn=True --num_workers=90 --num_envs_per_worker=12 --num_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --benchmark=False --max_grad_norm=0.0 --dmlab_renderer=software --decorrelate_experience_max_seconds=120 --reset_timeout_seconds=300 --encoder_conv_architecture=resnet_impala --encoder_conv_mlp_layers=512 --nonlinearity=relu --rnn_type=lstm --dmlab_extended_action_set=True --num_policies=4 --pbt_replace_reward_gap=0.05 --pbt_replace_reward_gap_absolute=5.0 --pbt_period_env_steps=10000000 --pbt_start_mutation=100000000 --with_pbt=True --experiment=dmlab_30_resnet_4pbt_w90_v12 --dmlab_one_task_per_worker=True --set_workers_cpu_affinity=True --max_policy_lag=35 --pbt_target_objective=dmlab_target_objective --dmlab30_dataset=~/datasets/brady_konkle_oliva2008 --dmlab_use_level_cache=True --dmlab_level_cache_path=/home/user/dmlab_cache","title":"Running Experiments"},{"location":"09-environment-integrations/dmlab/#models","text":"DMLab Command Line Parameter DMLab Environment name Model Checkpooints dmlab_30 DMLab-30 \ud83e\udd17 Hub DMLab30 checkpoints","title":"Models"},{"location":"09-environment-integrations/envpool/","text":"Envpool \u00b6 Installation \u00b6 Install Sample-Factory with Envpool dependencies with PyPI: pip install sample-factory[atari,envpool] pip install sample-factory[mujoco,envpool] Running Experiments \u00b6 EnvPool is a C++-based batched environment pool with pybind11 and thread pool. It has high performance (~1M raw FPS with Atari games, ~3M raw FPS with Mujoco simulator). We provide examples for envpool for Atari and Mujoco environments. The default parameters provide reasonable training speed, but can be tuning based on your machine configuration to achieve higher throughput. To train a model with envpool in the BreakoutNoFrameskip-v4 environment: python -m sf_examples.envpool.atari.train_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\" To visualize the training results, use the enjoy_atari script: python -m sf_examples.atari.enjoy_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\" Multiple experiments can be run in parallel with the launcher module. atari_envs is an example launcher script that runs atari envs with 4 seeds. python -m sample_factory.launcher.run --run=sf_examples.envpool.atari.experiments.atari_envs --backend=processes --max_parallel=8 --pause_between=1 --experiments_per_gpu=10000 --num_gpus=1","title":"Envpool"},{"location":"09-environment-integrations/envpool/#envpool","text":"","title":"Envpool"},{"location":"09-environment-integrations/envpool/#installation","text":"Install Sample-Factory with Envpool dependencies with PyPI: pip install sample-factory[atari,envpool] pip install sample-factory[mujoco,envpool]","title":"Installation"},{"location":"09-environment-integrations/envpool/#running-experiments","text":"EnvPool is a C++-based batched environment pool with pybind11 and thread pool. It has high performance (~1M raw FPS with Atari games, ~3M raw FPS with Mujoco simulator). We provide examples for envpool for Atari and Mujoco environments. The default parameters provide reasonable training speed, but can be tuning based on your machine configuration to achieve higher throughput. To train a model with envpool in the BreakoutNoFrameskip-v4 environment: python -m sf_examples.envpool.atari.train_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\" To visualize the training results, use the enjoy_atari script: python -m sf_examples.atari.enjoy_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\" Multiple experiments can be run in parallel with the launcher module. atari_envs is an example launcher script that runs atari envs with 4 seeds. python -m sample_factory.launcher.run --run=sf_examples.envpool.atari.experiments.atari_envs --backend=processes --max_parallel=8 --pause_between=1 --experiments_per_gpu=10000 --num_gpus=1","title":"Running Experiments"},{"location":"09-environment-integrations/isaacgym/","text":"IsaacGym \u00b6 Installation \u00b6 Install IsaacGym from NVIDIA at https://developer.nvidia.com/isaac-gym . Installation instructions can be found in the package's docs folder. Python 3.8 is compatable with both IsaacGym and Sample-Factory Install IsaacGymEnvs from https://github.com/NVIDIA-Omniverse/IsaacGymEnvs . Running Experiments \u00b6 Run IsaacGym experiments using scripts from the sf_examples.isaacgym_examples folder. Currently, we support the AllegroHand, Ant, Anymal, AnymalTerrain, BallBalance, Cartpole , Humanoid, and ShadowHand environments out of the box, and more environments can be added in train_isaacgym.py . To run an experiment in the Ant environment: python -m sf_examples.isaacgym_examples.train_isaacgym --actor_worker_gpus 0 --env=Ant --train_for_env_steps=100000000 --experiment=isaacgym_ant Multiple experiments can be run in parallel using the experiment launcher. See the experiments folder in sf_examples.isaacgym_examples for examples. To run multiple Ant and Humanoid experiments, run: python -m sample_factory.launcher.run --run=sf_examples.isaacgym_examples.experiments.isaacgym_basic_envs --backend=processes --max_parallel=2 --experiments_per_gpu=2 --num_gpus=1 Results \u00b6 Reports \u00b6 We tested the IsaacGym Ant and Humanoid environments with and without recurrence. When using an RNN and recurrence, the Ant and Humanoid environments see an improvement in sample efficiency. However, there is a decrease in wall time efficiency. https://wandb.ai/andrewzhang505/sample_factory/reports/IsaacGym-Ant-and-Humanoid--VmlldzozMDUxNTky The AllegroHand environment was tested with and without return normalization. Return normalization is essential to this environment as it improved the performance by around 200% https://wandb.ai/andrewzhang505/sample_factory/reports/IsaacGym-AllegroHand--VmlldzozMDUxNjA2 Models \u00b6 Environment HuggingFace Hub Models Evaluation Metrics Ant https://huggingface.co/andrewzhang505/isaacgym_ant 11830.10 \u00b1 875.26 Humanoid https://huggingface.co/andrewzhang505/isaacgym_humanoid 8839.07 \u00b1 407.26 AllegroHand https://huggingface.co/andrewzhang505/isaacgym_allegrohand 3608.18 \u00b1 1062.94 Videos \u00b6 Ant Environment \u00b6 Humanoid Environment \u00b6 AllegroHand Environment \u00b6","title":"IsaacGym"},{"location":"09-environment-integrations/isaacgym/#isaacgym","text":"","title":"IsaacGym"},{"location":"09-environment-integrations/isaacgym/#installation","text":"Install IsaacGym from NVIDIA at https://developer.nvidia.com/isaac-gym . Installation instructions can be found in the package's docs folder. Python 3.8 is compatable with both IsaacGym and Sample-Factory Install IsaacGymEnvs from https://github.com/NVIDIA-Omniverse/IsaacGymEnvs .","title":"Installation"},{"location":"09-environment-integrations/isaacgym/#running-experiments","text":"Run IsaacGym experiments using scripts from the sf_examples.isaacgym_examples folder. Currently, we support the AllegroHand, Ant, Anymal, AnymalTerrain, BallBalance, Cartpole , Humanoid, and ShadowHand environments out of the box, and more environments can be added in train_isaacgym.py . To run an experiment in the Ant environment: python -m sf_examples.isaacgym_examples.train_isaacgym --actor_worker_gpus 0 --env=Ant --train_for_env_steps=100000000 --experiment=isaacgym_ant Multiple experiments can be run in parallel using the experiment launcher. See the experiments folder in sf_examples.isaacgym_examples for examples. To run multiple Ant and Humanoid experiments, run: python -m sample_factory.launcher.run --run=sf_examples.isaacgym_examples.experiments.isaacgym_basic_envs --backend=processes --max_parallel=2 --experiments_per_gpu=2 --num_gpus=1","title":"Running Experiments"},{"location":"09-environment-integrations/isaacgym/#results","text":"","title":"Results"},{"location":"09-environment-integrations/isaacgym/#reports","text":"We tested the IsaacGym Ant and Humanoid environments with and without recurrence. When using an RNN and recurrence, the Ant and Humanoid environments see an improvement in sample efficiency. However, there is a decrease in wall time efficiency. https://wandb.ai/andrewzhang505/sample_factory/reports/IsaacGym-Ant-and-Humanoid--VmlldzozMDUxNTky The AllegroHand environment was tested with and without return normalization. Return normalization is essential to this environment as it improved the performance by around 200% https://wandb.ai/andrewzhang505/sample_factory/reports/IsaacGym-AllegroHand--VmlldzozMDUxNjA2","title":"Reports"},{"location":"09-environment-integrations/isaacgym/#models","text":"Environment HuggingFace Hub Models Evaluation Metrics Ant https://huggingface.co/andrewzhang505/isaacgym_ant 11830.10 \u00b1 875.26 Humanoid https://huggingface.co/andrewzhang505/isaacgym_humanoid 8839.07 \u00b1 407.26 AllegroHand https://huggingface.co/andrewzhang505/isaacgym_allegrohand 3608.18 \u00b1 1062.94","title":"Models"},{"location":"09-environment-integrations/isaacgym/#videos","text":"","title":"Videos"},{"location":"09-environment-integrations/isaacgym/#ant-environment","text":"","title":"Ant Environment"},{"location":"09-environment-integrations/isaacgym/#humanoid-environment","text":"","title":"Humanoid Environment"},{"location":"09-environment-integrations/isaacgym/#allegrohand-environment","text":"","title":"AllegroHand Environment"},{"location":"09-environment-integrations/megaverse/","text":"Megaverse \u00b6 Megaverse is a dedicated high-throughput RL environment with batched GPU rendering. This document demonstrates an example of external integration, i.e. another project using Sample Factory as a library. Very likely this is going to be the most common integration scenario. Installation \u00b6 Install Megaverse according to the readme of the repo Megaverse . Further instructions assume that you are in a Python (or Conda) environment with a working Megaverse installation. Running Experiments \u00b6 Run Megaverse experiments with the scripts in megaverse_rl . To train a model in the TowerBuilding environment: python -m megaverse_rl.train_megaverse --train_for_seconds=360000000 --train_for_env_steps=2000000000 --algo=APPO --gamma=0.997 --use_rnn=True --rnn_num_layers=2 --num_workers=12 --num_envs_per_worker=2 --num_epochs=1 --rollout=32 --recurrence=32 --batch_size=4096 --actor_worker_gpus 0 --env_gpu_observations=False --num_policies=1 --with_pbt=False --max_grad_norm=0.0 --exploration_loss=symmetric_kl --exploration_loss_coeff=0.001 --megaverse_num_simulation_threads=1 --megaverse_num_envs_per_instance=32 --megaverse_num_agents_per_env=1 --megaverse_use_vulkan=True --policy_workers_per_policy=2 --reward_clip=30 --env=TowerBuilding --experiment=TowerBuilding To visualize the training results, use the enjoy_megaverse script: python -m megaverse_rl.enjoy_megaverse --algo=APPO --env=TowerBuilding --experiment=TowerBuilding --megaverse_num_envs_per_instance=1 --fps=20 --megaverse_use_vulkan=True Multiple experiments can be run in parallel with the launcher module. megaverse_envs is an example launcher script that runs megaverse envs with 5 seeds. python -m sample_factory.launcher.run --run=megaverse_rl.runs.single_agent --backend=processes --max_parallel=2 --pause_between=1 --experiments_per_gpu=2 --num_gpus=1 Or you could run experiments on slurm: python -m sample_factory.launcher.run --run=megaverse_rl.runs.single_agent --backend=slurm --slurm_workdir=./slurm_megaverse --experiment_suffix=slurm --slurm_gpus_per_job=1 --slurm_cpus_per_gpu=16 --slurm_sbatch_template=./sample_factory/launcher/slurm/sbatch_timeout.sh --pause_between=1 --slurm_print_only=False Results \u00b6 Reports \u00b6 We trained models in the TowerBuilding environment in SF2 with single agent per env. https://wandb.ai/wmfrank/megaverse-benchmark/reports/Megaverse-trained-Sample-Factory--VmlldzoyNTAxMDUz Models \u00b6 An example APPO model trained on Megaverse environments is uploaded to the HuggingFace Hub. The models have all been trained for 2G steps. Environment HuggingFace Hub Models TowerBuilding https://huggingface.co/wmFrank/sample-factory-2-megaverse Tower Building with single agent \u00b6","title":"Megaverse"},{"location":"09-environment-integrations/megaverse/#megaverse","text":"Megaverse is a dedicated high-throughput RL environment with batched GPU rendering. This document demonstrates an example of external integration, i.e. another project using Sample Factory as a library. Very likely this is going to be the most common integration scenario.","title":"Megaverse"},{"location":"09-environment-integrations/megaverse/#installation","text":"Install Megaverse according to the readme of the repo Megaverse . Further instructions assume that you are in a Python (or Conda) environment with a working Megaverse installation.","title":"Installation"},{"location":"09-environment-integrations/megaverse/#running-experiments","text":"Run Megaverse experiments with the scripts in megaverse_rl . To train a model in the TowerBuilding environment: python -m megaverse_rl.train_megaverse --train_for_seconds=360000000 --train_for_env_steps=2000000000 --algo=APPO --gamma=0.997 --use_rnn=True --rnn_num_layers=2 --num_workers=12 --num_envs_per_worker=2 --num_epochs=1 --rollout=32 --recurrence=32 --batch_size=4096 --actor_worker_gpus 0 --env_gpu_observations=False --num_policies=1 --with_pbt=False --max_grad_norm=0.0 --exploration_loss=symmetric_kl --exploration_loss_coeff=0.001 --megaverse_num_simulation_threads=1 --megaverse_num_envs_per_instance=32 --megaverse_num_agents_per_env=1 --megaverse_use_vulkan=True --policy_workers_per_policy=2 --reward_clip=30 --env=TowerBuilding --experiment=TowerBuilding To visualize the training results, use the enjoy_megaverse script: python -m megaverse_rl.enjoy_megaverse --algo=APPO --env=TowerBuilding --experiment=TowerBuilding --megaverse_num_envs_per_instance=1 --fps=20 --megaverse_use_vulkan=True Multiple experiments can be run in parallel with the launcher module. megaverse_envs is an example launcher script that runs megaverse envs with 5 seeds. python -m sample_factory.launcher.run --run=megaverse_rl.runs.single_agent --backend=processes --max_parallel=2 --pause_between=1 --experiments_per_gpu=2 --num_gpus=1 Or you could run experiments on slurm: python -m sample_factory.launcher.run --run=megaverse_rl.runs.single_agent --backend=slurm --slurm_workdir=./slurm_megaverse --experiment_suffix=slurm --slurm_gpus_per_job=1 --slurm_cpus_per_gpu=16 --slurm_sbatch_template=./sample_factory/launcher/slurm/sbatch_timeout.sh --pause_between=1 --slurm_print_only=False","title":"Running Experiments"},{"location":"09-environment-integrations/megaverse/#results","text":"","title":"Results"},{"location":"09-environment-integrations/megaverse/#reports","text":"We trained models in the TowerBuilding environment in SF2 with single agent per env. https://wandb.ai/wmfrank/megaverse-benchmark/reports/Megaverse-trained-Sample-Factory--VmlldzoyNTAxMDUz","title":"Reports"},{"location":"09-environment-integrations/megaverse/#models","text":"An example APPO model trained on Megaverse environments is uploaded to the HuggingFace Hub. The models have all been trained for 2G steps. Environment HuggingFace Hub Models TowerBuilding https://huggingface.co/wmFrank/sample-factory-2-megaverse","title":"Models"},{"location":"09-environment-integrations/megaverse/#tower-building-with-single-agent","text":"","title":"Tower Building with single agent"},{"location":"09-environment-integrations/mujoco/","text":"MuJoCo \u00b6 Installation \u00b6 Install Sample Factory with MuJoCo dependencies with PyPI: pip install sample-factory[mujoco] Running Experiments \u00b6 Run MuJoCo experiments with the scripts in sf_examples.mujoco . The default parameters have been chosen to match CleanRL's results in the report below (please note that we can achieve even faster training on a multi-core machine with more optimal parameters). To train a model in the Ant-v4 environment: python -m sf_examples.mujoco.train_mujoco --algo=APPO --env=mujoco_ant --experiment=<experiment_name> To visualize the training results, use the enjoy_mujoco script: python -m sf_examples.mujoco.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<experiment_name> Multiple experiments can be run in parallel with the launcher module. mujoco_all_envs is an example launcher script that runs all mujoco envs with 10 seeds. python -m sample_factory.launcher.run --run=sf_examples.mujoco.experiments.mujoco_all_envs --backend=processes --max_parallel=4 --pause_between=1 --experiments_per_gpu=10000 --num_gpus=1 --experiment_suffix=0 List of Supported Environments \u00b6 Specify the environment to run with the --env command line parameter. The following MuJoCo v4 environments are supported out of the box, and more environments can be added as needed in sf_examples.mujoco.mujoco.mujoco_utils MuJoCo Environment Name Sample Factory Command Line Parameter Ant-v4 mujoco_ant HalfCheetah-v4 mujoco_halfcheetah Hopper-v4 mujoco_hopper Humanoid-v4 mujoco_humanoid Walker2d-v4 mujoco_walker InvertedDoublePendulum-v4 mujoco_doublependulum InvertedPendulum-v4 mujoco_pendulum Reacher-v4 mujoco_reacher Swimmer-v4 mujoco_swimmer Results \u00b6 Reports \u00b6 Sample Factory was benchmarked on MuJoCo against CleanRL. Sample-Factory was able to achieve similar sample efficiency as CleanRL using the same parameters. https://wandb.ai/andrewzhang505/sample_factory/reports/MuJoCo-Sample-Factory-vs-CleanRL-w-o-EnvPool--VmlldzoyMjMyMTQ0 Sample Factory can run experiments synchronously or asynchronously, with asynchronous execution usually having worse sample efficiency but runs faster. MuJoCo's environments were compared using the two modes in Sample-Factory https://wandb.ai/andrewzhang505/sample_factory/reports/MuJoCo-Synchronous-vs-Asynchronous--VmlldzoyMzEzNDUz Sample Factory comparison with CleanRL in terms of wall time. Both experiments are run on a 16 core machine with 1 GPU. Sample-Factory was able to complete 10M samples 5 times as fast as CleanRL https://wandb.ai/andrewzhang505/sample_factory/reports/MuJoCo-Sample-Factory-vs-CleanRL-Wall-Time--VmlldzoyMzg2MDA3 Models \u00b6 Various APPO models trained on MuJoCo environments are uploaded to the HuggingFace Hub. The models have all been trained for 10M steps. Videos of the agents after training can be found on the HuggingFace Hub. The models below are the best models from the experiment against CleanRL above. The evaluation metrics here are obtained by running the model 10 times. Environment HuggingFace Hub Models Evaluation Metrics Ant-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-ant 5876.09 \u00b1 166.99 HalfCheetah-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-halfcheetah 6262.56 \u00b1 67.29 Humanoid-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-humanoid 5439.48 \u00b1 1314.24 Walker2d-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-walker 5487.74 \u00b1 48.96 Hopper-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-hopper 2793.44 \u00b1 642.58 InvertedDoublePendulum-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-doublependulum 9350.13 \u00b1 1.31 InvertedPendulum-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-pendulum 1000.00 \u00b1 0.00 Reacher-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-reacher -4.53 \u00b1 1.79 Swimmer-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-swimmer 117.28 \u00b1 2.91 Videos \u00b6 Below are some video examples of agents in various MuJoCo envioronments. Videos for all environments can be found in the HuggingFace Hub pages linked above. HalfCheetah-v4 \u00b6 Ant-v4 \u00b6 InvertedDoublePendulum-v4 \u00b6","title":"MuJoCo"},{"location":"09-environment-integrations/mujoco/#mujoco","text":"","title":"MuJoCo"},{"location":"09-environment-integrations/mujoco/#installation","text":"Install Sample Factory with MuJoCo dependencies with PyPI: pip install sample-factory[mujoco]","title":"Installation"},{"location":"09-environment-integrations/mujoco/#running-experiments","text":"Run MuJoCo experiments with the scripts in sf_examples.mujoco . The default parameters have been chosen to match CleanRL's results in the report below (please note that we can achieve even faster training on a multi-core machine with more optimal parameters). To train a model in the Ant-v4 environment: python -m sf_examples.mujoco.train_mujoco --algo=APPO --env=mujoco_ant --experiment=<experiment_name> To visualize the training results, use the enjoy_mujoco script: python -m sf_examples.mujoco.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<experiment_name> Multiple experiments can be run in parallel with the launcher module. mujoco_all_envs is an example launcher script that runs all mujoco envs with 10 seeds. python -m sample_factory.launcher.run --run=sf_examples.mujoco.experiments.mujoco_all_envs --backend=processes --max_parallel=4 --pause_between=1 --experiments_per_gpu=10000 --num_gpus=1 --experiment_suffix=0","title":"Running Experiments"},{"location":"09-environment-integrations/mujoco/#list-of-supported-environments","text":"Specify the environment to run with the --env command line parameter. The following MuJoCo v4 environments are supported out of the box, and more environments can be added as needed in sf_examples.mujoco.mujoco.mujoco_utils MuJoCo Environment Name Sample Factory Command Line Parameter Ant-v4 mujoco_ant HalfCheetah-v4 mujoco_halfcheetah Hopper-v4 mujoco_hopper Humanoid-v4 mujoco_humanoid Walker2d-v4 mujoco_walker InvertedDoublePendulum-v4 mujoco_doublependulum InvertedPendulum-v4 mujoco_pendulum Reacher-v4 mujoco_reacher Swimmer-v4 mujoco_swimmer","title":"List of Supported Environments"},{"location":"09-environment-integrations/mujoco/#results","text":"","title":"Results"},{"location":"09-environment-integrations/mujoco/#reports","text":"Sample Factory was benchmarked on MuJoCo against CleanRL. Sample-Factory was able to achieve similar sample efficiency as CleanRL using the same parameters. https://wandb.ai/andrewzhang505/sample_factory/reports/MuJoCo-Sample-Factory-vs-CleanRL-w-o-EnvPool--VmlldzoyMjMyMTQ0 Sample Factory can run experiments synchronously or asynchronously, with asynchronous execution usually having worse sample efficiency but runs faster. MuJoCo's environments were compared using the two modes in Sample-Factory https://wandb.ai/andrewzhang505/sample_factory/reports/MuJoCo-Synchronous-vs-Asynchronous--VmlldzoyMzEzNDUz Sample Factory comparison with CleanRL in terms of wall time. Both experiments are run on a 16 core machine with 1 GPU. Sample-Factory was able to complete 10M samples 5 times as fast as CleanRL https://wandb.ai/andrewzhang505/sample_factory/reports/MuJoCo-Sample-Factory-vs-CleanRL-Wall-Time--VmlldzoyMzg2MDA3","title":"Reports"},{"location":"09-environment-integrations/mujoco/#models","text":"Various APPO models trained on MuJoCo environments are uploaded to the HuggingFace Hub. The models have all been trained for 10M steps. Videos of the agents after training can be found on the HuggingFace Hub. The models below are the best models from the experiment against CleanRL above. The evaluation metrics here are obtained by running the model 10 times. Environment HuggingFace Hub Models Evaluation Metrics Ant-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-ant 5876.09 \u00b1 166.99 HalfCheetah-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-halfcheetah 6262.56 \u00b1 67.29 Humanoid-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-humanoid 5439.48 \u00b1 1314.24 Walker2d-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-walker 5487.74 \u00b1 48.96 Hopper-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-hopper 2793.44 \u00b1 642.58 InvertedDoublePendulum-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-doublependulum 9350.13 \u00b1 1.31 InvertedPendulum-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-pendulum 1000.00 \u00b1 0.00 Reacher-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-reacher -4.53 \u00b1 1.79 Swimmer-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-swimmer 117.28 \u00b1 2.91","title":"Models"},{"location":"09-environment-integrations/mujoco/#videos","text":"Below are some video examples of agents in various MuJoCo envioronments. Videos for all environments can be found in the HuggingFace Hub pages linked above.","title":"Videos"},{"location":"09-environment-integrations/mujoco/#halfcheetah-v4","text":"","title":"HalfCheetah-v4"},{"location":"09-environment-integrations/mujoco/#ant-v4","text":"","title":"Ant-v4"},{"location":"09-environment-integrations/mujoco/#inverteddoublependulum-v4","text":"","title":"InvertedDoublePendulum-v4"},{"location":"09-environment-integrations/swarm-rl/","text":"Quad-Swarm-RL Integrations \u00b6 Installation \u00b6 Clone https://github.com/Zhehui-Huang/quad-swarm-rl into your home directory Install dependencies in your conda environment cd ~/quad-swarm-rl pip install -e . Note: if you have any error with bezier, run: BEZIER_NO_EXTENSION=true pip install bezier==2020.5.19 pip install -e . Running Experiments \u00b6 The environments can be run from the quad_swarm_rl folder in the downloaded quad-swarm-rl directory instead of from sample-factory directly. Experiments can be run with the train script and viewed with the enjoy script. If you are running custom experiments, it is recommended to use the quad_multi_mix_baseline runner script and make any modifications as needed. See sf2_single_drone and sf2_multi_drone runner scripts for an examples. The quadrotor environments have many unique parameters that can be found in quadrotor_params.py . Some relevant params for rendering results include --quads_view_mode which can be set to local or global for viewing multi-drone experiments, and --quads_mode which determines which scenario(s) to train on, with mix using all scenarios. Results \u00b6 Reports \u00b6 Comparison using a single drone between normalized (input and return normalization) and un-normalized experiments. Normalization helped the drones learn in around half the number of steps. https://wandb.ai/andrewzhang505/sample_factory/reports/Quad-Swarm-RL--VmlldzoyMzU1ODQ1 Experiments with 8 drones in scenarios with and without obstacles. All experiments used input and return normalization. Research and development are still being done on multi-drone scenarios to reduce the number of collisions. https://wandb.ai/andrewzhang505/sample_factory/reports/Quad-Swarm-RL-Multi-Drone--VmlldzoyNDkwNDQ0 Models \u00b6 Description HuggingFace Hub Models Evaluation Metrics Single drone with normalization https://huggingface.co/andrewzhang505/quad-swarm-single-drone-sf2 0.03 \u00b1 1.86 Multi drone without obstacles https://huggingface.co/andrewzhang505/quad-swarm-rl-multi-drone-no-obstacles -0.40 \u00b1 4.47 Multi drone with obstacles https://huggingface.co/andrewzhang505/quad-swarm-rl-multi-drone-obstacles -2.84 \u00b1 3.71 Videos \u00b6 Single drone with normalization flying between dynamic goals.","title":"Quad-Swarm-RL Integrations"},{"location":"09-environment-integrations/swarm-rl/#quad-swarm-rl-integrations","text":"","title":"Quad-Swarm-RL Integrations"},{"location":"09-environment-integrations/swarm-rl/#installation","text":"Clone https://github.com/Zhehui-Huang/quad-swarm-rl into your home directory Install dependencies in your conda environment cd ~/quad-swarm-rl pip install -e . Note: if you have any error with bezier, run: BEZIER_NO_EXTENSION=true pip install bezier==2020.5.19 pip install -e .","title":"Installation"},{"location":"09-environment-integrations/swarm-rl/#running-experiments","text":"The environments can be run from the quad_swarm_rl folder in the downloaded quad-swarm-rl directory instead of from sample-factory directly. Experiments can be run with the train script and viewed with the enjoy script. If you are running custom experiments, it is recommended to use the quad_multi_mix_baseline runner script and make any modifications as needed. See sf2_single_drone and sf2_multi_drone runner scripts for an examples. The quadrotor environments have many unique parameters that can be found in quadrotor_params.py . Some relevant params for rendering results include --quads_view_mode which can be set to local or global for viewing multi-drone experiments, and --quads_mode which determines which scenario(s) to train on, with mix using all scenarios.","title":"Running Experiments"},{"location":"09-environment-integrations/swarm-rl/#results","text":"","title":"Results"},{"location":"09-environment-integrations/swarm-rl/#reports","text":"Comparison using a single drone between normalized (input and return normalization) and un-normalized experiments. Normalization helped the drones learn in around half the number of steps. https://wandb.ai/andrewzhang505/sample_factory/reports/Quad-Swarm-RL--VmlldzoyMzU1ODQ1 Experiments with 8 drones in scenarios with and without obstacles. All experiments used input and return normalization. Research and development are still being done on multi-drone scenarios to reduce the number of collisions. https://wandb.ai/andrewzhang505/sample_factory/reports/Quad-Swarm-RL-Multi-Drone--VmlldzoyNDkwNDQ0","title":"Reports"},{"location":"09-environment-integrations/swarm-rl/#models","text":"Description HuggingFace Hub Models Evaluation Metrics Single drone with normalization https://huggingface.co/andrewzhang505/quad-swarm-single-drone-sf2 0.03 \u00b1 1.86 Multi drone without obstacles https://huggingface.co/andrewzhang505/quad-swarm-rl-multi-drone-no-obstacles -0.40 \u00b1 4.47 Multi drone with obstacles https://huggingface.co/andrewzhang505/quad-swarm-rl-multi-drone-obstacles -2.84 \u00b1 3.71","title":"Models"},{"location":"09-environment-integrations/swarm-rl/#videos","text":"Single drone with normalization flying between dynamic goals.","title":"Videos"},{"location":"09-environment-integrations/vizdoom/","text":"VizDoom \u00b6 Installation \u00b6 To install VizDoom just follow system setup instructions from the original repository ( VizDoom linux_deps ), after which the latest VizDoom can be installed from PyPI: pip install vizdoom Running Experiments \u00b6 Run experiments with the scripts in sf_examples.vizdoom . Train for 4B env steps (also can be stopped at any time with Ctrl+C and resumed by using the same cmd). This is more or less optimal training setup for a 10-core machine. python -m sf_examples.vizdoom.train_vizdoom --env=doom_battle --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --batch_size=2048 --wide_aspect_ratio=False --num_workers=20 --num_envs_per_worker=20 --num_policies=1 --experiment=doom_battle_w20_v20 Run at any point to visualize the experiment: python -m sf_examples.vizdoom.enjoy_vizdoom --env=doom_battle --algo=APPO --experiment=doom_battle_w20_v20 Launcher scripts are also provided in sf_examples.vizdoom.experiments to run experiments in parallel or on slurm. Reproducing Paper Results \u00b6 Train on one of the 6 \"basic\" VizDoom environments: python -m sf_examples.vizdoom.train_vizdoom --train_for_env_steps=500000000 --algo=APPO --env=doom_my_way_home --env_frameskip=4 --use_rnn=True --num_workers=36 --num_envs_per_worker=8 --num_policies=1 --batch_size=2048 --wide_aspect_ratio=False --experiment=doom_basic_envs Doom \"battle\" and \"battle2\" environments, 36-core server (72 logical cores) with 4 GPUs: python -m sf_examples.vizdoom.train_vizdoom --env=doom_battle --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=72 --num_envs_per_worker=8 --num_policies=1 --batch_size=2048 --wide_aspect_ratio=False --max_grad_norm=0.0 --experiment=doom_battle python -m sf_examples.vizdoom.train_vizdoom --env=doom_battle2 --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=72 --num_envs_per_worker=8 --num_policies=1 --batch_size=2048 --wide_aspect_ratio=False --max_grad_norm=0.0 --experiment=doom_battle_2 Duel and deathmatch versus bots, population-based training, 36-core server: python -m sf_examples.vizdoom.train_vizdoom --env=doom_duel_bots --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --reward_scale=0.5 --num_workers=72 --num_envs_per_worker=32 --num_policies=8 --batch_size=2048 --benchmark=False --res_w=128 --res_h=72 --wide_aspect_ratio=False --pbt_replace_reward_gap=0.2 --pbt_replace_reward_gap_absolute=3.0 --pbt_period_env_steps=5000000 --save_milestones_sec=1800 --with_pbt=True --experiment=doom_duel_bots python -m sf_examples.vizdoom.train_vizdoom --env=doom_deathmatch_bots --train_for_seconds=3600000 --algo=APPO --use_rnn=True --gamma=0.995 --env_frameskip=2 --num_workers=80 --num_envs_per_worker=24 --num_policies=8 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --with_pbt=True --pbt_period_env_steps=5000000 --experiment=doom_deathmatch_bots Duel and deathmatch self-play, PBT, 36-core server: python -m sf_examples.vizdoom.train_vizdoom --env=doom_duel --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --num_workers=72 --num_envs_per_worker=16 --num_policies=8 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --benchmark=False --pbt_replace_reward_gap=0.5 --pbt_replace_reward_gap_absolute=0.35 --pbt_period_env_steps=5000000 --with_pbt=True --pbt_start_mutation=100000000 --experiment=doom_duel_full python -m sf_examples.vizdoom.train_vizdoom --env=doom_deathmatch_full --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --num_workers=72 --num_envs_per_worker=16 --num_policies=8 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --benchmark=False --pbt_replace_reward_gap=0.1 --pbt_replace_reward_gap_absolute=0.1 --pbt_period_env_steps=5000000 --with_pbt=True --pbt_start_mutation=100000000 --experiment=doom_deathmatch_full Reproducing benchmarking results: This achieves 50K+ framerate on a 10-core machine (Intel Core i9-7900X): python -m sf_examples.vizdoom.train_vizdoom --env=doom_benchmark --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=20 --num_envs_per_worker=32 --num_policies=1 --batch_size=4096 --experiment=doom_battle_appo_fps_20_32 --res_w=128 --res_h=72 --wide_aspect_ratio=False --policy_workers_per_policy=2 --worker_num_splits=2 This achieves 100K+ framerate on a 36-core machine: python -m sf_examples.vizdoom.train_vizdoom --env=doom_benchmark --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=72 --num_envs_per_worker=24 --num_policies=1 --batch_size=8192 --wide_aspect_ratio=False --experiment=doom_battle_appo_w72_v24 --policy_workers_per_policy=2 Results \u00b6 Reports \u00b6 We reproduced the paper results in SF2 in the Battle and Battle2 and compared the results using input normalization. Input normalization has improved results in the Battle environment. This experiment with input normalization was run with sf_examples.vizdoom.experiments.sf2_doom_battle_envs . Note that normalize_input=True is set compared to the results from the paper https://wandb.ai/andrewzhang505/sample_factory/reports/VizDoom-Battle-Environments--VmlldzoyMzcyODQx In SF2's bot environments (deathmatch_bots and duel_bots), we trained the agents against randomly generated bots as opposed to a curriculum of increasing bot difficulty. This is because the latest version of ViZDoom environment no longer provides the bots used in the curriculum, and SF2 no longer requires the curriculum to train properly. However, due to the differences in bot difficulty, the current training results are no longer comparable to the paper. An example training curve on deathmatch_bots with the same parameters as in the paper is shown below. Additionally, the report includes 8 agents trained using PBT against duel-bots with normalization and we were able to get better results than the Sample Factory paper: https://wandb.ai/andrewzhang505/sample_factory/reports/ViZDoom-Bots--VmlldzoyNzY2NDI1 We also trained in the doom_duel multi-agent environment using self play. The training metrics of the experiment can be found on the Hugging Face Hub: https://huggingface.co/andrewzhang505/doom-duel-selfplay/tensorboard The reward scaling done by PBT can be found under zz_pbt . For example in this experiment, the reward scaling related to damage dealt rew_DAMAGECOUNT_0 increase more than 10x from 0.01 to around 0.15 at max. The true_objective reported corresponds to the fraction of matches won. In this experiment, the agents performed fairly equally as seen under policy_stats/avg_true_objective as agents rarely win over 60% of matches. Models \u00b6 The models below are the best models from the input normalization experiment above. The evaluation metrics here are obtained by running the model 10 times. Environment HuggingFace Hub Models Evaluation Metrics Battle https://huggingface.co/andrewzhang505/sample-factory-2-doom-battle 59.37 \u00b1 3.93 Battle2 https://huggingface.co/andrewzhang505/sample-factory-2-doom-battle2 36.40 \u00b1 4.20 Deathmatch-Bots https://huggingface.co/andrewzhang505/doom_deathmatch_bots 85.66 \u00b1 28.53 Duel-Bots https://huggingface.co/andrewzhang505/doom_duel_bots_pbt 55.39 \u00b1 17.13 Duel https://huggingface.co/andrewzhang505/doom-duel-selfplay Videos \u00b6 Doom Battle \u00b6 Doom Battle2 \u00b6 Doom Deathmatch Bots \u00b6 Doom Duel Bots PBT \u00b6 Doom Duel Multi-Agent \u00b6","title":"VizDoom"},{"location":"09-environment-integrations/vizdoom/#vizdoom","text":"","title":"VizDoom"},{"location":"09-environment-integrations/vizdoom/#installation","text":"To install VizDoom just follow system setup instructions from the original repository ( VizDoom linux_deps ), after which the latest VizDoom can be installed from PyPI: pip install vizdoom","title":"Installation"},{"location":"09-environment-integrations/vizdoom/#running-experiments","text":"Run experiments with the scripts in sf_examples.vizdoom . Train for 4B env steps (also can be stopped at any time with Ctrl+C and resumed by using the same cmd). This is more or less optimal training setup for a 10-core machine. python -m sf_examples.vizdoom.train_vizdoom --env=doom_battle --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --batch_size=2048 --wide_aspect_ratio=False --num_workers=20 --num_envs_per_worker=20 --num_policies=1 --experiment=doom_battle_w20_v20 Run at any point to visualize the experiment: python -m sf_examples.vizdoom.enjoy_vizdoom --env=doom_battle --algo=APPO --experiment=doom_battle_w20_v20 Launcher scripts are also provided in sf_examples.vizdoom.experiments to run experiments in parallel or on slurm.","title":"Running Experiments"},{"location":"09-environment-integrations/vizdoom/#reproducing-paper-results","text":"Train on one of the 6 \"basic\" VizDoom environments: python -m sf_examples.vizdoom.train_vizdoom --train_for_env_steps=500000000 --algo=APPO --env=doom_my_way_home --env_frameskip=4 --use_rnn=True --num_workers=36 --num_envs_per_worker=8 --num_policies=1 --batch_size=2048 --wide_aspect_ratio=False --experiment=doom_basic_envs Doom \"battle\" and \"battle2\" environments, 36-core server (72 logical cores) with 4 GPUs: python -m sf_examples.vizdoom.train_vizdoom --env=doom_battle --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=72 --num_envs_per_worker=8 --num_policies=1 --batch_size=2048 --wide_aspect_ratio=False --max_grad_norm=0.0 --experiment=doom_battle python -m sf_examples.vizdoom.train_vizdoom --env=doom_battle2 --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=72 --num_envs_per_worker=8 --num_policies=1 --batch_size=2048 --wide_aspect_ratio=False --max_grad_norm=0.0 --experiment=doom_battle_2 Duel and deathmatch versus bots, population-based training, 36-core server: python -m sf_examples.vizdoom.train_vizdoom --env=doom_duel_bots --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --reward_scale=0.5 --num_workers=72 --num_envs_per_worker=32 --num_policies=8 --batch_size=2048 --benchmark=False --res_w=128 --res_h=72 --wide_aspect_ratio=False --pbt_replace_reward_gap=0.2 --pbt_replace_reward_gap_absolute=3.0 --pbt_period_env_steps=5000000 --save_milestones_sec=1800 --with_pbt=True --experiment=doom_duel_bots python -m sf_examples.vizdoom.train_vizdoom --env=doom_deathmatch_bots --train_for_seconds=3600000 --algo=APPO --use_rnn=True --gamma=0.995 --env_frameskip=2 --num_workers=80 --num_envs_per_worker=24 --num_policies=8 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --with_pbt=True --pbt_period_env_steps=5000000 --experiment=doom_deathmatch_bots Duel and deathmatch self-play, PBT, 36-core server: python -m sf_examples.vizdoom.train_vizdoom --env=doom_duel --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --num_workers=72 --num_envs_per_worker=16 --num_policies=8 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --benchmark=False --pbt_replace_reward_gap=0.5 --pbt_replace_reward_gap_absolute=0.35 --pbt_period_env_steps=5000000 --with_pbt=True --pbt_start_mutation=100000000 --experiment=doom_duel_full python -m sf_examples.vizdoom.train_vizdoom --env=doom_deathmatch_full --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --num_workers=72 --num_envs_per_worker=16 --num_policies=8 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --benchmark=False --pbt_replace_reward_gap=0.1 --pbt_replace_reward_gap_absolute=0.1 --pbt_period_env_steps=5000000 --with_pbt=True --pbt_start_mutation=100000000 --experiment=doom_deathmatch_full Reproducing benchmarking results: This achieves 50K+ framerate on a 10-core machine (Intel Core i9-7900X): python -m sf_examples.vizdoom.train_vizdoom --env=doom_benchmark --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=20 --num_envs_per_worker=32 --num_policies=1 --batch_size=4096 --experiment=doom_battle_appo_fps_20_32 --res_w=128 --res_h=72 --wide_aspect_ratio=False --policy_workers_per_policy=2 --worker_num_splits=2 This achieves 100K+ framerate on a 36-core machine: python -m sf_examples.vizdoom.train_vizdoom --env=doom_benchmark --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=72 --num_envs_per_worker=24 --num_policies=1 --batch_size=8192 --wide_aspect_ratio=False --experiment=doom_battle_appo_w72_v24 --policy_workers_per_policy=2","title":"Reproducing Paper Results"},{"location":"09-environment-integrations/vizdoom/#results","text":"","title":"Results"},{"location":"09-environment-integrations/vizdoom/#reports","text":"We reproduced the paper results in SF2 in the Battle and Battle2 and compared the results using input normalization. Input normalization has improved results in the Battle environment. This experiment with input normalization was run with sf_examples.vizdoom.experiments.sf2_doom_battle_envs . Note that normalize_input=True is set compared to the results from the paper https://wandb.ai/andrewzhang505/sample_factory/reports/VizDoom-Battle-Environments--VmlldzoyMzcyODQx In SF2's bot environments (deathmatch_bots and duel_bots), we trained the agents against randomly generated bots as opposed to a curriculum of increasing bot difficulty. This is because the latest version of ViZDoom environment no longer provides the bots used in the curriculum, and SF2 no longer requires the curriculum to train properly. However, due to the differences in bot difficulty, the current training results are no longer comparable to the paper. An example training curve on deathmatch_bots with the same parameters as in the paper is shown below. Additionally, the report includes 8 agents trained using PBT against duel-bots with normalization and we were able to get better results than the Sample Factory paper: https://wandb.ai/andrewzhang505/sample_factory/reports/ViZDoom-Bots--VmlldzoyNzY2NDI1 We also trained in the doom_duel multi-agent environment using self play. The training metrics of the experiment can be found on the Hugging Face Hub: https://huggingface.co/andrewzhang505/doom-duel-selfplay/tensorboard The reward scaling done by PBT can be found under zz_pbt . For example in this experiment, the reward scaling related to damage dealt rew_DAMAGECOUNT_0 increase more than 10x from 0.01 to around 0.15 at max. The true_objective reported corresponds to the fraction of matches won. In this experiment, the agents performed fairly equally as seen under policy_stats/avg_true_objective as agents rarely win over 60% of matches.","title":"Reports"},{"location":"09-environment-integrations/vizdoom/#models","text":"The models below are the best models from the input normalization experiment above. The evaluation metrics here are obtained by running the model 10 times. Environment HuggingFace Hub Models Evaluation Metrics Battle https://huggingface.co/andrewzhang505/sample-factory-2-doom-battle 59.37 \u00b1 3.93 Battle2 https://huggingface.co/andrewzhang505/sample-factory-2-doom-battle2 36.40 \u00b1 4.20 Deathmatch-Bots https://huggingface.co/andrewzhang505/doom_deathmatch_bots 85.66 \u00b1 28.53 Duel-Bots https://huggingface.co/andrewzhang505/doom_duel_bots_pbt 55.39 \u00b1 17.13 Duel https://huggingface.co/andrewzhang505/doom-duel-selfplay","title":"Models"},{"location":"09-environment-integrations/vizdoom/#videos","text":"","title":"Videos"},{"location":"09-environment-integrations/vizdoom/#doom-battle","text":"","title":"Doom Battle"},{"location":"09-environment-integrations/vizdoom/#doom-battle2","text":"","title":"Doom Battle2"},{"location":"09-environment-integrations/vizdoom/#doom-deathmatch-bots","text":"","title":"Doom Deathmatch Bots"},{"location":"09-environment-integrations/vizdoom/#doom-duel-bots-pbt","text":"","title":"Doom Duel Bots PBT"},{"location":"09-environment-integrations/vizdoom/#doom-duel-multi-agent","text":"","title":"Doom Duel Multi-Agent"},{"location":"10-huggingface/huggingface/","text":"Hugging Face \ud83e\udd17 Hub \u00b6 Sample Factory has integrations with \ud83e\udd17 Hugging Face Hub to push models with evaluation results and training metrics to the hub. Setting Up \u00b6 The Hugging Face Hub requires git lfs to download model files. sudo apt install git-lfs git lfs install To upload files to the Hugging Face Hub, you need to sign up and log in to your Hugging Face account with: huggingface-cli login As part of the huggingface-cli login , you should generate a token with write access at https://huggingface.co/settings/tokens Downloading Models \u00b6 Using the load_from_hub Scipt \u00b6 To download a model from the Hugging Face Hub to use with Sample-Factory, use the load_from_hub script: python -m sample_factory.huggingface.load_from_hub -r <HuggingFace_repo_id> -d <train_dir_path> The command line arguments are: -r : The repo ID for the HF repository to download. The repo ID should be in the format <username>/<repo_name> -d : An optional argument to specify the directory to save the experiment to. Defaults to ./train_dir which will save the repo to ./train_dir/<repo_name> Download Model Repository Directly \u00b6 Hugging Face repositories can be downloaded directly using git clone : git clone <URL of Hugging Face Repo> Using Downloaded Models with Sample Factory \u00b6 After downloading the model, you can run the models in the repo with the enjoy script corresponding to your environment. For example, if you are downloading a mujoco-ant model, it can be run with: python -m sf_examples.mujoco.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<repo_name> --train_dir=./train_dir Note, you may have to specify the --train_dir if your local train_dir has a different path than the one in the config.json Uploading Models \u00b6 Using enjoy.py \u00b6 You can upload your models to the Hub using your environment's enjoy script with the --push_to_hub flag. Uploading using enjoy can also generate evaluation metrics and a replay video. The evaluation metrics are generated by running your model on the specified environment for a number of episodes and reporting the mean and std reward of those runs. Other relevant command line arguments are: --hf_repository : The repository to push to. Must be of the form <username>/<repo_name> . The model will be saved to https://huggingface.co/<username>/<repo_name> --max_num_episodes : Number of episodes to evaluate on before uploading. Used to generate evaluation metrics. It is recommended to use multiple episodes to generate an accurate mean and std. --max_num_frames : Number of frames to evaluate on before uploading. An alternative to max_num_episodes --no_render : A flag that disables rendering and showing the environment steps. It is recommended to set this flag to speed up the evaluation process. You can also save a video of the model during evaluation to upload to the hub with the --save_video flag --video_frames : The number of frames to be rendered in the video. Defaults to -1 which renders an entire episode --video_name : The name of the video to save as. If None , will save to replay.mp4 in your experiment directory For example: python -m sf_examples.mujoco.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<repo_name> --train_dir=./train_dir --max_num_episodes=10 --push_to_hub --hf_repository=<username>/<hf_repo_name> --save_video --no_render Using the push_to_hub Script \u00b6 If you want to upload without generating evaluation metrics or a replay video, you can use the push_to_hub script: python -m sample_factory.huggingface.push_to_hub -r <hf_username>/<hf_repo_name> -d <experiment_dir_path> The command line arguments are: -r : The repo_id to save on HF Hub. This is the same as hf_repository in the enjoy script and must be in the form <hf_username>/<hf_repo_name> -d : The full path to your experiment directory to upload","title":"Hugging Face \ud83e\udd17 Hub"},{"location":"10-huggingface/huggingface/#hugging-face-hub","text":"Sample Factory has integrations with \ud83e\udd17 Hugging Face Hub to push models with evaluation results and training metrics to the hub.","title":"Hugging Face \ud83e\udd17 Hub"},{"location":"10-huggingface/huggingface/#setting-up","text":"The Hugging Face Hub requires git lfs to download model files. sudo apt install git-lfs git lfs install To upload files to the Hugging Face Hub, you need to sign up and log in to your Hugging Face account with: huggingface-cli login As part of the huggingface-cli login , you should generate a token with write access at https://huggingface.co/settings/tokens","title":"Setting Up"},{"location":"10-huggingface/huggingface/#downloading-models","text":"","title":"Downloading Models"},{"location":"10-huggingface/huggingface/#using-the-load_from_hub-scipt","text":"To download a model from the Hugging Face Hub to use with Sample-Factory, use the load_from_hub script: python -m sample_factory.huggingface.load_from_hub -r <HuggingFace_repo_id> -d <train_dir_path> The command line arguments are: -r : The repo ID for the HF repository to download. The repo ID should be in the format <username>/<repo_name> -d : An optional argument to specify the directory to save the experiment to. Defaults to ./train_dir which will save the repo to ./train_dir/<repo_name>","title":"Using the load_from_hub Scipt"},{"location":"10-huggingface/huggingface/#download-model-repository-directly","text":"Hugging Face repositories can be downloaded directly using git clone : git clone <URL of Hugging Face Repo>","title":"Download Model Repository Directly"},{"location":"10-huggingface/huggingface/#using-downloaded-models-with-sample-factory","text":"After downloading the model, you can run the models in the repo with the enjoy script corresponding to your environment. For example, if you are downloading a mujoco-ant model, it can be run with: python -m sf_examples.mujoco.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<repo_name> --train_dir=./train_dir Note, you may have to specify the --train_dir if your local train_dir has a different path than the one in the config.json","title":"Using Downloaded Models with Sample Factory"},{"location":"10-huggingface/huggingface/#uploading-models","text":"","title":"Uploading Models"},{"location":"10-huggingface/huggingface/#using-enjoypy","text":"You can upload your models to the Hub using your environment's enjoy script with the --push_to_hub flag. Uploading using enjoy can also generate evaluation metrics and a replay video. The evaluation metrics are generated by running your model on the specified environment for a number of episodes and reporting the mean and std reward of those runs. Other relevant command line arguments are: --hf_repository : The repository to push to. Must be of the form <username>/<repo_name> . The model will be saved to https://huggingface.co/<username>/<repo_name> --max_num_episodes : Number of episodes to evaluate on before uploading. Used to generate evaluation metrics. It is recommended to use multiple episodes to generate an accurate mean and std. --max_num_frames : Number of frames to evaluate on before uploading. An alternative to max_num_episodes --no_render : A flag that disables rendering and showing the environment steps. It is recommended to set this flag to speed up the evaluation process. You can also save a video of the model during evaluation to upload to the hub with the --save_video flag --video_frames : The number of frames to be rendered in the video. Defaults to -1 which renders an entire episode --video_name : The name of the video to save as. If None , will save to replay.mp4 in your experiment directory For example: python -m sf_examples.mujoco.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<repo_name> --train_dir=./train_dir --max_num_episodes=10 --push_to_hub --hf_repository=<username>/<hf_repo_name> --save_video --no_render","title":"Using enjoy.py"},{"location":"10-huggingface/huggingface/#using-the-push_to_hub-script","text":"If you want to upload without generating evaluation metrics or a replay video, you can use the push_to_hub script: python -m sample_factory.huggingface.push_to_hub -r <hf_username>/<hf_repo_name> -d <experiment_dir_path> The command line arguments are: -r : The repo_id to save on HF Hub. This is the same as hf_repository in the enjoy script and must be in the form <hf_username>/<hf_repo_name> -d : The full path to your experiment directory to upload","title":"Using the push_to_hub Script"},{"location":"11-release-notes/release-notes/","text":"Recent releases \u00b6 v2.0.2 \u00b6 cfg.json renamed to config.json for consistency with other HuggingFace integrations We can still load from legacy checkpoints ( cfg.json will be renamed to config.json ) v2.0.1 \u00b6 Added mujoco & isaacgym examples to the PyPI package Added missing __init__.py files v2.0.0 \u00b6 Major update, adds new functionality, changes API and configuration parameters Major API update, codebase rewritten from scratch for better maintainability and clarity Synchronous and asynchronous training modes Serial and parallel execution modes Support for vectorized and GPU-accelerated environments in batched sampling mode Integration with Hugging Face Hub New environment integrations, CI, and 40+ documentation pages See v1 to v2 transition guide for details. v1.121.4 \u00b6 Support Weights and Biases (see section \"WandB support\") More configurable population-based training: can set from command line whether or not to mutate gamma, plus the perturbation magnitude for all float hyperparams can also be set from command line: --pbt_optimize_gamma: Whether to optimize gamma, discount factor, or not (experimental) (default: False) --pbt_perturb_min: When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default: 1.05) --pbt_perturb_max: When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default: 1.5) v1.121.3 \u00b6 Fixed a small bug related to population-based training (a reward shaping dictionary was assumed to be a flat dict, while it could be a nested dict in some envs) v1.121.2 \u00b6 Fixed a bug that prevented Vizdoom *.cfg and *.wad files from being copied to site-packages during installation from PyPI Added example on how to use custom Vizdoom envs without modifying the source code ( sample_factory_examples/train_custom_vizdoom_env.py ) v1.121.0 \u00b6 Added fixed KL divergence penalty as in https://arxiv.org/pdf/1707.06347.pdf Its usage is highly encouraged in environments with continuous action spaces (i.e. set --kl_loss_coeff=1.0). Otherwise numerical instabilities can occur in certain environments, especially when the policy lag is high More summaries related to the new loss v1.120.2 \u00b6 More improvements and fixes in runner interface, including support for NGC cluster v1.120.1 \u00b6 Runner interface improvements for Slurm v1.120.0 \u00b6 Support inactive agents. To deactivate an agent for a portion of the episode the environment should return info={'is_active': False} for the inactive agent. Useful for environments such as hide-n-seek. Improved memory consumption and performance with better shared memory management. Experiment logs are now saved into the experiment folder as sf_log.txt DMLab-related bug fixes (courtesy of @donghoonlee04 and @sungwoong . Thank you!)","title":"Recent releases"},{"location":"11-release-notes/release-notes/#recent-releases","text":"","title":"Recent releases"},{"location":"11-release-notes/release-notes/#v202","text":"cfg.json renamed to config.json for consistency with other HuggingFace integrations We can still load from legacy checkpoints ( cfg.json will be renamed to config.json )","title":"v2.0.2"},{"location":"11-release-notes/release-notes/#v201","text":"Added mujoco & isaacgym examples to the PyPI package Added missing __init__.py files","title":"v2.0.1"},{"location":"11-release-notes/release-notes/#v200","text":"Major update, adds new functionality, changes API and configuration parameters Major API update, codebase rewritten from scratch for better maintainability and clarity Synchronous and asynchronous training modes Serial and parallel execution modes Support for vectorized and GPU-accelerated environments in batched sampling mode Integration with Hugging Face Hub New environment integrations, CI, and 40+ documentation pages See v1 to v2 transition guide for details.","title":"v2.0.0"},{"location":"11-release-notes/release-notes/#v11214","text":"Support Weights and Biases (see section \"WandB support\") More configurable population-based training: can set from command line whether or not to mutate gamma, plus the perturbation magnitude for all float hyperparams can also be set from command line: --pbt_optimize_gamma: Whether to optimize gamma, discount factor, or not (experimental) (default: False) --pbt_perturb_min: When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default: 1.05) --pbt_perturb_max: When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default: 1.5)","title":"v1.121.4"},{"location":"11-release-notes/release-notes/#v11213","text":"Fixed a small bug related to population-based training (a reward shaping dictionary was assumed to be a flat dict, while it could be a nested dict in some envs)","title":"v1.121.3"},{"location":"11-release-notes/release-notes/#v11212","text":"Fixed a bug that prevented Vizdoom *.cfg and *.wad files from being copied to site-packages during installation from PyPI Added example on how to use custom Vizdoom envs without modifying the source code ( sample_factory_examples/train_custom_vizdoom_env.py )","title":"v1.121.2"},{"location":"11-release-notes/release-notes/#v11210","text":"Added fixed KL divergence penalty as in https://arxiv.org/pdf/1707.06347.pdf Its usage is highly encouraged in environments with continuous action spaces (i.e. set --kl_loss_coeff=1.0). Otherwise numerical instabilities can occur in certain environments, especially when the policy lag is high More summaries related to the new loss","title":"v1.121.0"},{"location":"11-release-notes/release-notes/#v11202","text":"More improvements and fixes in runner interface, including support for NGC cluster","title":"v1.120.2"},{"location":"11-release-notes/release-notes/#v11201","text":"Runner interface improvements for Slurm","title":"v1.120.1"},{"location":"11-release-notes/release-notes/#v11200","text":"Support inactive agents. To deactivate an agent for a portion of the episode the environment should return info={'is_active': False} for the inactive agent. Useful for environments such as hide-n-seek. Improved memory consumption and performance with better shared memory management. Experiment logs are now saved into the experiment folder as sf_log.txt DMLab-related bug fixes (courtesy of @donghoonlee04 and @sungwoong . Thank you!)","title":"v1.120.0"},{"location":"12-community/citation/","text":"Citation \u00b6 If you use this repository in your work or otherwise wish to cite it, please make reference to our ICML2020 paper. @inproceedings{petrenko2020sf, title={Sample Factory: Egocentric 3D Control from Pixels at 100000 FPS with Asynchronous Reinforcement Learning}, author={Petrenko, Aleksei and Huang, Zhehui and Kumar, Tushar and Sukhatme, Gaurav and Koltun, Vladlen}, booktitle={ICML}, year={2020} } For questions, issues, inquiries please email apetrenko1991@gmail.com . Github issues and pull requests are welcome.","title":"Citation"},{"location":"12-community/citation/#citation","text":"If you use this repository in your work or otherwise wish to cite it, please make reference to our ICML2020 paper. @inproceedings{petrenko2020sf, title={Sample Factory: Egocentric 3D Control from Pixels at 100000 FPS with Asynchronous Reinforcement Learning}, author={Petrenko, Aleksei and Huang, Zhehui and Kumar, Tushar and Sukhatme, Gaurav and Koltun, Vladlen}, booktitle={ICML}, year={2020} } For questions, issues, inquiries please email apetrenko1991@gmail.com . Github issues and pull requests are welcome.","title":"Citation"},{"location":"12-community/contribution/","text":"Contribute to SF \u00b6 How to contribute to Sample Factory? \u00b6 Sample Factory is an open source project, so all contributions and suggestions are welcome. You can contribute in many different ways: giving ideas, answering questions, reporting bugs, proposing enhancements, improving the documentation, fixing bugs. Huge thanks in advance to every contributor! How to work on an open Issue? \u00b6 You have the list of open Issues at: https://github.com/alex-petrenko/sample-factory/issues Some of them may have the label help wanted : that means that any contributor is welcomed! If you would like to work on any of the open Issues: Make sure it is not already assigned to someone else. You have the assignee (if any) on the top of the right column of the Issue page. You can self-assign it by commenting on the Issue page with one of the keywords: #take or #self-assign . Work on your self-assigned issue and eventually create a Pull Request. How to create a Pull Request? \u00b6 Fork the repository by clicking on the 'Fork' button on the repository's page. This creates a copy of the code under your GitHub user account. Clone your fork to your local disk, and add the base repository as a remote: git clone git@github.com:<your Github handle>/sample-factory.git cd sample-factory git remote add upstream https://github.com/alex-petrenko/sample-factory.git Create a new branch to hold your development changes: git checkout -b a-descriptive-name-for-my-changes do not work on the main branch. Set up a development environment by running the following command in a virtual (or conda) environment: pip install -e . [ dev ] (If sample-factory was already installed in the virtual environment, remove it with pip uninstall sample-factory before reinstalling it in editable mode with the -e flag.) Develop the features on your branch. Format your code. Run black and isort so that your newly added files look nice with the following command: make format If you want to enable auto format checking before every commit, you can run the following command: pre-commit install Run unittests with the following command: make test Once you're happy with your files, add your changes and make a commit to record your changes locally: git add sample-factory/<your_dataset_name> git commit It is a good idea to sync your copy of the code with the original repository regularly. This way you can quickly account for changes: git fetch upstream git rebase upstream/main Push the changes to your account using: git push -u origin a-descriptive-name-for-my-changes Once you are satisfied, go the webpage of your fork on GitHub. Click on \"Pull request\" to send your to the project maintainers for review. See also additional notes on how to contribute to the documentation website.","title":"Contribute to SF"},{"location":"12-community/contribution/#contribute-to-sf","text":"","title":"Contribute to SF"},{"location":"12-community/contribution/#how-to-contribute-to-sample-factory","text":"Sample Factory is an open source project, so all contributions and suggestions are welcome. You can contribute in many different ways: giving ideas, answering questions, reporting bugs, proposing enhancements, improving the documentation, fixing bugs. Huge thanks in advance to every contributor!","title":"How to contribute to Sample Factory?"},{"location":"12-community/contribution/#how-to-work-on-an-open-issue","text":"You have the list of open Issues at: https://github.com/alex-petrenko/sample-factory/issues Some of them may have the label help wanted : that means that any contributor is welcomed! If you would like to work on any of the open Issues: Make sure it is not already assigned to someone else. You have the assignee (if any) on the top of the right column of the Issue page. You can self-assign it by commenting on the Issue page with one of the keywords: #take or #self-assign . Work on your self-assigned issue and eventually create a Pull Request.","title":"How to work on an open Issue?"},{"location":"12-community/contribution/#how-to-create-a-pull-request","text":"Fork the repository by clicking on the 'Fork' button on the repository's page. This creates a copy of the code under your GitHub user account. Clone your fork to your local disk, and add the base repository as a remote: git clone git@github.com:<your Github handle>/sample-factory.git cd sample-factory git remote add upstream https://github.com/alex-petrenko/sample-factory.git Create a new branch to hold your development changes: git checkout -b a-descriptive-name-for-my-changes do not work on the main branch. Set up a development environment by running the following command in a virtual (or conda) environment: pip install -e . [ dev ] (If sample-factory was already installed in the virtual environment, remove it with pip uninstall sample-factory before reinstalling it in editable mode with the -e flag.) Develop the features on your branch. Format your code. Run black and isort so that your newly added files look nice with the following command: make format If you want to enable auto format checking before every commit, you can run the following command: pre-commit install Run unittests with the following command: make test Once you're happy with your files, add your changes and make a commit to record your changes locally: git add sample-factory/<your_dataset_name> git commit It is a good idea to sync your copy of the code with the original repository regularly. This way you can quickly account for changes: git fetch upstream git rebase upstream/main Push the changes to your account using: git push -u origin a-descriptive-name-for-my-changes Once you are satisfied, go the webpage of your fork on GitHub. Click on \"Pull request\" to send your to the project maintainers for review. See also additional notes on how to contribute to the documentation website.","title":"How to create a Pull Request?"},{"location":"12-community/doc-contribution/","text":"Doc Contribution \u00b6 Clone the repo. You should be in the root folder containing \u2018docs\u2019, \u2018mkdocs.yml\u2019 config file, and \u2018docs.yml\u2019 github actions file. Install dev dependencies (includes mkdocs deps): pip install -e . [ dev ] Serve the website locally mkdocs docs-serve you should see the website on your localhost port now. Modify or create markdown files modify / create your markdown files in \u2018docs\u2019 folder. add your markdown path in the \u2018nav\u2019 section of \u2018mkdocs.yml\u2019. Example folder-yml correspondence: Commit and push your changes to remote repo. Github actions will automatically push your changes to your github pages website.","title":"Doc Contribution"},{"location":"12-community/doc-contribution/#doc-contribution","text":"Clone the repo. You should be in the root folder containing \u2018docs\u2019, \u2018mkdocs.yml\u2019 config file, and \u2018docs.yml\u2019 github actions file. Install dev dependencies (includes mkdocs deps): pip install -e . [ dev ] Serve the website locally mkdocs docs-serve you should see the website on your localhost port now. Modify or create markdown files modify / create your markdown files in \u2018docs\u2019 folder. add your markdown path in the \u2018nav\u2019 section of \u2018mkdocs.yml\u2019. Example folder-yml correspondence: Commit and push your changes to remote repo. Github actions will automatically push your changes to your github pages website.","title":"Doc Contribution"}]}